<!DOCTYPE html>
<html  lang="en">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 3.9.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>Tree-Based Learning Algorithms - Part 2 - DeepBlog</title>


    <meta name="description" content="Decision tree can be very non-robust. A small change in the training data can result in a large change in the tree and, consequently, the final predictions.  To resolve the problem above, some techni">
<meta name="keywords" content="Deep Learning, Machine Learning, NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="Tree-Based Learning Algorithms - Part 2">
<meta property="og:url" content="http://sxiazr.github.io/2019/11/24/tree-based-learning-algorithms-part-2/index.html">
<meta property="og:site_name" content="DeepBlog">
<meta property="og:description" content="Decision tree can be very non-robust. A small change in the training data can result in a large change in the tree and, consequently, the final predictions.  To resolve the problem above, some techni">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://sxiazr.github.io/images/og_image.png">
<meta property="og:updated_time" content="2020-01-01T12:33:34.590Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tree-Based Learning Algorithms - Part 2">
<meta name="twitter:description" content="Decision tree can be very non-robust. A small change in the training data can result in a large change in the tree and, consequently, the final predictions.  To resolve the problem above, some techni">
<meta name="twitter:image" content="http://sxiazr.github.io/images/og_image.png">







<link rel="icon" href="/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/css/back-to-top.css">


    
    

    
    
    
    

    
    
<link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    
        <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    

    


<link rel="stylesheet" href="/css/style.css">

</head>
<body class="is-3-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.svg" alt="Tree-Based Learning Algorithms - Part 2" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">Home</a>
                
                <a class="navbar-item"
                href="/archives">Archives</a>
                
                <a class="navbar-item"
                href="/categories">Categories</a>
                
                <a class="navbar-item"
                href="/tags">Tags</a>
                
                <a class="navbar-item"
                href="/about">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    
                    <a class="navbar-item" target="_blank" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main"><div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-11-24T16:00:00.000Z">2019-11-25</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    15 minutes read (About 2272 words)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span> visits
                </span>
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                Tree-Based Learning Algorithms - Part 2
            
        </h1>
        <div class="content">
            <blockquote>
<p>Decision tree can be very <strong>non-robust</strong>. A small change in the training data can result in a large change in the tree and, consequently, the final predictions.</p>
</blockquote>
<p>To resolve the problem above, some techniques, often called <strong><a href="/2019/11/29/model-ensemble/" title="ensemble methods">ensemble methods</a></strong>, construct multiple decision trees that are combined to yield a single and better consensus prediction.</p>
<a id="more"></a>
<h1 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h1><p>The <strong>random forest</strong> approach is a <strong>bagging method</strong> where <strong>deep trees</strong> (<a href="#Rule-of-Thumb">why deep</a>), fitted on bootstrap samples, are combined to produce an output with lower variance. However, it thus <strong>increasing the computational complexity by  $B$  times</strong>.</p>
<p><strong>Feature Sampling</strong>:<br>In addition to bagging, in the case of random forests, as each tree is constructed, <strong>only a random sample of predictors (features)</strong> is taken before each node is split, which <strong>decorrelates the trees</strong>.  This random small tweak has benefits below:</p>
<ul>
<li><p>There are often a <strong>few predictors that dominate</strong> the decision tree fitting process. Consequently, many other predictors, which could be <strong>useful for very local features</strong> of the data, are rarely selected as splitting variables. With this tweak, each predictor will have <strong>at least several opportunities</strong> to be the predictor defining a split.</p>
</li>
<li><p><strong>It makes the decision making process more robust to missing features</strong>: Observations with missing features can still be regressed or classified based on the features that are not missing.</p>
</li>
</ul>
<p><strong>Feature Importance Measures</strong>:<br>We can obtain an <strong>overall summary of the importance</strong> of each predictor (feature) estimating the <strong>metrics</strong> we mentioned in <a href="/2019/11/19/tree-based-learning-algorithms-part-1/" title="last post">last post</a>. We can <strong>record the total amount of metrics</strong> due to splits over <strong>a given predictor</strong>, averaged over all  $B$  trees. A large value indicates an important predictor. </p>
<p><strong>Essential parameters</strong>:</p>
<ul>
<li><strong>n_Trees</strong>:  In practice, few hundreds trees is often a good choice.</li>
<li><strong>n_Features</strong>:  Typically, if there are a total of  $D$ predictors,  $D/3$ predictors in the case of regression and $\sqrt{D}$ predictors in the case of classification make a good choice.</li>
</ul>
<h1 id="Boosted-Trees"><a href="#Boosted-Trees" class="headerlink" title="Boosted Trees"></a>Boosted Trees</h1><p>In this post, we will focus specifically on the two most common boosting algorithms: AdaBoost and Gradient Boosting.</p>
<h2 id="Generic-Setting-and-Algorithm"><a href="#Generic-Setting-and-Algorithm" class="headerlink" title="Generic Setting and Algorithm"></a>Generic Setting and Algorithm</h2><p>In supervised learning problems, one has an output variable $y$ and a vector of input variables $\mathbf{x}$ described via a joint probability distribution $P(\mathbf{x}, y)$. </p>
<p>Using a training set $\{(\mathbf{x}_{1},y_{1}),\dots ,(\mathbf{x}_{n},y_{n})\}$, the goal is to find an approximation ${\hat {F}}(\mathbf{x})$ that minimizes the expected value of some specified loss functions.</p>
$$\hat{F}=\underset{F}{\arg \min } \mathbb{E}_{\mathbf{x}, y}[L(y, F(\mathbf{x}))]$$

<p>In boosting method, it seeks ${\hat {F}}(\mathbf{x})$ in the form of a <strong>weighted sum</strong> of functions $h_{i}(\mathbf{x})$ (weak learners) from some class $\mathcal {H}$:</p>
$$\hat{F}(\mathbf{x})=\sum_{i=1}^{M} \gamma_{i} h_{i}(\mathbf{x})+\text { const. }$$

<p>It does so by starting with a model, consisting of a constant function $F_{0}(\mathbf{x})$:</p>
$${F_{0}(\mathbf{x})=\underset{\gamma}{\arg \min } \sum_{i=1}^{n} L\left(y_{i}, \gamma\right)}$$
<p>, and <strong>incrementally expands it</strong> in a <strong>greedy fashion</strong>:</p>
$$F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})+\gamma_{m}h_m\\{F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})+\underset{h_{m} \in \mathcal{H}}{\gamma_{m}\arg \min }\left[\sum_{i=1}^{n} L\left(y_{i}, F_{m-1}\left(\mathbf{x}_{i}\right)+h_{m}\left(\mathbf{x}_{i}\right)\right)\right]}$$, where $h_{m}\in {\mathcal {H}}$ is a weak learner function, $F_{m}(\mathbf{x})$ is the aggregated function after $m$ steps.

<p>The above algorithm of boosting is generic. One can come with several strategies for the choice of the <strong>weak learner</strong>, <strong>residual (error)</strong> to fit, and the <strong>weighting parameter</strong> $\lambda$. Therefore, there are various boosting methods. </p>
<h2 id="Adaptive-Boosting"><a href="#Adaptive-Boosting" class="headerlink" title="Adaptive Boosting"></a>Adaptive Boosting</h2><p>The <strong><a href="https://en.wikipedia.org/wiki/AdaBoost" target="_blank" rel="noopener">AdaBoost</a></strong> (Adaptive Boosting) approach is a <strong>boosting method</strong> where <strong>shallow trees</strong> (<a href="#Rule-of-Thumb">why shallow</a>), fitted on weighted samples, are combined to produce an output with <strong>lower bias</strong>. </p>
<p><strong>Be Compared to Random Forest</strong>:<br>Compared to Random Forest, AdaBoost is generally <strong>less computational</strong> (shallow trees). However, these computations to fit different weak learners  <strong>can’t be done in parallel</strong>. Overall, Random Forest is usually less accurate than Boosting on a wide range of tasks and usually slower in the runtime.</p>
<p><strong>Algorithm Setting</strong>:<br>The total error $E$ defined in AdaBoost is the <strong>sum of its exponential loss</strong> ($\sum_{i=1}^{N} e^{-y_{i} f\left(\mathbf{x}_{i}\right)}$, different loss function result in <strong>different variations</strong>) on each data point, given as follows:</p>

$$E=\sum_{i=1}^{N} e^{-y_{i} F_{m}\left(\mathbf{x}_{i}\right)}=\sum_{i=1}^{N} e^{-y_{i} F_{m-1}\left(\mathbf{x}_{i}\right)} e^{-y_{i} \gamma_{m} h_{m}\left(\mathbf{x}_{i}\right)}\\E=\sum_{i=1}^{N} w_{i}^{(m)} e^{-y_{i} \gamma_{m} h_{m}\left(\mathbf{x}_{i}\right)}$$, where $w_{i}^{(m)}=e^{-y_{i} F_{m-1}\left(\mathbf{x}_{i}\right)}$ is a constant error of last iteration.

<p>So we can determine the optimal weak learner $h_m$ at step $m$ by $\underset{h_m}{\arg \min }E$ and then the weighting parameter $\gamma_m$​ by $\frac{d E}{d \gamma_{m}}=0$. A derivation of them in binary classification can be seen at <a href="https://en.wikipedia.org/wiki/AdaBoost#Derivation" target="_blank" rel="noopener">wikipedia</a>.</p>
<p><strong>Intuition of the Algorithm</strong>:<br>AdaBoost <strong>weighs more on misclassified samples and good learners</strong> than the correctly classified samples and bad learners, respectively. </p>
<ul>
<li><p>First, it <strong>weighs observations</strong> in the dataset and train a new weak learner with a special focus given to the <strong>misclassified observations</strong>:$$E=\sum_{i=1}^{N} \underbrace{e^{-y_{i} F_{m-1}\left(\mathbf{x}_{i}\right)}}_{Weights} \underbrace{e^{-y_{i} \gamma_{m} h_{m}\left(\mathbf{x}_{i}\right)}}_{\text{Loss of each observation}}$$So $\underset{h_{m}}{\arg \min } E$ can <strong>not only be seen</strong> as an optimization of the boosted model after $m$ steps on the initial dataset <strong>but also</strong> a weak learner on the weighted dataset.</p>
</li>
<li><p>Second, it <strong>adds the weak learner</strong> with a coefficient $\gamma_m$ that <strong>expresse the performances of this weak learner</strong>: the better a weak learner performs, the more it contributes to the strong learner. </p>
<p>For example, in binary classification case: $\gamma_m=\frac{1}{2} \ln \left(\frac{\sum_{y_{i}=h_{m}\left(\mathbf{x}_{i}\right)} w_{i}^{(m)}}{\sum_{y_{i} \neq h_{m}\left(\mathbf{x}_{i}\right)} w_{i}^{(m)}}\right)$, in which the larger amount of misclassified data is the smaller $\lambda_m$ is.</p>
</li>
</ul>
<h2 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h2><p>Differ from Adaboost, in <a href="https://en.wikipedia.org/wiki/Gradient_boosting" target="_blank" rel="noopener">Gradient Boosting</a>, the decision tree model is trained on the pseudo-residuals, rather than residuals.</p>
<p><strong>Algorithm Setting</strong>:<br>The idea is to apply a  <a href="https://en.wikipedia.org/wiki/Steepest_descent" title="Steepest descent" target="_blank" rel="noopener">steepest descent</a>  step to this minimization problem (<strong>functional gradient descent</strong>, which regards the <strong>entire function as a variable</strong>).</p>
<p> We would update the model in accordance with the following equations:</p>

$$
r_{im}=\nabla_{F_{m-1}} L\left(y_{i}, F_{m-1}\left(\mathbf{x}_{i}\right)\right)\\
{F_{m}(x)=F_{m-1}(\mathbf{x})-\gamma_{m} \sum_{i=1}^{n}r_{im}} $$
<p>, where $r_{im}$ is the <strong>pseudo-residuals</strong> and the model is <strong>initialized with a constant value</strong> $F_{0}(\mathbf{x})=\underset{\gamma}{\arg \min } \sum_{i=1}^{n} L\left(y_{i}, \gamma\right)$. </p>
<p><strong>Generic Gradient Boosting Method</strong>:</p>
<ul>
<li>Initialize model with a constant value $F_{0}(\mathbf{x})=\underset{\gamma}{\arg \min } \sum_{i=1}^{n} L\left(y_{i}, \gamma\right)$.</li>
<li>For $m=1$ to $M$, a weak learner $h_{m}(\mathbf{x})$ is fitted to pseudo-residuals $\left\{\left(\mathbf{x}_i, r_{i m}\right)\right\}_{i=1}^{n}$.</li>
<li>Compute multiplier $\gamma _{m}$ using <a href="https://en.wikipedia.org/wiki/Line_search" title="Line search" target="_blank" rel="noopener"><strong>line search</strong></a>:$$\gamma_{m}=\underset{\gamma}{\arg \min } \sum_{i=1}^{n} L\left(y_{i}, F_{m-1}\left(\mathbf{x}_{i}\right)+\gamma h_{m}\left(\mathbf{x}_{i}\right)\right)$$</li>
<li>Update the model: $F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})+\gamma_{m} h_{m}(\mathbf{x})$</li>
</ul>
<p><strong>Compared to Adaboost</strong>:<br>Gradient boosting uses instead <strong>a gradient descent approach</strong> and can more <strong>easily be adapted</strong> to large number of loss functions. Thus, gradient boosting can be considered as <strong>a generalization of Adaboost</strong> to <strong>arbitrary differentiable loss functions</strong>.</p>
<h2 id="Regularization-Parameters-of-Boosted-Trees"><a href="#Regularization-Parameters-of-Boosted-Trees" class="headerlink" title="Regularization/Parameters of Boosted Trees"></a>Regularization/Parameters of Boosted Trees</h2><p>Several so-called  regularization techniques reduce this  overfitting effect by constraining the fitting procedure.</p>
<ul>
<li><p><strong>Iterations</strong>:   Namely, <strong>the number of trees</strong> in the model. Increasing it reduces the error on training set, but may lead to overfitting.</p>
</li>
<li><p><strong>Shrinkage / Learning Rate</strong>:  Shrinkage consists in modifying the update rule as follows:$$F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})+\nu \cdot \gamma _{m}h_{m}(\mathbf{x}),\quad 0<\nu \leq 1$$, where parameter $\nu$ is called the “learning rate”. < p>
<p>Empirically using small learning rates (such as  $\nu &lt;0.1$) improves models’ generalization ability (<strong>increase bias but decrease variance</strong>). However, it comes at the price of increasing computational time both during training and predicting, since lower learning rate <strong>requires more iterations</strong>.</p>
</\nu></p></li>
<li><p><strong>Sub-Sampling</strong>:  Motivated by the bagging method, a decision tree can be fit on a <strong>subsample</strong> of the training set drawn at random <strong>without replacement</strong>. Sampling ratio ($f$) <strong>introduces randomness</strong> into the algorithm and help prevent overfitting acting as a kind of regularization. $0.5\leq f\leq 0.8$ (typically set to $0.5$)  leads to good results. </p>
<p>Also, like in bagging, subsampling allows one to define an  <a href="https://en.wikipedia.org/wiki/Out-of-bag_error" title="Out-of-bag error" target="_blank" rel="noopener">out-of-bag error</a>, which helps avoid the need for an independent validation dataset, but often <strong>underestimate actual performance improvement</strong> and the <strong>optimal number of iterations</strong>.</p>
</li>
<li><p><strong>Minimum Sample Size for Splitting Trees</strong>:  Ignore any splits that lead to nodes containing fewer than this number of training set instances, which reduces variance.</p>
</li>
<li><p><strong>Sampling Features</strong>: Randomly choosing small subsets of features for different <strong>trees</strong>/<strong>layers</strong>/<strong>nodes</strong>, as in the case of Random Forest.</p>
</li>
<li><p><strong>Penalize Complexity of Tree</strong>:<br>Penalize model complexity of the learned model. The model complexity can be defined as:</p>
<ul>
<li><p>The <strong>proportional number of leaves</strong> in the learned trees. </p>
</li>
<li><p>$\ell _{2}$ penalty on the leaf values (score).</p>
<p>The <strong>joint optimization</strong> of <strong>loss and model complexity</strong> corresponds (equals) to a post-pruning algorithm to remove branches that fail to reduce the loss by a threshold. </p>
</li>
</ul>
</li>
</ul>
<h2 id="Xgboost"><a href="#Xgboost" class="headerlink" title="Xgboost"></a>Xgboost</h2><p>The biggest drawback of the gradient boosting trees is that the algorithm is quite sequential and hence is very slow.</p>
<p><a href="https://github.com/dmlc/xgboost" target="_blank" rel="noopener">XGBoost</a> provides an <strong>optimized implementation</strong> of gradient boosted trees. It uses various tricks like <strong>regularization</strong>, <strong>column block for parallel learning</strong>, and cache-aware access to accelerate the speed. </p>
<p><strong>Algorithm Setting</strong>:<br>The objective loss of Xgboost and its <strong>Second-order Taylor approximation</strong> is as following:</p>
$$\begin{split}\tilde{\mathcal{L}}&=\underbrace{\sum_{i=1}^{n} L\left(y_{i}, F_{m-1}\left(\mathbf{x}_{i}\right)+h_{m}\left(\mathbf{x}_{i}\right)\right)}_{Original loss}+\underbrace{\eta T+\sum_{j=1}^{T}\frac{1}{2} \lambda\left\|\omega_{j}\right\|^{2}}_{Regularization}\\&\approx\sum_{i=1}^{n}\left[L\left(y_{i}, F_{m-1}\right)+\frac{\partial{L\left(y_{i}, F_{m-1}\left(\mathbf{x}_{i}\right)\right)}}{\partial{F_{m-1}}} h_{m}\left(\mathbf{x}_{i}\right) + \frac{1}{2} \frac{\partial^2{L\left(y_{i}, F_{m-1}\left(\mathbf{x}_{i}\right)\right)}}{\partial F_{m-1}^2}h_{m}^{2}\left(\mathbf{x}_{i}\right)\right]\\&+ \eta T+\sum_{j=1}^{T}\frac{1}{2} \lambda\left\|\omega_{j}\right\|^{2}\\&=
\sum_{i=1}^{n}\left[L\left(y_{i}, F_{m-1}\right)+g^1_i h_{m}\left(\mathbf{x}_{i}\right) + \frac{1}{2} g^2_i h_{m}^{2}\left(\mathbf{x}_{i}\right)\right]+ \eta T+\sum_{j=1}^{T}\frac{1}{2} \lambda\left\|\omega_{j}\right\|^{2}\end{split}$$, where $\omega_j$ is the score in leaf nodes of weak learner $h_m$, and assume $I_{j}=\left\{i | q\left(\mathbf{x}_{i}\right)=j\right\}$ as the instance set of leaf $j$: $$\sum_{i=1}^{n}h_{m}\left(\mathbf{x}_{i}\right)=\sum_{j=1}^{T}\left(\sum_{i \in I_{j}} 1\right) w_{j}$$
<p>By <strong>setting derivative of loss to 0</strong>, we can get the <strong>optimal leaf scores</strong> for $h_m$ :</p>
$$
w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g^1_{i}}{\sum_{i \in I_{j}} g^2_{i}+\lambda}
$$
<p>But there could be <strong>an infinite number</strong> of possible trees with these optimal leaf scores, <a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener"><strong>detailed method</strong></a> is needed to specify the tree.</p>
<p><strong>Compared to GBDT</strong>:</p>
<ul>
<li>From the <strong>perspective of optimization</strong>, GBDT adopts <strong>fastest descent</strong> method to get the optimal solution of loss function, while Xgboost uses the <strong>analytic method</strong> to expand the loss function to its second-order approximation and find the analytic solution.</li>
<li>The <strong>parallelism</strong> of Xgboost is different from random forest. Xgboost can only carry out the next iteration after the previous iteration. <strong>The parallelism of xgboost is on the feature level</strong>.</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://reckoning.dev/boosted-trees/" target="_blank" rel="noopener">Blog: Understanding Boosted Trees Models</a><br><a href="https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205" target="_blank" rel="noopener">Blog: Ensemble methods: bagging, boosting and stacking</a><br><a href="https://stats.stackexchange.com/questions/380023/how-can-we-explain-the-fact-that-bagging-reduces-the-variance-while-retaining-t" target="_blank" rel="noopener">StackExchange: How can we explain the fact that “Bagging reduces the variance while retaining the bias” mathematically?</a><br><a href="https://snaildove.github.io/2018/10/02/get-started-XGBoost/" target="_blank" rel="noopener">Blog: XGBoost原理和底层实现剖析</a><br><a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="noopener">PPT: Introduction to Boosted Tree</a><br><a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">Paper: XGBoost: A Scalable Tree Boosting System</a></p>

        </div>
        
        
        
        
<div class="sharethis-inline-share-buttons"></div>
<script type='text/javascript' src='//platform-api.sharethis.com/js/sharethis.js#property=5d5e5688c60153001277c466&amp;product=inline-share-buttons' async='async'></script>

        
    </div>
</div>





<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2019/11/29/model-ensemble/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">Model Ensemble</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2019/11/19/tree-based-learning-algorithms-part-1/">
                <span class="level-item">Tree-Based Learning Algorithms - Part 1</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">Comments</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: false,
        verify: false,
        app_id: '0UMdpjsaIDv3QIm6TvTb5dMg-MdYXbMMI',
        app_key: 'WV1yT5spokAKC90uiItn48tR',
        lang: 'en',
        placeholder: 'Welcome your ideas',
	visitor: 'true'
    });
</script>

    </div>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                        <figure class="image is-128x128 has-mb-6">
                            <img class="is-rounded" src="/./images/profile-pic.jpg" alt="Shengzhao Xia">
                        </figure>
                    
                    
                    <p class="is-size-4 is-block">
                        Shengzhao Xia
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Master Student
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>EPFL, Lausanne</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Posts
                    </p>
                    <p class="title has-text-weight-normal">
                        14
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Categories
                    </p>
                    <p class="title has-text-weight-normal">
                        0
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Tags
                    </p>
                    <p class="title has-text-weight-normal">
                        0
                    </p>
                </div>
            </div>
        </nav>
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/ppoffice" target="_blank">
                Follow</a>
        </div>
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Github" href="https://github.com/ppoffice">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Facebook" href="https://facebook.com">
                
                <i class="fab fa-facebook"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Twitter" href="https://twitter.com">
                
                <i class="fab fa-twitter"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Dribbble" href="https://dribbble.com">
                
                <i class="fab fa-dribbble"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="RSS" href="/">
                
                <i class="fas fa-rss"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        
    
        
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Categories
            </h3>
            <ul class="menu-list">
            
            </ul>
        </div>
    </div>
</div>
    
        
    
    
        <div class="column-right-shadow  ">
        
            
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Recent
        </h3>
        
        <article class="media">
            
            <a href="/2020/06/30/binary-tree/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="数据结构：二叉树">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-06-30T16:00:00.000Z">2020-07-01</time></div>
                    <a href="/2020/06/30/binary-tree/" class="title has-link-black-ter is-size-6 has-text-weight-normal">数据结构：二叉树</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/12/09/build-transformer-from-scratch-part-1/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Build Transformer from Scratch - Part 1">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-12-09T16:00:00.000Z">2019-12-10</time></div>
                    <a href="/2019/12/09/build-transformer-from-scratch-part-1/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Build Transformer from Scratch - Part 1</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/11/30/entropy-in-information-theory/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Entropy in Information Theory">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-11-30T16:00:00.000Z">2019-12-01</time></div>
                    <a href="/2019/11/30/entropy-in-information-theory/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Entropy in Information Theory</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/11/29/model-ensemble/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Model Ensemble">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-11-29T16:00:00.000Z">2019-11-30</time></div>
                    <a href="/2019/11/29/model-ensemble/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Model Ensemble</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/11/24/tree-based-learning-algorithms-part-2/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Tree-Based Learning Algorithms - Part 2">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-11-24T16:00:00.000Z">2019-11-25</time></div>
                    <a href="/2019/11/24/tree-based-learning-algorithms-part-2/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Tree-Based Learning Algorithms - Part 2</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Archives
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2020/07/">
                <span class="level-start">
                    <span class="level-item">July 2020</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/12/">
                <span class="level-start">
                    <span class="level-item">December 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/11/">
                <span class="level-start">
                    <span class="level-item">November 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">6</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/10/">
                <span class="level-start">
                    <span class="level-item">October 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/08/">
                <span class="level-start">
                    <span class="level-item">August 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Tags
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
            </div>
        </div>
    </div>
</div>
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.svg" alt="Tree-Based Learning Algorithms - Part 2" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2020 Shengzhao Xia&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                <br>
                <span id="busuanzi_container_site_pv" class="theme-info">
                Visited by <span id="busuanzi_value_site_uv">0</span> users
                </span>
                <span id="busuanzi_container_site_uv" class="theme-info">
                <span id="busuanzi_value_site_pv">0</span> times
                </span>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>

<script>
var IcarusThemeSettings = {
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>



    
    
    
    <script src="/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>

    
    

<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>


    
    

    
    
    
    

    
    
    
    
    

    


<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>