{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/08/22/hello-world/"},{"title":"Normalization in Deep Learning","text":"Normalization refers to a process that makes something more normal or regular. In statistics, normalization means adjusting values measured on different scales to a common scale, or adjustments to bring the entire probability distributions of adjusted values into alignment. Normalization in deep learning can make the model to train efficiently. Why does Normalization work?Intuition 1: Input Normalization $\\frac{X - \\mu}{\\sigma}$ Make training less sensitive to the scale of features. After optimization, the feature with a large scale will always be weighted with a small value ($\\omega_{small}$), and vise versa ($\\omega_{large}$). Because of the large scale of the feature, a tiny change in the small weight ($\\omega_{small}$) will change the prediction by a lot compared to the same change in the large weight ($\\omega_{large}$). It means that setting $\\omega_{small}$ correctly might dominate the optimization process and the feature with a large scale is of more importance which actually makes no sense. Make optimization well-conditioned. The rough intuition is that the cost function will be more round and easier to optimize when features are all on similar scales. Cost function of the unnormalized features can be a very elongated bowl, whereas if we normalize the features, then the cost function will look more symmetric. Having an elongated cost function, we might have to use a very small learning rate and gradient descent might need a lot of steps to oscillate back and forth before it finally gets to the minimum. If we have more spherical contours, then gradient descent can pretty much go straight to the minimum and we can take much larger steps with gradient descent without much oscillation. Intuition 2: Covariate Shift Covariate shift refers to the problem that the distribution of the input values changes, but the concept (model) being learned remains stationary. In deep learning, the basic idea behind normalization is to limit the covariate shift, which allows the model to learn on a more stable distribution, and would thus accelerate the training of the network. Batch NormalizationIn deep learning, we are particularly concerned with the change in the distribution of the inputs to the hidden nodes within a network. A neural network changes the weights of each layer over the training, which means that the activations of each layer change as well. Since the activations of a previous layer are the inputs of the next layer, each layer in the neural network is faced with a situation where the input distribution changes with each step (covariate shift). Batch Normalization at Training Time What batch normalization does is, especially from the perspective of the later layers of the neural network, it limits the earlier layers to not get to shift around much, by restricting them to have the same mean and variance. It weakens the coupling between what the early layers and the later layers. For a layer of the network with d-dimensional input, $x = (x^{(1)},…,x^{(d)})$, each dimension is then normalized separately as following: $$\\hat{x}_i^{(k)} = \\frac{x_i^{(k)}-\\mu_B^{(k)}}{\\sqrt{\\sigma_B^{(k)^2}+\\epsilon}}$$where $\\mu_B^{(k)}$ and $\\sigma_B^{(k)^2}$ are the per-dimension mean and variance, respectively. $\\epsilon$ is added in the denominator for numerical stability and is an arbitrarily small constant. The resulting normalized $\\hat{x}^{(k)}$ have zero mean and unit variance, to restore the representation power of the network or better take advantage of the nonlinearity (if activations are normalized), a transformation step then follows as:$$y_i^{(k)} = \\gamma^{(k)} \\hat{x}_{i}^{(k)} +\\beta^{(k)}$$where the parameters $\\gamma^{(k)}$and $\\beta^{(k)}$ are subsequently learnt in the optimization process and can convert the mean and variance to any value that the network desires. Formally, the transform $BN_{\\gamma,\\beta}: x_{1…m} \\rightarrow y_{1…m}$ is denoted as the Batch Normalizing Transform. There are some debates in deep learning about whether weshould normalize the value before the activation function or after it. In practice, normalizing before the activation function is done much more often. Regularization can be motivated as a technique to improve the generalizability of a learned model. Batch normalization has a slight regularization effect. The mean and variance are a little bit noisy because they are estimated on a mini-batch. Similar to dropout, batch normalization adds small noise to the hidden layers and therefore has a very slight regularization effect. We don’t particularly use to batch normalization as a regularization but use it as a way to speed up learning. Batch Normalization at Test Time At the test time, we use the $\\gamma$ and $\\beta$ learned from training, the $\\mu$ and $\\sigma^2$ are estimated from the training set. Running the whole training set to get $\\mu$ and $\\sigma^2$ could be memory-consuming. So we keep a running average of the $\\mu$ and $\\sigma^2$ for each layer as we train the neural network across different mini-batches to get an estimation of them. In practice, the moving average people usually use here is exponentially weighted average. An exponential moving average (EMA), also known as an exponentially weighted moving average (EWMA), is a first-order filter that applies weighting factors which decrease exponentially. The EMA for a series $Y$ is calculated recursively: $$S_t = \\begin{cases} \\alpha \\cdot Y_1, &amp; t = 1 \\\\ \\alpha \\cdot Y_t + (1 - \\alpha) \\cdot S_{t-1}, &amp; t &gt; 1\\end{cases}$$Where: $\\alpha$ represents the degree of weighting decrease, a constant smoothing factor between 0 and 1. $Y_t$ and $S_t$ are the values at a time period $t$ respectively. This formula can also be expressed in technical analysis terms as follows: How the EMA steps towards the latest datum point:$$\\text{EMA}_\\text{today} = \\text{EMA}_\\text{yesterday} + \\alpha \\left[\\text{price}_\\text{today} - \\text{EMA}_\\text{yesterday}\\right]$$ How the weighting factor on each data point $p_1$, $p_2$, etc., decreases exponentially by expanding out $\\text{EMA}_\\text{yesterday}$ :$$\\begin{split}\\text{EMA}_\\text{today}= &amp; { \\alpha \\left[p_1 + (1 - \\alpha) p_2 + (1 - \\alpha)^2 p_3 + (1 - \\alpha)^3 p_4 + \\cdots \\right] } \\\\= &amp;\\frac{p_1 + (1 - \\alpha) p_2 + (1 - \\alpha)^2 p_3 + (1 - \\alpha)^3 p_4 + \\cdots}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + (1 - \\alpha)^3 + \\cdots}\\end{split}$$since $1/\\alpha = 1 + (1 - \\alpha) + (1 - \\alpha)^2 + \\cdots$ (weighted average). A couple of things about choosing $\\alpha$: A higher $\\alpha$ discounts older observations faster so that the EMA will adapt more quickly to the current changes. When compute EMA, we can think of $S_t$ as approximately averaging over $\\frac{1}{\\alpha}$ number of data. So with a low value of $\\alpha$, the EMA is much smoother and also has more latency. (Always set to 0.1 in deep learning) Bias correction is a technical modification that makes EMA estimate more accurate, especially during its initial phase. The EMA is very low when the iteration just starts off and we will get a much lower value that is not a very good estimate of the first data. In order to fix the problem, we could instead of taking $S_t$, take $\\frac{S_t}{1-(1-a)^t}$, namely bias correction. It will make the EMA larger when $t$ is small and make almost no difference when $t$ is large enough. Weight NormalizationThe Limitations of Batch NormalizationThe key limitation of batch normalization is that it is dependent on the mini-batch. It puts a lower limit on the batch size. Ideally, we want to use the global mean and variance to normalize the inputs to a layer. However, it’s too costly and the mean and variance are simply estimates on mini-batch, which means that they contain a certain amount of noises. Smaller mini-batch sizes increase the variance of these estimates. It can be a problem in settings such as online learning and reinforcement learning which is highly sensitive to noise. It is difficult to apply to recurrent connections in recurrent neural network. In a recurrent neural network, the recurrent activations of each time-step will have different statistics. This means that we have to fit a separate batch normalization layer for each time-step and it forces us to store the statistics for each time-step during training. Theory of Weight Normalization What weight normalization does is it separates the norm of the weight vector from its direction. In an effort to speed up the convergence of optimization procedure, it proposes to reparameterize each weight vector $\\omega$ in terms of a parameter vector $\\boldsymbol{v}$ and a scalar parameter $g$ in the following way: $$\\omega = \\frac{g}{||\\boldsymbol{v}||}\\boldsymbol{v}$$It then optimizes both$\\boldsymbol{v}$ and $g$ using gradient descent. This reparameterization has the effect of fixing the Euclidean norm of the weight vector $\\omega$: we now have $||\\omega||=g$, independent of the parameters $\\boldsymbol{v}$. We, therefore, call this reparameterization weight normalization. Layer NormalizationTheory of Layer Normalization The key feature of layer normalization is that it normalizes the inputs across the features. The equations of layer normalization and batch normalization are similar:$$\\hat{x}_i^{(k)} = \\frac{x_i^{(k)}-\\mu_L}{\\sqrt{\\sigma_L^2+\\epsilon}}$$where $\\mu_L$ and $\\sigma_L^2$ are the mean and variance across the features, respectively. ReferenceQuora: Why do we normalize the dataCoursera: Deep Learning SpecializationBlog: An Intuitive Explanation of Why Batch Normalization Really WorksBlog: Weight Normalization and Layer Normalization ExplainedBlog: An Overview of Normalization Methods in Deep Learning","link":"/2019/10/14/normalization-in-deep-learning/"},{"title":"Optimization Algorithm in Deep Learning","text":"Gradient Descent Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point as following:$$w = w - \\eta \\nabla Q(w) $$ Batch Gradient Descent Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated. Batch gradient descent decreases update frequency but takes much more time per iteration. Batch gradient descent has a more stable cost gradient and the cost goes down on every iteration. However, it may result in premature convergence of the model to a local minimum. Batch gradient descent requires the entire training dataset in memory and available to the algorithm, which is memory-consuming. Stochastic Gradient Descent Stochastic gradient descent is a variation of the gradient descent algorithm that calculates the error and updates the model for each example in the training dataset. It can be regarded as a stochastic approximation of batch gradient descent optimization, since it replaces the actual gradient by an estimate. Stochastic gradient descent increases model update frequency and can result in faster learning on some problems. Stochastic gradient descent can be extremely noisy, which may cause the model error/cost to jump around (higher variance). The noisy update process can allow the model to avoid local minima, but it also makes it hard to settle on a minimum (oscillate around the region of the minimum). Stochastic gradient descent loses the speed up from vectorization (matrix computation). Mini-Batch Gradient Descent Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients. Mini-batch gradient descent seeks to find a balance between the robustness of batch gradient descent and the efficiency of stochastic gradient descent.$$w = w - \\eta \\nabla Q(w) = w - \\eta \\sum_{i=1}^n \\nabla Q_i(w)/n$$ The cost of mini-batch gradient descent should trend downwards with noises (oscillations). Since some easy mini-batches might have a lower cost, while hard mini-batches (with some mislabeled examples) might have a higher cost. The larger step size can be adopted compared to stochastic gradient descent since the gradient is more stable. The batch size is a hyper-parameter that can be tuned as a power of two that fits the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, and so on. title: Optimization Algorithm in Deep Learning date: 2019-10-17 00:00:00Gradient Descent Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point as following:$$\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n \\nabla F(\\mathbf{x}_n),\\ n \\ge 0$$ Batch gradient descent Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated. With batch gradient descent on every iteration you go through the entire training set and you’d expect the cost to go down on every single iteration. The decreased update frequency results in a more stable error gradient and may result in a more stable convergence on some problems Fewer updates to the model means this variant of gradient descent is more computationally efficient than stochastic gradient descent. The more stable error gradient may result in premature convergence of the model to a less optimal set of parameters. Commonly, batch gradient descent is implemented in such a way that it requires the entire training dataset in memory and available to the algorithm.Model updates, and in turn training speed, may become very slow for large datasets. As stochastic gradient descent won’t ever converge, it’ll always just kind of oscillate and wander around the region of the minimum. But it won’t ever just head to the minimum and stay there. In practice, the mini-batch size you use will be somewhere in between. Stochastic Gradient Descent Stochastic gradient descent is a variation of the gradient descent algorithm that calculates the error and updates the model for each example in the training dataset. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). But a huge disadvantage to stochastic gradient descent is that you lose almost all your speed up from vectorization.Because, here you’re processing a single training example at a time. The way you process each example is going to be very inefficient. Mini-batch gradient descentAnd It’s not guaranteed to always head toward the minimum but it tends to head more consistently in direction of the minimum than the consequent descent. And then it doesn’t always exactly convert or oscillate in a very small region. If that’s an issue you can always reduce the learning rate slowly. Anything from 64 up to maybe 512 are quite typical. And because of the way computer memory is layed out and accessed, sometimes your code runs faster if your mini-batch size is a power of 2. One last tip is to make sure that your mini batch。 All of your X{t}, Y{t} that that fits in CPU/GPU memory. Try a few different powers of two and then see if you can pick one that makes your gradient descent optimization algorithm as efficient as possible. algorithm called momentumGradient descent with momentum gradient descent with momentum In one sentence, the basic idea is to compute an exponentially weighted average of your gradients, and then use that gradient to update your weights instead. So what this does is smooth out the steps of gradient descent. If you average out these gradients, you find that the oscillations in the vertical direction will tend to average out to something closer to zero. So, in the vertical direction, where you want to slow things down, this will average out positive and negative numbers, so the average will be close to zero. you find that the gradient descent with momentum ends up eventually just taking steps that are much smaller oscillations in the vertical direction, but are more directed to just moving quickly in the horizontal direction. And so this allows your algorithm to take a more straightforward path, or to damp out the oscillations in this path to the minimum. They kind of minimize this type of bowl shaped function then these derivative terms you can think of as providing acceleration to a ball that you’re rolling down hill. And these momentum terms you can think of as representing the velocity.And data, because this number a little bit less than one, displays a row of friction and it prevents your ball from speeding up without limit. The most common value for Beta is 0.9. We’re averaging over the last ten days temperature. So it is averaging of the last ten iteration’s gradients. Well, and how about bias correction, right? So do you want to take vdW and vdb and divide it by 1 minus beta to the t. In practice, people don’t usually do this because after just ten iterations, your moving average will have warmed up and is no longer a bias estimate. So in practice, I don’t really see people bothering with bias correction when implementing gradient descent or momentum. RMSprop RMSprop, which stands for root mean square prop. So for clarity, this squaring operation is an element-wise squaring operation. So what this is doing is really keeping an exponentially weighted average of the squares of the derivatives. So with this terms SdW an Sdb, what we’re hoping is that SdW will be relatively small, so that here we’re dividing by relatively small number. Whereas Sdb will be relatively large, so that here we’re dividing yt relatively large number in order to slow down the updates on a vertical dimension. And indeed if you look at the derivatives, these derivatives are much larger in the vertical direction than in the horizontal direction. So the net effect of this is that your up days in the vertical direction are divided by a much larger number, and so that helps damp out the oscillations. Whereas the updates in the horizontal direction are divided by a smaller number. And one effect of this is also that you can therefore use a larger learning rate alpha, and get faster learning without diverging in the vertical direction. (改变了相对比例) but your intuition is that in dimensions where you’re getting these oscillations, you end up computing a larger sum. A weighted average for these squares and derivatives, and so you end up dumping ] out the directions in which there are these oscillations. Just to ensure numerical stability, when you implement this in practice you add a very, very small epsilon to the denominator. 10 to the -8 would be a reasonable default, but this just ensures slightly greater numerical stability that for numerical round off or whatever reason, So that’s RMSprop, and similar to momentum, has the effects of damping out the oscillations in gradient descent, in mini-batch gradient descent. and allowing you to maybe use a larger learning rate alpha. And certainly speeding up the learning speed of your algorithm. Adam Adam In the typical implementation of Adam, you do implement bias correction. So, this algorithm has a number of hyper parameters. The learning with hyper parameter alpha is still important and usually needs to be tuned. So you just have to try a range of values and see what works. A common choice really the default choice for ß1 is 0.9. So this is a moving average, weighted average of dw right this is the momentum light term. The hyper parameter for ß2, the authors of the Adam paper, inventors of the Adam algorithm recommend 0.999. And then Epsilon, the choice of epsilon doesn’t matter very much. But the authors of the Adam paper recommended it 10 to the minus 8. Adam stands for Adaptive Moment Estimation. So ß1 is computing the mean of the derivatives. This is called the first moment. And ß2 is used to compute exponentially weighted average of the ²s and that’s called the second moment. ReferenceCoursera: Deep Learning SpecializationBlog: A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size","link":"/2019/10/16/optimization-algorithm-in-deep-learning/"}],"tags":[],"categories":[]}