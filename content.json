{"pages":[],"posts":[{"title":"Data Leakage","text":"Data leakage happens when your training data contains information about the target, but similar information will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production. There are two main types of leakage: target leakage and train-test contamination. Leaky PredictorsThis occurs when your predictors include data that will not be available at the time you make predictions. To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Train-Test ContaminationValidation is meant to be a measure of how the model does on data that it hasn’t considered before. You can corrupt this process if you leak data from the validation set into the training set. This is called train-test contamination. If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps (e.g. fitting an imputer for missing values, normalization) before train-test split. Negative examples as below: “In order to avoid problems with concept drift, I assigned articles to training or test alternating between the two. Performance on the test set was astonishingly good even when it was obvious that the model was over fitting. As it turned out, the data set had lots of near-duplicate articles so many articles in the training set also appeared in the test set. This duplication is coupled with an incautious way of splitting the training data wreaked havoc on my results.” – Ted Dunning from Quora “For example, there are 3 speakers and 500 recordings for each speaker. If we do a random split, our training and test set will share the same speaker saying the same words, which will boost our algorithm performance but fail when tested on a new speaker. The proper way to do it is to split the speakers, i.e., use 2 speakers for training and the third for testing.” – Dima Shulga from Medium ResolutionHere are several strategies to find and eliminate data leakage: Exploratory data analysis (EDA) can be a powerful tool for identifying data leakage. If the performance of your algorithm is too good to be true, data leakage may be the reason. Perform early in-the-field testing of algorithms. Any significant data leakage would be reflected as a difference between estimated and realized out-of-sample performance. This is perhaps the best approach in identifying data leakage, but it can also be challenging to isolate the cause of such performance discrepancy as data leakage since the cause actually could be classical over-fitting, sampling bias, etc. ReferenceBlog: Discriminative Modeling vs Generative ModelingKaggle: What is Data LeakageBlog: Ask a Data Scientist: Data Leakage","link":"/2019/11/09/data-leakage/"},{"title":"Google Cloud for Deep Learning - Part 1","text":"Google Cloud is a suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products. Google compute engine instance is a virtual machine (VM) hosted on Google Cloud where you could run our GPU-enabled deep learning project. What the most important is that a $300 trial credit is offered by google to try this cloud services! The series of passages aiming to use the Google Cloud for deep learning has two parts: The first part is about creating a VM instance on the Google cloud and the second part is about installing some deep learning related configurations (jupyter notebook and GPUs). Let’s take a step to the Part 1! Create a VM InstanceStep 1: Create a Google Account with CreditYou should activate your free trial to get $300 credit for the trial. Your payment information will be required but you won’t be charged unless you manually upgrade to a paid account. So don’t be too worried about the autocharge! Step 2 : Create a New ProjectClick the “project button“ in the top left cornor (right to the “Google Cloud Platform“ icon) to create a new project. Step 3: Create a VM InstanceClick on the “three lines button“ (menu) on the top left corner and then click on “Compute Engine“. There some tips for the new VM instance: Select a region with zone that has GPU resources, e.g. “us-west1-a”. Customize your machine type (CPU, GPU) as you want, which can be adjusted after the creation. Usually select “Ubuntu 16.04 LTS“ as the image in your boot disk. Tick both “http“ and “https“ under the firewall options to allow the network traffic. Expand the drop-down menu “Management, security, disks …“, choose the “disk“ tab and untick “Delete boot disk when instance is deleted“. You can also add an additonal disk if you want. IMPORTANT : DON’T FORGET TO STOP YOUR GPU INSTANCE, OTHERWISE GCP WILL KEEP CHARGING. Step 4: Request for Increase in QuotaYou probably get an error “Quota ‘GPUS_ALL_REGIONS’ exceeded. Limit: 0.0 globally“, which requires you to increase the quota. Click “Quotas“ in “IAM &amp; Admin“, send a quota increase request for “Compute Engine API GPUs (all regions)“ to at least set the limit to 1. An email will be sent automatically and your application will be processed in following days. If you want to use mutiple GPUs in one VM instance, you need to increase the quota for a specific kind of GPUs in the same zone as your VM instance as well, such as “Compute Engine API NVIDIA K80 GPUs“ in “us-west1-a”. Step 5: Connecting to the VM via SSHThree of those methods will be mentioned briefly. Method 1: SSH from the browser Method 2: Log in via SSH password verification Log in using method 1, switch to root and reset the passward: 12$ sudo -i $ sudo passwd root Edit the SSH configuration file: 12$ vim /etc/ssh/sshd_config# Eneter :wq to quit with change; Eneter :q! to quit without change and change things as following: 12PermitRootLogin yesPasswordAuthentication yes Restart SSH and activate new setting: 1$ service sshd restart Connect to Google Cloud using passward set above. 12$ ssh user@host# First time: Type yes if any notification host here should be the external-ip as below: Method 3 :Log in via the local private key file Generate SSH key file (in macOS): 1$ ssh-keygen -t rsa Copy the public key: 123$ cat ~/.ssh/id_rsa.pub# ...45UVp1 user@computer-name.local# Change user@computer-name.local as the user you want to login, e.g. root Paste it to “Compute Engine“ → “Metadata“, Google will write the public key to ~/.ssh/authorized_keys: Use the corresponding private key to login and enter passphrase for key: 1$ ssh -i ~/.ssh/id_rsa user@host Small Tips:Modifying Existing InstancesYou can modify resources (CPU/GPUs) on your existing instances, but you must first stop the instance and change its host maintenance. At the top of the instance details page, click “Stop“ to stop the instance. After the instance stops running, click “Edit“ to change the instance properties. ReferenceBlog: Running Jupyter Notebook on Google Cloud Platform in 15 minBlog: Mac使用SSH登录Google Cloud PlatformBlog: Google Cloud Platform SSH 连接配置","link":"/2019/10/17/google-cloud-for-deep-learning-part-1/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/08/22/hello-world/"},{"title":"Independent and Identically Distributed","text":"DefinitionIn probability theory and statistics, a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. In machine learning theory, i.i.d. assumption is often made for training datasets to imply that all samples stem from the same generative process ($y=f(\\mathbf{x})+\\varepsilon$), which means $\\left(\\mathbf{x}_{n}, y_{n}\\right)$ are i.i.d. samples according to $\\mathcal{D}$ ($S=\\left\\{ \\left(\\mathbf{x}_{n}, y_{n}\\right) \\text {i.i.d.} \\sim \\mathcal{D}\\right\\}_{n=1}^{N}$) and that the generative process is assumed to have no memory of past generated samples. Independent means the joint distribution can “splits up” into n individual marginals, one for each random variable:$$f_{X_{1}, \\ldots, X_{n}}\\left(x_{1}, \\ldots, x_{n}\\right)=f_{X_{1}}\\left(x_{1}\\right) \\times \\cdots \\times f_{X_{n}}\\left(x_{n}\\right)$$ Identical means they all have the same identical probability mass functions. Or in coin flip case, each random variable shares the same parameter p.$$f_{X_{1}}\\left(x_{1}\\right) \\times \\cdots \\times f_{X_{n}}\\left(x_{n}\\right)=\\left[p^{x_{1}}(1-p)^{1-x_{1}}\\right] \\times \\cdots \\times\\left[p^{x_{n}}(1-p)^{1-x_{n}}\\right]$$ ExplanationThe assumption (or requirement) that observations be i.i.d. tends to simplify the underlying mathematics of many statistical methods (e.g. maximizing the log-likelihood, minimizing the empirical risk, bootstrap aggregation, random splitting in model assessment, see more). In practical applications of statistical modeling, however, the assumption may or may not be realistic. To partially test how realistic the assumption is on a given data set, the correlation can be computed. In many problems, data is required to be sampled from the same distribution because it is hoped that the model trained with the training data set can be reasonably applied to the test set. Feature engineering is actually a way to remove noises and expose the latent mutual distribution. *Note about the Distribution (statistical model):In statistical classification, including machine learning, two main approaches are called the generative approach and the discriminative approach. Given an observable variable $X$ and a target variable $Y$, a generative model is a statistical model of the joint probability distribution on $X \\times Y$, $P(X, Y)$. (E.g. Naive Bayes, Gaussian Mixture Model) A discriminative model is a model of the conditional probability of the target $Y$, given an observation $x$, symbolically, $P(Y | X=x)$. (E.g. kNN, Logistic regression, SVM, Neural networks) Contrast: A generative algorithm models how the data was generated in order to categorize a signal. It predicts the most possible known label for the unknown variable using Bayes Rules (It tries to learn $p(X,Y)$ which can be transformed into $p(Y|X)$ later). A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal. So, discriminative algorithms try to learn $p(Y|X)$ directly from the data. ExamplesThe following are examples or applications of i.i.d. random variables: A sequence of fair or loaded dice rolls is i.i.d. A sequence of fair or unfair coin flips is i.i.d. The following examples data samples do not satisfy i.i.d. assumption: A medical dataset where multiple samples are taken from multiple patients, it is very likely that samples from same patients may be correlated. Samples drawn from time dependent processes, for example, year-wise census data. ReferenceStackExchange: On the importance of the i.i.d. assumption in statistical learningStackExchange: What does it mean by independently and identically distributed random variables? Zhihu: 独立同分布 independent and identically distributed","link":"/2019/11/06/independent-and-identically-distributed/"},{"title":"Google Cloud for Deep Learning - Part 2","text":"The series of passages to use the Google Cloud for deep learning has two parts: The first part is about creating a VM instance on the Google cloud and the second part is about installing some deep learning related configurations (jupyter notebook and GPUs). Now, Let’s move to the Part 2! Configure the Deep Learning EnviromentStep 1: Install GPU Drivers and CUDAAfter you create an instance with one or more GPUs, your system requires device drivers so that your applications can access the device. Install CUDA, which includes the NVIDIA driver. Following steps to install CUDA and the associated drivers for NVIDIA® GPUs. Run command line to create a script: 1$ vim installNvidia.sh Paste the following script to install CUDA: 12345678910111213#!/bin/bashecho \"Checking for CUDA and installing.\"# Check for CUDA and try to install.if ! dpkg-query -W cuda-10-0; then # The 16.04 installer works with 16.10. curl -O http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.0.130-1_amd64.deb dpkg -i ./cuda-repo-ubuntu1604_10.0.130-1_amd64.deb apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub apt-get update apt-get install cuda-10-0 -yfi# Enable persistence modenvidia-smi -pm 1 Run command line to execute the script: 1$ bash installNvidia.sh Verify that the driver installed and initialized properly. 1$ nvidia-smi The output of the command looks similar to the following: 123456789+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.92 Driver Version: 410.92 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla P100-PCIE... On | 00000000:00:04.0 Off | 0 || N/A 34C P0 26W / 250W | 0MiB / 16276MiB | 0% Default |+-------------------------------+----------------------+----------------------+ Step 2: Install cuDNN The NVIDIA CUDA® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. IMPORTANT: If you want to use pytorch with an NVIDIA GPU…We ship with everything in-built (pytorch binaries include CUDA, CuDNN, NCCL, MKL, etc.) When using pytorch, you just need to install the NVIDIA drivers and the binaries will come with the other libs. If you want to build from source, you would need to install CUDA, cuDNN etc. The cuDNN can be installed as the following: Check the version of your CUDA and download the corresponding version of cuDNN which requires a NVIDIA account. 1234# Check the version of CUDA$ cat /usr/local/cuda/version.txt# Check the version of cuDNN$ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 Download it to the local and copy it to the remote: Or use wget to download it. It’s a bit more tricky, you have to start the downloading and copy the downloading link from the browser downloader. Then decompress downloaded compressed file and get a folder cuda: 1$ tar -zxvf file Copy files to finish the installation 12$ sudo cp cuda/lib64/* /usr/local/cuda/lib64/$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ Step 3: Install Anaconda with Jupyter Notebook Copy the bash (.sh file) installer link: Use wget to download the bash installer: 1$ wget https://repo.continuum.io/archive/Anaconda3&lt;release&gt;.sh Run the bash script to install Anaconda3: 1$ bash Anaconda3-5.2.0-Linux-x86_64.sh source the .bashrc file to add Anaconda to your PATH. Now that Anaconda3 is added to PATH, source the .bashrcfile to load the new PATH environment variable. 1$ source ~/.bashrc To verify the installation is complete, open Python from the command line: 1$ python Step 4: Local Access to Jupyte Service Set firewall rules in “VPC network“ → “Firewall rules“ → “Create firewall rules“. Set Source IP ranges as 0.0.0.0/0 to allow all IPs to access. Set “Protocols and Ports“ to “tcp:port“, which the notebook server will listen on (e.g. 8888). Create a Jupyter configuration file if it doesn’t exist: 1$ jupyter notebook --generate-config Add a few lines to your Jupyter configuration file: 1$ vim .jupyter/jupyter_notebook_config.py 1234567c.NotebookApp.ip = '*'# The port the notebook server will listen on.c.NotebookApp.port = 8888# Whether to open in a browser after starting.c.NotebookApp.open_browser = False# Whether to allow the user to run the notebook as root.c.NotebookApp.allow_root = True Run the jupyter notebook: 12$ jupyter notebook# Then paste returned URL to the browser: http://host(static-ip):8888/?token=...ccf7bb Small Tips:Make External IP Address as StaticBy default, the external IP address is dynamic and we need to make it static to make our life easier. “VPC network“ →”External IP addresses“, and change the “Type“ from “Ephemeral“ to “Static“ Useful Script for Uninstallation12$ sudo apt-get --purge remove cuda # uninstall cuda $ sudo apt autoremove Create Conda Environment With conda, you can create, export, list, remove, and update environments that have different versions of Python and/or packages installed in them. 123# Create and activate the environment$ conda create --name myenv python=3.6$ source activate myenv Transfer Files Between the Local and RemoteUse scp command From the local to the remote: 1234$ scp local_file remote_username@remote_ip:remote_folder$ scp local_file remote_username@remote_ip:remote_file# Add -r parameter to move the entire folder$ scp -r local_folder remote_username@remote_ip:remote_folder From the remote to the local: just reverse the path of file. Use FTP application Use FileZilla® as an example, remember that you should use sftp://external-ip as the host. ReferenceDoc: Compute Engin DocmentationBlog: Running Jupyter Notebook on Google Cloud Platform in 15 minBlog: Installing Anaconda on LinuxBlog: Linux 教程","link":"/2019/10/21/google-cloud-for-deep-learning-part-2/"},{"title":"Model Ensemble","text":"Unity is strength. Ensemble methods combine multiple learning algorithms to obtain better predictive performance than any of the constituent learning algorithms alone. The main hypothesis is that when weak learning algorithms (weak learners) are correctly combined, we can obtain more accurate and/or robust models. Most of the time, these weak learners perform not so well by themselves either because they have a high bias or variance. Then, the idea of ensemble methods is to try reducing bias and/or variance of such weak learners by combining several of them to create a strong learner with better performances. Rule of ThumbHomogeneous weak learners: Bagging and boosting utilize a single base learning algorithm so that we have homogeneous weak learners trained in different ways (different data, different random state but same hyper-parameter). The ensemble model we obtain is then said to be “homogeneous”. Heterogeneous weak learners: Stacking uses different types of base learning algorithms. Some heterogeneous weak learners are then combined into a “heterogeneous” ensembles model. The choice of weak learners should be coherent with the way we aggregate these models. If we choose base models with low bias but high variance, it should be with an ensemble method that tends to reduce variance, and vice versa. BaggingBagging (bootstrap aggregating) is an ensemble method that often considers homogeneous weak learners and learns them independently from each other in parallel. It then combines weak learners following some deterministic averaging process as below: $$\\hat{f}_{b a g}=\\left\\{\\begin{array}{ll}{\\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}^{b}(x)} & {\\text { for Regression Problems }} \\\\ {\\underset{b=1 \\ldots B}{\\arg \\max } \\hat{f}^{b}(x)} & {\\text { for Classification Problems }}\\end{array}\\right.$$ Bagging is designed to improve the stability and reduces variance to avoid overfitting. The base learner it contains is supposed to be the one with low bias but high variance. Why does bagging reduce the variance?Assume that $Y_{1}, Y_{2}, \\ldots, Y_{n}$ represent the predictions of each weak learner and they are random variables with variance $\\sigma^{2}$. If each weak learner is trained on independent training set drawn from the true unknown underlying distribution, then $Y_{1}, Y_{2}, \\ldots, Y_{n}$ are i.d.d.. Then the variance of their average (i.d.d. random variables) is: $$\\operatorname{Var}\\left(\\frac{Y_{1}+Y_{2}+\\ldots+Y_{n}}{n}\\right)=\\operatorname{Var}\\left(\\frac{Y_{1}}{n}\\right)+\\ldots+\\operatorname{Var}\\left(\\frac{Y_{n}}{n}\\right)=\\frac{\\sigma^{2}}{n}$$, which means that the variance decreases from $\\sigma^{2}$ to $\\frac{\\sigma^{2}}{n}$ (while the **average does not change**). However, there is one problem here - we do not have access to multiple training data sets. Instead, we can bootstrap by taking repeated samples from the initial training data set. BootstrappingIn statistics, bootstrapping is any test or metric that relies on random sampling with replacement on the initial dataset. There two hypotheses proposed in Bootstrapping. First, the size $N$ of the initial dataset should be large enough to capture most of the complexity of the true unknown underlying distribution so that sampling from the dataset is a good approximation of sampling from the real distribution (representativity). Second, the size $N$ of the dataset should be large enough compared to the size $B$ of the bootstrap samples so that samples are not too much correlated (independence). Under these hypotheses above, these bootstrapped training data sets can be seen as being drawn both directly from the true unknown underlying distribution. So, they can be considered as approximately representative and independent (almost i.i.d.) samples of the true data distribution. Out-of-Bag (OOB) ErrorOOB error is the mean prediction error on each training sample $x_i$, using only the weak learners that did not have $x_i$ in their bootstrap samples. One big advantage of bagging is that we can compute testing error without any cross-validation by computing OOB error. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation. Bagging in tree modelBagging works particularly well for decision trees when they are aggregated as Random forest. BoostingBoosting is an ensemble method that often considers homogeneous weak learners and learns them from previously learned weak learners in sequential. Boosting is designed to improve the stability and reduces bias to avoid underfitting. The base learner it contains is supposed to be the one with low variance but high bias. Why does bagging reduce the bias?In contrast, boosting does not involve bootstrapping. Instead, the weak learner is sequentially fitted on a modified (weighted) version of the original data set. The fitting process is adaptative: each learner is fitted giving more importance to observations in the dataset that were badly handled by the previous learners. Intuitively, each new learner focus its efforts on the most difficult observations to fit up to now, so that we obtain, at the end, a strong learner with lower bias. Boosting in tree modelBoosting works particularly well for decision trees when they are aggregated as Boosted trees. StackingStacking (also called meta ensembling) is a model ensembling technique that often considers heterogeneous weak learners and learns them independently from each other in parallel. Stacked model will outperform each of the individual model due its ability to highlight each base model where it performs best and discredit each base model where it performs poorly. For this reason, stacking is most effective when the base models are significantly different. So intuitively, stacking will ideally reduce both bias and variance, but it’s especially efficient at preventing overfitting and variance. Algorithm First, all of weak learners are trained using the available data. Then, a combiner algorithm (meta-model) is trained to make a final prediction using all the predictions of the weak learners as inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques. In practice, a logistic regression model is often used as the combiner. When we train stacked models: In order to prevent data leakage, we should make sure that the data having been used for the training of the weak learners will not be used again for the training of the meta-model. And to make more efficient use of data, k-folds cross-training approach can be adopted to train the model, which can be shown as the figure below. Train each weak learner on $k-1$ folds and make predictions on the remaining fold, and do it iteratively to obtain predictions for all observations. Then train our meta-model on all these predictions. Multi-levels StackingA possible extension of stacking is multi-level stacking. It consists in doing stacking with multiple layers. We should also mention that adding levels can either be data expensive (if k-folds like technique is not used) or time expensive (if k-folds like technique is used). ReferenceBlog: Ensemble methods: bagging, boosting and stackingKaggle: Stacked Regressions to predict House PricesKaggle: Introduction to Ensembling/Stacking in PythonGuide to Model Stacking (i.e. Meta Ensembling)","link":"/2019/11/29/model-ensemble/"},{"title":"Model Selection & Bias–Variance Tradeoff","text":"In this passage, I summarize some basic concepts in Machine Learning, which mainly include model validation methods in model selection and the interpretation in bias–variance tradeoff. Probabilistic SetupIn the following, we set up a general probabilistic view on the data. Assume that the data is generated as (Data Generation Model) $$y=f(\\mathbf{x})+\\varepsilon$$, where $f$ is some arbitrary and unknown function and $\\mathbf{x}$ is generated according to some ﬁxed but unknown distribution $\\mathcal{D}_{\\mathbf{x}}$ . $\\varepsilon$ is an additive noise with distribution $\\mathcal{D}_{\\varepsilon}$, which is independent from sample to sample and independent from the data. Assume $\\varepsilon$ has zero mean (otherwise this constant can be absorbed into $f$). There is an unknown underlying joint distribution $\\mathcal{D}$ on pairs $(\\mathbf{x}, y)$, with range $\\mathcal{X} \\times \\mathcal{Y}$. The data set $\\mathcal{S}$ we see consists of independent and identically distributed(i.i.d.) samples from $\\mathcal{D}$: $$S=\\left\\{ \\left(\\mathbf{x}_{n}, y_{n}\\right) \\text {i.i.d.} \\sim \\mathcal{D}\\right\\}_{n=1}^{N}$$ Write $f_{\\mathcal{S}} = \\mathcal{A}(\\mathcal{S})$, where $\\mathcal{A}$ denotes the learning algorithm, $f_{\\mathcal{S}}$ is the resulting prediction function or model which depends on the data subset $\\mathcal{S}$ we are given. If we want to indicate that $f_{\\mathcal{S}}$ also depends on hyper-parameters of the model, we can add a subscript to write $f_{\\mathcal{S, \\lambda}}$. Model Selection Model (actually hyper-parameters) is selected by the result of VALIDATION. Given a model $f_{\\mathcal{S}}$, how can we assess if $f_{\\mathcal{S}}$ is any good? Theoretically, the best way is to compute the expected error over all possible samples $(\\mathbf{x}, y)$ chosen from $\\mathcal{D}$: $$L_{\\mathcal{D}}(f_{\\mathcal{S}})=\\mathbb{E}_{\\mathcal{D}}[\\ell(y, f_{\\mathcal{S}}(\\mathbf{x}))]$$ , where $\\ell(\\cdot, \\cdot)$ is our loss function. The quantity $L_{\\mathcal{D}}(f_{\\mathcal{S}})$ is called generalization error. Training, Validation &amp; Test SetTraining Set: A set of data used for learning, to fit the weights of the model. We would use the training set to find the “optimal” weights with the back-propagation algorithm. (Get optimal training error) Validation Set: A set of data used to tune the hyper-parameters of the model. We would use the validation set to find the “optimal” hyper-parameters or determine a stopping point for the back-propagation algorithm. (Get optimal validation error) Test Set: A set of data used only to assess the performance of the fully-trained model. After assessing the final model on the test set, YOU MUST NOT tune the model any further! Why separate test and validation sets?The error estimate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model. Generalization Error V.S. Training ErrorGeneralization error is the quantity we are fundamentally interested in, but we cannot compute it since $\\mathcal{D}$ is unknown. Since we have given a data subset $\\mathcal{S}$, it is natural to compute the equivalent empirical quantity: $$L_{\\mathcal{S}}\\left(f_{\\mathcal{S}}\\right)=\\frac{1}{|\\mathcal{S}|} \\sum_{\\left(\\mathbf{x}_{n}, y_{n}\\right) \\in \\mathcal{S}} \\ell\\left(y_{n}, f_{\\mathcal{S}}\\left(\\mathbf{x}_{n}\\right)\\right)$$ This is often called the training error, but $L_{\\mathcal{S}}(f_{\\mathcal{S}})$ might not be close to $L_{\\mathcal{D}}(f_{\\mathcal{S}})$ for the sake of overﬁtting. Generalization Error V.S. Test ErrorValidating model on the same data we trained it on will result in overfitting, so we could split the data into a training and a test set (a.k.a. validation set), call them $\\mathcal{S}_{\\text {train}}$ and $\\mathcal{S}_{\\text {test}}$, respectively. From the training set, we learn the model $\\mathcal{S}_{\\text {train}}$ and then compute the error on the test set: $$L_{\\mathcal{S}_{\\text {test}}}(f_{\\mathcal{S}_{\\text {train}}})=\\frac{1}{\\left|\\mathcal{S}_{\\text {test }}\\right|} \\sum_{\\left(y_{n}, \\mathbf{X}_{n}\\right) \\in \\mathcal{S}_{\\text {test}}}\\ell\\left(y_{n}, f_{\\mathcal{S}_{\\text {train }}}\\left(\\mathbf{x}_{n}\\right)\\right)$$, where $L_{\\mathcal{S}_{\\text {test}}}(f_{\\mathcal{S}_{\\text {train}}})$ is called **test error (empirical error)**. Indeed, the expectation of test error equals generalization error: $$L_{\\mathcal{D}}\\left(f_{\\mathcal{S}_{\\text {train }}}\\right)=\\mathbb{E}_{\\mathcal{S}_{\\text {test }} \\sim \\mathcal{D}}\\left[L_{\\mathcal{S}_{\\text {test }}}\\left(f_{\\mathcal{S}_{\\text {train }}}\\right)\\right]$$ **But for a particular test set $\\mathcal{S}_{\\text {test}}$, how far are these apart?** According to lemma: Chernoff bound. Let $\\Theta_{1}, \\ldots, \\Theta_{N}$ be a sequence of i.i.d. random variables with mean $\\mathbb{E}[\\Theta]$ and range $[a, b]$. Then, for any $\\varepsilon&gt; 0$,$$\\mathbb{P}\\left[\\left|\\frac{1}{N} \\sum_{n=1}^{N} \\Theta_{n}-\\mathbb{E}[\\Theta]\\right|\\geq \\varepsilon \\right] \\leq 2 e^{-2N\\varepsilon^{2}(b-a)^2}$$ we claim that: $$\\mathbb{P}\\left[\\left|L_{\\mathcal{D}}(f)-L_{\\mathcal{S}_{\\text {test}}}(f)\\right| \\geq \\sqrt{\\left.\\frac{(b-a)^{2} \\ln (2 / \\delta)}{2\\left|\\mathcal{S}_{\\text {test }}\\right|}\\right)}\\right] \\leq \\delta$$, from which we could have the **insight** that the difference decreases as $\\mathcal{O}(1 / \\sqrt{\\left|S_{\\text {test }}\\right|})$. So the more data points we have, the more conﬁdent that the **empirical loss we measure is close to the true loss**. Cross Validation Splitting the data once into two parts (one for training and one for testing) is not the most efficient way to use the data. Cross-validation is a better way. Methods: See more K-fold Cross-Validation: Randomly partition the data into K groups. Each time leave out exactly one of the K groups for testing and use the remaining K-1 groups for training. Stratified K-fold: It is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set. Nested Cross-Validation: Always used for time series data, where particular care must be taken in splitting the data in order to prevent data leakage. It means to evaluate the model for time series data on the “future” observations. Nested Cross-Validation is a variation of k-fold which returns first k folds as train set and the (k+1) th fold as test set. Outcomes Cross-validation returns an unbiased estimate of the generalization error and its variance. We can average the K validation errors as the validation result. The purpose of cross-validation is model checking, not model building. Once we have used cross-validation to select the better performing model, we train that model on all the data. Cross-validation will not be used in complex deep learning model with a large dataset, since it’s too computationally expensive. Advantages Use All Data: Splitting the data once into two parts is not the most efficient way to use the data. It only gives predictions on part of the data (test set). If the test set we pick happens to contain a bunch of points that are particularly easy (or hard) to predict, we will not come up with a rational estimate of the model’s ability to learn and predict. Prevent Data Leakage: Work with Dependent/Grouped Data: For example, there are 3 speakers and 500 recordings for each speaker. If we do a random split, our training and test set will share the same speaker saying the same words, which will boost our algorithm performance but fail when tested on a new speaker. The proper way to do it is to split the speakers, i.e., use 2 speakers for training and the third for testing (cross-validation on the speakers level). Cross-validation can be used in models stacking, where we can’t train both our models on the same dataset because then, our second model will learn on ground truth information that our first model already seen. Bias–Variance Tradeoff The bias–variance tradeoff is the property of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples and vice versa. The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). Ideally, one wants to choose a model that both accurately captures the regularities in its training data (high variance), but also generalizes well to unseen data (high bias). There is an inherent bias-variance tradeoff when we perform the model selection. Error DecompositionFollowing Probabilistic Setup and test error defined above, we look at the mean squared error of the prediction function $f_{\\mathcal{S}}$ for a ﬁxed element $\\mathbf{x}_0$. Imagine that we are running the experiment many times: we create $\\mathcal{S}_{\\text {train }}$, learn the model $f_{\\mathcal{S}_{\\text {train }}}$, and then evaluate the performance by computing the mean squared error for $\\mathbf{x}_0$ ($y_0=f(\\mathbf{x}_0)+\\varepsilon$). $$ \\begin{split} & \\mathbb{E}_{\\mathcal{S}_{\\text {train }} \\sim \\mathcal{D}, \\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)+\\varepsilon-f_{\\mathcal{S}_{\\text {train }}}\\left(\\mathbf{x}_{0}\\right)\\right)^{2}\\right] \\\\ = &\\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]+\\mathbb{E}_{\\mathcal{S}_{\\text {train } \\sim \\mathcal{D}}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-f_{\\mathcal{S}_{\\text {train }}\\left(\\mathbf{x}_{0}\\right)}\\right)^{2}\\right] \\\\ &+2\\cdot \\underbrace{\\mathbb{E}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]}_{0}\\cdot\\mathbb{E}_{\\mathcal{S}_{\\text {train } \\sim \\mathcal{D}}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-f_{\\mathcal{S}_{\\text {train }}\\left(\\mathbf{x}_{0}\\right)}\\right)\\right] \\\\ = &\\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]+\\mathbb{E}_{\\mathcal{S}_{\\text {train } \\sim \\mathcal{D}}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-f_{\\mathcal{S}_{\\text {train }}\\left(\\mathbf{x}_{0}\\right)}\\right)^{2}\\right] \\\\ = & \\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]+\\mathbb{E}_{\\mathcal{S} \\sim \\mathcal{D}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-\\mathbb{E}_{\\mathcal{S}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]\\right) + \\left(\\mathbb{E}_{\\mathcal{S}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]-f_{\\mathcal{S}}\\left(\\mathbf{x}_{0}\\right)\\right)^2\\right] \\\\ =& \\underbrace{\\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}[\\varepsilon]}_{\\text {noise variance }} \\\\ & + \\underbrace{\\left(f\\left(\\mathbf{x}_{0}\\right)-\\mathbb{E}_{\\mathcal{S}_{\\text {train }}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}_{\\text {train }}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]\\right)^{2}}_{\\text {bias }}\\\\ & + \\mathbb{E}_{\\mathcal{S}_{\\text {train }} \\sim \\mathcal{D}}[\\underbrace{\\left(\\mathbb{E}_{\\mathcal{S}_{\\text {train }}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}_{\\text {train }}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]-f_{\\mathcal{S}_{\\text {train }}}\\left(\\mathbf{x}_{0}\\right)\\right)^{2}}_{\\text { variance }}] \\end{split} $$ Each of the three terms above is nonnegative, so each of them is a lower bound on the true error for the input $\\mathbf{x}_0$. $\\varepsilon$ is an irreducible error. The bias term is the square of the difference between the actual value $f\\left(\\mathbf{x}_{0}\\right)$ and the expected prediction $\\mathbb{E}_{S^{\\prime} \\sim \\mathcal{D}}\\left[f_{S^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]$. The variance term is the variance of the prediction function. Approaches Simple model, more data, less features, underfitting → Large bias Complex model, less data, more features, overfitting → Large variance Dimensionality reduction and feature selection can decrease variance by simplifying models. Learning algorithms typically have some tunable hyper-parameters that control bias and variance. For example: Linear and generalized linear models can be regularized to decrease variance at the cost of increasing bias. (It’s the essence of regulation to avoid overfitting) In artificial neural networks, the variance increases and the bias decreases as the number of hidden units increase. In k-nearest neighbor models, a high value of k (simple model) leads to high bias and low variance. In decision trees, the depth of the tree determines the variance. Ensemble learning is also one way of resolving the trade-off. For example, boosting combines many “weak” (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines “strong” learners in a way that reduces their variance. ReferenceCourse: Machine Learning@EPFLWikipedia: Bias–variance tradeoffBlog: 5 Reasons why you should use Cross-Validation in Data Science ProjectsStack Exchange: How to choose a predictive model after k-fold cross-validation?Zhihu: 求大神通俗易懂的解释下交叉验证cross-validation？","link":"/2019/10/31/model-selection-&-bias–variance-tradeoff/"},{"title":"Normalization in Deep Learning","text":"Normalization refers to a process that makes something more normal or regular. In statistics, normalization means adjusting values measured on different scales to a common scale, or adjustments to bring the entire probability distributions of adjusted values into alignment. Normalization in deep learning can make the model to train efficiently. Why does Normalization work?Intuition 1: Input Normalization $\\frac{X - \\mu}{\\sigma}$ Make training less sensitive to the scale of features. After optimization, the feature with a large scale will always be weighted with a small value ($\\omega_{small}$), and vise versa ($\\omega_{large}$). Because of the large scale of the feature, a tiny change in the small weight ($\\omega_{small}$) will change the prediction by a lot compared to the same change in the large weight ($\\omega_{large}$). It means that setting $\\omega_{small}$ correctly might dominate the optimization process and the feature with a large scale is of more importance which actually makes no sense. Make optimization well-conditioned. The rough intuition is that the cost function will be more round and easier to optimize when features are all on similar scales. Cost function of the unnormalized features can be a very elongated bowl, whereas if we normalize the features, then the cost function will look more symmetric. Having an elongated cost function, we might have to use a very small learning rate and gradient descent might need a lot of steps to oscillate back and forth before it finally gets to the minimum. If we have more spherical contours, then gradient descent can pretty much go straight to the minimum and we can take much larger steps with gradient descent without much oscillation. Intuition 2: Covariate Shift Covariate shift refers to the problem that the distribution of the input values changes, but the concept (model) being learned remains stationary. In deep learning, the basic idea behind normalization is to limit the covariate shift, which allows the model to learn on a more stable distribution, and would thus accelerate the training of the network. Batch NormalizationIn deep learning, we are particularly concerned with the change in the distribution of the inputs to the hidden nodes within a network. A neural network changes the weights of each layer over the training, which means that the activations of each layer change as well. Since the activations of a previous layer are the inputs of the next layer, each layer in the neural network is faced with a situation where the input distribution changes with each step (covariate shift). Batch Normalization at Training Time What batch normalization does is, especially from the perspective of the later layers of the neural network, it limits the earlier layers to not get to shift around much, by restricting them to have the same mean and variance. It weakens the coupling between what the early layers and the later layers. For a layer of the network with d-dimensional input, $x = (x^{(1)},…,x^{(d)})$, each dimension is then normalized separately as following: $$\\hat{x}_i^{(k)} = \\frac{x_i^{(k)}-\\mu_B^{(k)}}{\\sqrt{\\sigma_B^{(k)^2}+\\epsilon}}$$where $\\mu_B^{(k)}$ and $\\sigma_B^{(k)^2}$ are the per-dimension mean and variance, respectively. $\\epsilon$ is added in the denominator for numerical stability and is an arbitrarily small constant. The resulting normalized $\\hat{x}^{(k)}$ have zero mean and unit variance, to restore the representation power of the network or better take advantage of the nonlinearity (if activations are normalized), a transformation step then follows as:$$y_i^{(k)} = \\gamma^{(k)} \\hat{x}_{i}^{(k)} +\\beta^{(k)}$$where the parameters $\\gamma^{(k)}$and $\\beta^{(k)}$ are subsequently learnt in the optimization process and can convert the mean and variance to any value that the network desires. Formally, the transform $BN_{\\gamma,\\beta}: x_{1…m} \\rightarrow y_{1…m}$ is denoted as the Batch Normalizing Transform. There are some debates in deep learning about whether weshould normalize the value before the activation function or after it. In practice, normalizing before the activation function is done much more often. Regularization can be motivated as a technique to improve the generalizability of a learned model. Batch normalization has a slight regularization effect. The mean and variance are a little bit noisy because they are estimated on a mini-batch. Similar to dropout, batch normalization adds small noise to the hidden layers and therefore has a very slight regularization effect. We don’t particularly use to batch normalization as a regularization but use it as a way to speed up learning. Batch Normalization at Test Time At the test time, we use the $\\gamma$ and $\\beta$ learned from training, the $\\mu$ and $\\sigma^2$ are estimated from the training set. Running the whole training set to get $\\mu$ and $\\sigma^2$ could be memory-consuming. So we keep a running average of the $\\mu$ and $\\sigma^2$ for each layer as we train the neural network across different mini-batches to get an estimation of them. In practice, the moving average people usually use here is exponentially weighted average. An exponential moving average (EMA), also known as an exponentially weighted moving average (EWMA), is a first-order filter that applies weighting factors which decrease exponentially. The EMA for a series $Y$ is calculated recursively: $$S_t = \\begin{cases} \\alpha \\cdot Y_1, &amp; t = 1 \\\\ \\alpha \\cdot Y_t + (1 - \\alpha) \\cdot S_{t-1}, &amp; t &gt; 1\\end{cases}$$Where: $\\alpha$ represents the degree of weighting decrease, a constant smoothing factor between 0 and 1. $Y_t$ and $S_t$ are the values at a time period $t$ respectively. This formula can also be expressed in technical analysis terms as follows: How the EMA steps towards the latest datum point:$$\\text{EMA}_\\text{today} = \\text{EMA}_\\text{yesterday} + \\alpha \\left[\\text{price}_\\text{today} - \\text{EMA}_\\text{yesterday}\\right]$$ How the weighting factor on each data point $p_1$, $p_2$, etc., decreases exponentially by expanding out $\\text{EMA}_\\text{yesterday}$ :$$\\begin{split}\\text{EMA}_\\text{today}= &amp; { \\alpha \\left[p_1 + (1 - \\alpha) p_2 + (1 - \\alpha)^2 p_3 + (1 - \\alpha)^3 p_4 + \\cdots \\right] } \\\\= &amp;\\frac{p_1 + (1 - \\alpha) p_2 + (1 - \\alpha)^2 p_3 + (1 - \\alpha)^3 p_4 + \\cdots}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + (1 - \\alpha)^3 + \\cdots}\\end{split}$$since $1/\\alpha = 1 + (1 - \\alpha) + (1 - \\alpha)^2 + \\cdots$ (weighted average). A couple of things about choosing $\\alpha$: A higher $\\alpha$ discounts older observations faster so that the EMA will adapt more quickly to the current changes. When compute EMA, we can think of $S_t$ as approximately averaging over $\\frac{1}{\\alpha}$ number of data. So with a low value of $\\alpha$, the EMA is much smoother and also has more latency. (Always set to 0.1 in deep learning) Bias correction is a technical modification that makes EMA estimate more accurate, especially during its initial phase. The EMA is very low when the iteration just starts off and we will get a much lower value that is not a very good estimate of the first data. In order to fix the problem, we could instead of taking $S_t$, take $\\frac{S_t}{1-(1-a)^t}$, namely bias correction. It will make the EMA larger when $t$ is small and make almost no difference when $t$ is large enough. Weight NormalizationThe Limitations of Batch NormalizationThe key limitation of batch normalization is that it is dependent on the mini-batch. It puts a lower limit on the batch size. Ideally, we want to use the global mean and variance to normalize the inputs to a layer. However, it’s too costly and the mean and variance are simply estimates on mini-batch, which means that they contain a certain amount of noises. Smaller mini-batch sizes increase the variance of these estimates. It can be a problem in settings such as online learning and reinforcement learning which is highly sensitive to noise. It is difficult to apply to recurrent connections in recurrent neural network. In a recurrent neural network, the recurrent activations of each time-step will have different statistics. This means that we have to fit a separate batch normalization layer for each time-step and it forces us to store the statistics for each time-step during training. Theory of Weight Normalization What weight normalization does is it separates the norm of the weight vector from its direction. In an effort to speed up the convergence of optimization procedure, it proposes to reparameterize each weight vector $\\omega$ in terms of a parameter vector $\\boldsymbol{v}$ and a scalar parameter $g$ in the following way: $$\\omega = \\frac{g}{||\\boldsymbol{v}||}\\boldsymbol{v}$$It then optimizes both$\\boldsymbol{v}$ and $g$ using gradient descent. This reparameterization has the effect of fixing the Euclidean norm of the weight vector $\\omega$: we now have $||\\omega||=g$, independent of the parameters $\\boldsymbol{v}$. We, therefore, call this reparameterization weight normalization. Layer NormalizationTheory of Layer Normalization The key feature of layer normalization is that it normalizes the inputs across the features. The equations of layer normalization and batch normalization are similar:$$\\hat{x}_i^{(k)} = \\frac{x_i^{(k)}-\\mu_L}{\\sqrt{\\sigma_L^2+\\epsilon}}$$where $\\mu_L$ and $\\sigma_L^2$ are the mean and variance across the features, respectively. ReferenceQuora: Why do we normalize the dataCoursera: Deep Learning SpecializationBlog: An Intuitive Explanation of Why Batch Normalization Really WorksBlog: Weight Normalization and Layer Normalization ExplainedBlog: An Overview of Normalization Methods in Deep Learning","link":"/2019/10/14/normalization-in-deep-learning/"},{"title":"Optimization Algorithm in Deep Learning","text":"An optimization algorithm is a procedure which is executed iteratively by comparing various solutions until an optimum or a satisfactory solution is found. Here, we introduce some optimization algorithms commonly used in deep learning. Gradient Descent Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point as following:$$w = w - \\eta \\nabla Q(w) $$ Batch Gradient Descent Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated. Batch gradient descent decreases update frequency but takes much more time per iteration. Batch gradient descent has a more stable cost gradient and the cost goes down on every iteration. However, it may result in premature convergence of the model to a local minimum. It is an estimate of the function gradient. Batch gradient descent requires the entire training dataset in memory and available to the algorithm, which is memory-consuming. Stochastic Gradient Descent Stochastic gradient descent is a variation of the gradient descent algorithm that calculates the error and updates the model for each example in the training dataset. It can be regarded as a stochastic approximation of batch gradient descent optimization, since it replaces the actual gradient by an estimate. Stochastic gradient descent increases model update frequency and can result in faster learning on some problems. Stochastic gradient descent can be extremely noisy, which may cause the model error/cost to jump around (higher variance). It is an unbiased estimate of the full gradient. The noisy update process can allow the model to avoid local minima, but it also makes it hard to settle on a minimum (oscillate around the region of the minimum). Stochastic gradient descent loses the speed up from vectorization (matrix computation). Mini-Batch Gradient Descent Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients. Mini-batch gradient descent seeks to find a balance between the robustness of batch gradient descent and the efficiency of stochastic gradient descent.$$w = w - \\eta \\nabla Q(w) = w - \\eta \\sum_{i=1}^n \\nabla Q_i(w)/n$$ The cost of mini-batch gradient descent should trend downwards with noises (oscillations). Since some easy mini-batches might have a lower cost, while hard mini-batches (with some mislabeled examples) might have a higher cost. The larger step size can be adopted compared to stochastic gradient descent since the gradient is more stable. The batch size is a hyper-parameter that can be tuned as a power of two that fits the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, and so on. Adaptive Learning Rate DescentAdaGrad AdaGrad (adaptive gradient descent) algorithm is a modified stochastic gradient descent with per-parameter learning rate. AdaGrad has a base learning rate $\\eta$, but this is multiplied with the elements of a vector ($G_{j, j}$) which is the diagonal of the matrix:$$G = \\sum_{\\tau=1}^t g_\\tau g_\\tau^T$$ where $g_\\tau = \\nabla Q_i(w)$, the stochastic gradient at iteration $\\tau$. The diagonal is given by $G_{j,j} = \\sum_{\\tau=1}^t g_{\\tau,j}^2$. The formula for an update is now:$$w = w - \\eta, diag(G)^{-\\frac{1}{2}} \\circ g$$, where the $\\circ$ is the element-wise product. It can also be written as per-parameter updates $w_j = w_j - \\frac{\\eta}{\\sqrt{G_{j,j}}} g_j.$ AdaGrad increases the learning rate for sparser parameters and decreases the learning rate for ones that are less sparse. This strategy often improves convergence performance over sparse data. The intuition behind is that: with the number of updates increases, it is getting closer to the optimal solution, the learning rate supposed to be down to avoid oscillations. In AdaGrad, the learning rate will shrink with large updates or frequent updates.RMSProp RMSProp (Root Mean Square Propagation) is also a method in which the learning rate is adapted for each of the parameters. The idea is to divide the learning rate by an exponentially weighted average (detailed described in my previous blog) of the magnitudes of recent gradients. The formula for an update is shown below:$$\\begin{cases}v_w=\\gamma v_w+(1-\\gamma)(\\nabla Q(w))^2 \\\\w=w-\\frac{\\eta}{\\sqrt{v_w+\\epsilon}}\\nabla Q(w)\\end{cases}$$ where, $\\gamma$ is the forgetting factor, $(\\nabla Q(w))^2$ is component-wise squared and $\\epsilon$ is set to ${10}^{-8}$ to avoid blow up. The intuition of RMSProp is quite similar to Adagrad. The advantage of using the moving average instead of the accumulation of $(\\nabla Q(w))^2$ is that the learning rate can not only be gradually adapted smaller but larger if the feature becomes sparse in recent updates. Gradient Descent with Momentum Stochastic gradient descent with momentum remembers the update $\\Delta \\omega$ at each iteration, and determines the next update as a linear combination of the gradient and the previous update. In one sentence, the basic idea is to compute an exponentially weighted average of your gradients, and then use that gradient to update your weights instead. Stochastic gradient descent with momentum remembers the update $w$ at each iteration:$$\\begin{cases}{m_{w}=\\beta m_{w}+(1-\\beta) \\nabla Q(w)} \\\\ \\tilde{m}_ w = \\frac{m_{w}}{1-\\beta^{t}} \\\\{w=w-\\alpha \\tilde{m}_{w}}\\end{cases}$$where $t$ is the iteration, $\\alpha$ is the learning rate, $\\beta$ is the forgetting factor always set to 0.9 (average approximately over last 10 gradients). The name momentum stems from an analogy to momentum in physics. Unlike in classical stochastic gradient descent, it tends to keep traveling in the same direction, preventing oscillations. Momentum averages out oscillations of its gradients. The oscillations in some directions will tend to be averaged out and the gradients will get closer to zero. Adam Adam (Adaptive Moment Estimation ) combines RMSprop together with momentum and is an update to the RMSProp optimizer. Given parameters $w^{(t)}$ and a loss function $Q^{(t)}$, where $t$ indexes the current training iteration (indexed at 0), Adam’s parameter update is given by: $$\\begin{cases}m_w ^ {(t+1)} \\leftarrow \\beta_1 m_w ^ {(t)} + (1 - \\beta_1) \\nabla _w Q ^ {(t)}\\\\v_w ^ {(t+1)} \\leftarrow \\beta_2 v_w ^ {(t)} + (1 - \\beta_2) (\\nabla _w Q ^ {(t)} )^2\\\\\\hat{m}_w = \\frac{m_w ^ {(t+1)}}{1 - (\\beta_1) ^{t+1}}\\\\\\hat{v}_w = \\frac{ v_w ^ {(t+1)}}{1 - (\\beta_2) ^{t+1}}\\\\w ^ {(t+1)} \\leftarrow w ^ {(t)} - \\eta \\frac{\\hat{m}_w}{\\sqrt{\\hat{v}_w} + \\epsilon}\\end{cases}$$ where $\\epsilon$ is a small scalar used to prevent division by 0, and $\\beta_1$ and $\\beta_2$ are the forgetting factors for gradients and second moments of gradients, respectively. Squaring and square-rooting is done elementwise. Learning rate $\\eta$ is important and usually we need to try a range of values and see what works. A common choice for $\\beta_1$ is 0.9 and the hyper parameter $\\beta_2$ is recommend 0.999. $\\eta$ is always set to $10^{-8}$ and bias correction is implemented.Second-Order Methods… to be continued.ReferenceWiki: Stochastic gradient descentCoursera: Deep Learning SpecializationBlog: A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size","link":"/2019/10/16/optimization-algorithm-in-deep-learning/"},{"title":"Tree-Based Learning Algorithms - Part 1","text":"Tree-based models consist of decision tree and other approaches involving producing multiple trees that are combined to yield a single and better consensus prediction. These models partition the feature space into a set of polyhedrons, and then fit a simple model (like a constant) in each one. Unlike linear models, they map non-linear relationships quite well. They are adaptable at solving both classification or regression. In this post, we will look at the mathematical details and parameter explanations of several tree-based models, including Decision tree, Random forest, Boosted trees. Decision TreeA decision tree is a flowchart-like structure in which: Each internal node represents a “test” on an attribute. Each branch represents the outcome of the test. Each leaf node represents a class label (decision taken after computing all attributes). The paths from the root to the leaf represent classification rules. Decision Tree Learning Decision tree learning is the construction of a decision tree from class-labeled training tuples. There are many specific decision-tree algorithms, and notable ones include ID3, C4.5, CART. Algorithms for constructing decision trees is greedy which: Usually, work top-down by choosing a variable at each step that best splits the set of items. This process is repeated on each derived subset in a recursive manner (Depth First). The recursion is completed when the subset at a node reaches a predefined stopping criterion. According to the above, there will naturally be a question: What are the best splits? Different algorithms use different metrics for measuring “best”. These generally measure the homogeneity of the target variable within the subsets. Variance reductionIntroduced in CART, variance reduction is often employed in cases where the target variable is continuous (regression tree). The variance reduction of a node $N$ is defined as the total reduction of the variance of the target variable $x$ due to the split at this node: $$\\begin{split} I_{V}(N)=&\\frac{1}{|S|^{2}} \\sum_{i \\in S} \\sum_{j \\in S} \\frac{1}{2}\\left(x_{i}-x_{j}\\right)^{2}\\\\&-\\underbrace{\\left(\\frac{1}{\\left|S_{t}\\right|^{2}} \\sum_{i \\in S_{t}} \\sum_{j \\in S_{t}} \\frac{1}{2}\\left(x_{i}-x_{j}\\right)^{2}+\\frac{1}{\\left|S_{f}\\right|^{2}} \\sum_{i \\in S_{f}} \\sum_{j \\in S_{f}} \\frac{1}{2}\\left(x_{i}-x_{j}\\right)^{2}\\right)}_{\\text {Minimum: Largest Homogeneity}}\\end{split}$$ , where $S$, $S_{t}$, and $S_{f}$ are the set of pre-split sample indices, set of sample indices for which the split test is true, and set of sample indices for which the split test is false, respectively. Gini impurityIntroduced in CART for classification trees, Gini impurity is a measure of how often a randomly chosen element ($p_{i}$) from the set would be incorrectly labeled if it was randomly labeled according to the DISTRIBUTION of labels in the subset ($1-p_{i}$). The Gini impurity can be computed by summing the probability $p_{i}$ of an item with label $i$ being chosen times the probability $1-p_{i}$ of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category: $$\\operatorname{Gini}(T)=\\sum_{i=1}^{J} p_{i} \\sum_{k \\neq i} p_{k}=\\sum_{i=1}^{J} p_{i}\\left(1-p_{i}\\right) $$, where $J$ is the amount of classes and $i\\in \\{1,2,...,J\\}$. And the Gini impurity reduction can be defined as below: $$\\begin{split}{IG(T, a)}&= \\underbrace{\\mathrm{Gini}(T)}_{\\text {Gini (parent) }}-\\quad \\underbrace{\\mathrm{Gini}(T | a)}_{\\text {Weighted Sum of Gini (Children) }} \\\\&=\\sum_{i=1}^{J} p_{i}\\left(1-p_{i}\\right)-\\sum_{a} p(a) \\sum_{i=1}^{J}-\\operatorname{P}(i | a) (1- \\operatorname{P}(i | a))\\end{split}$$ Information gainInformation gain is based on the concept of entropy in information theory, which we described in . Entropy is defined as below: $$\\mathrm{H}(T)=\\mathrm{I}_{E}\\left(p_{1}, p_{2}, \\ldots, p_{J}\\right)=-\\sum_{i=1}^{J} p_{i} \\log _{2} p_{i}$$, where $p_{1},p_{2},...$ are fractions that add up to 1 and represent the percentage of each class present in the child node. $$\\begin{split}{IG(T, a)}&= \\underbrace{\\mathrm{H}(T)}_{\\text {Entropy (parent) }}-\\quad \\underbrace{\\mathrm{H}(T | a)}_{\\text {Weighted Sum of Entropy (Children) }} \\\\&=-\\sum_{i=1}^{J} p_{i} \\log _{2} p_{i}-\\sum_{a} p(a) \\sum_{i=1}^{J}-\\operatorname{P}(i | a) \\log _{2} \\operatorname{P}(i | a) \\end{split}$$ CART was implemented in Scikit-learn - sklearn.tree.DecisionTreeClassifier and sklearn.tree.DecisionTreeRegressor. The parameters below are essential: max_features: The number of randomly chosen features to consider when looking for the best split. A low value can be set to control over-fitting. Since the algorithm is greedy, setting max_features &lt; n_features will try more splitting possibilities. max_depth, min_samples_split, … : These parameters resulting in a less complex tree (less nodes) can control over-fitting. Tree Pruning Decision-tree learners can create over-complex trees that do not generalize well from the training data. (Overfitting) Tree pruning is a technique that reduces the size of the decision tree by removing sections of the tree (substitute sub-trees with their leaf nodes) that provide little power to classify instances. Pruning reduces the complexity of the final decision tree, and hence improves predictive accuracy by the reduction of overfitting. There are two types of Pruning: Pre-pruning (also known as early stopping)Pre-pruning will stop the tree-building process early while constructing a decision tree. To avoid overfitting, if the error does not decrease significantly enough or the tree reaches its maximum depth (doesn’t meet the threshold), then we stop. However, early stopping may underfit by stopping too early. The current split may be of little benefit, but having made it, subsequent splits more significantly reduce the error. Post-pruning After the growth of the decision tree is completed, the tree can be post-pruned to be simplified. The process of post-pruning is to check each sub-tree that, if we substitute the sub-tree with their leaf nodes, whether the increase in impurity measure is less than the specified threshold. Post-pruning in CARTThe post-pruning in CART has two steps: Continuously prune the decision tree $T_0$ built by CART to generate a sub-tree sequence ${T_0, T_1, …, T_n}$. For each non-leaf node, you can compute its regularizied cost as below: $$C_{a}\\left(T_{t}\\right)=C\\left(T_{t}\\right)+a|T_t|$$, where $T_t$ is an arbitrary sub-tree whose root is $t$, $C(T_t)$ is the total impurity measure of the sub-tree, $\\alpha \\geq 0$ is the coefficient of the regularization term, $| T_t |$ is the number of leaf nodes of the sub-tree. If the decision tree is pruned at non-leaf node $t$, then $T_t$ turns into a single node tree whose regularizied cost becomes as below: $$C_{\\alpha}(t)=C(t)+\\alpha * 1$$ Only if the regularizied cost decreases ($C_{\\alpha}(t) \\leq C_{\\alpha}\\left(T_{t}\\right)$), then this sub-tree $T_t$ will be pruned, namely: $$C(t)+\\alpha \\leq C\\left(T_{t}\\right)+\\alpha\\left|T_{t}\\right|\\Rightarrow\\alpha \\geq \\frac{C(t)-C\\left(T_{t}\\right)}{\\left|T_{t}\\right|-1}$$ It means that for a non-leaf node $t$, the minimum regularization coefficient $\\alpha$ needed to prune $T_t$ is $\\alpha = \\frac{C(t)-C\\left(T_{t}\\right)}{\\left|T_{t}\\right|-1}$. We can iteratively compute the $\\alpha$ for each node in the last pruned tree and prune the node with the minimum $\\alpha$. We do it until only the root node $T_0$ is remained and finally will get the sub-tree sequence ${T_0, T_1, …, T_n}$. Test each subtree in the sequence of cross-validation subtrees, and select the optimal subtree as the final pruning result. ReferenceBlog: A Practical Guide to Tree-Based Learning AlgorithmsBlog: Machine Learning: Pruning Decision TreesZhihu: CART树怎么进行剪枝Blog: 决策树及其剪枝原理","link":"/2019/11/19/tree-based-learning-algorithms-part-1/"},{"title":"Tree-Based Learning Algorithms - Part 2","text":"Decision tree can be very non-robust. A small change in the training data can result in a large change in the tree and, consequently, the final predictions. To resolve the problem above, some techniques, often called ensemble methods, construct multiple decision trees that are combined to yield a single and better consensus prediction. Random ForestThe random forest approach is a bagging method where deep trees (why deep), fitted on bootstrap samples, are combined to produce an output with lower variance. However, it thus increasing the computational complexity by $B$ times. Feature Sampling:In addition to bagging, in the case of random forests, as each tree is constructed, only a random sample of predictors (features) is taken before each node is split, which decorrelates the trees. This random small tweak has benefits below: There are often a few predictors that dominate the decision tree fitting process. Consequently, many other predictors, which could be useful for very local features of the data, are rarely selected as splitting variables. With this tweak, each predictor will have at least several opportunities to be the predictor defining a split. It makes the decision making process more robust to missing features: Observations with missing features can still be regressed or classified based on the features that are not missing. Feature Importance Measures:We can obtain an overall summary of the importance of each predictor (feature) estimating the metrics we mentioned in last post. We can record the total amount of metrics due to splits over a given predictor, averaged over all $B$ trees. A large value indicates an important predictor. Essential parameters: n_Trees: In practice, few hundreds trees is often a good choice. n_Features: Typically, if there are a total of $D$ predictors, $D/3$ predictors in the case of regression and $\\sqrt{D}$ predictors in the case of classification make a good choice. Boosted TreesIn this post, we will focus specifically on the two most common boosting algorithms: AdaBoost and Gradient Boosting. Generic Setting and AlgorithmIn supervised learning problems, one has an output variable $y$ and a vector of input variables $\\mathbf{x}$ described via a joint probability distribution $P(\\mathbf{x}, y)$. Using a training set $\\{(\\mathbf{x}_{1},y_{1}),\\dots ,(\\mathbf{x}_{n},y_{n})\\}$, the goal is to find an approximation ${\\hat {F}}(\\mathbf{x})$ that minimizes the expected value of some specified loss functions. $$\\hat{F}=\\underset{F}{\\arg \\min } \\mathbb{E}_{\\mathbf{x}, y}[L(y, F(\\mathbf{x}))]$$ In boosting method, it seeks ${\\hat {F}}(\\mathbf{x})$ in the form of a weighted sum of functions $h_{i}(\\mathbf{x})$ (weak learners) from some class $\\mathcal {H}$: $$\\hat{F}(\\mathbf{x})=\\sum_{i=1}^{M} \\gamma_{i} h_{i}(\\mathbf{x})+\\text { const. }$$ It does so by starting with a model, consisting of a constant function $F_{0}(\\mathbf{x})$: $${F_{0}(\\mathbf{x})=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, \\gamma\\right)}$$, and **incrementally expands it** in a **greedy fashion**: $$F_{m}(\\mathbf{x})=F_{m-1}(\\mathbf{x})+\\gamma_{m}h_m\\\\{F_{m}(\\mathbf{x})=F_{m-1}(\\mathbf{x})+\\underset{h_{m} \\in \\mathcal{H}}{\\gamma_{m}\\arg \\min }\\left[\\sum_{i=1}^{n} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)+h_{m}\\left(\\mathbf{x}_{i}\\right)\\right)\\right]}$$, where $h_{m}\\in {\\mathcal {H}}$ is a weak learner function, $F_{m}(\\mathbf{x})$ is the aggregated function after $m$ steps. The above algorithm of boosting is generic. One can come with several strategies for the choice of the weak learner, residual (error) to fit, and the weighting parameter $\\lambda$. Therefore, there are various boosting methods. Adaptive BoostingThe AdaBoost (Adaptive Boosting) approach is a boosting method where shallow trees (why shallow), fitted on weighted samples, are combined to produce an output with lower bias. Be Compared to Random Forest:Compared to Random Forest, AdaBoost is generally less computational (shallow trees). However, these computations to fit different weak learners can’t be done in parallel. Overall, Random Forest is usually less accurate than Boosting on a wide range of tasks and usually slower in the runtime. Algorithm Setting:The total error $E$ defined in AdaBoost is the sum of its exponential loss ($\\sum_{i=1}^{N} e^{-y_{i} f\\left(\\mathbf{x}_{i}\\right)}$, different loss function result in different variations) on each data point, given as follows: $$E=\\sum_{i=1}^{N} e^{-y_{i} F_{m}\\left(\\mathbf{x}_{i}\\right)}=\\sum_{i=1}^{N} e^{-y_{i} F_{m-1}\\left(\\mathbf{x}_{i}\\right)} e^{-y_{i} \\gamma_{m} h_{m}\\left(\\mathbf{x}_{i}\\right)}\\\\E=\\sum_{i=1}^{N} w_{i}^{(m)} e^{-y_{i} \\gamma_{m} h_{m}\\left(\\mathbf{x}_{i}\\right)}$$, where $w_{i}^{(m)}=e^{-y_{i} F_{m-1}\\left(\\mathbf{x}_{i}\\right)}$ is a constant error of last iteration. So we can determine the optimal weak learner $h_m$ at step $m$ by $\\underset{h_m}{\\arg \\min }E$ and then the weighting parameter $\\gamma_m$​ by $\\frac{d E}{d \\gamma_{m}}=0$. A derivation of them in binary classification can be seen at wikipedia. Intuition of the Algorithm:AdaBoost weighs more on misclassified samples and good learners than the correctly classified samples and bad learners, respectively. First, it weighs observations in the dataset and train a new weak learner with a special focus given to the misclassified observations:$$E=\\sum_{i=1}^{N} \\underbrace{e^{-y_{i} F_{m-1}\\left(\\mathbf{x}_{i}\\right)}}_{Weights} \\underbrace{e^{-y_{i} \\gamma_{m} h_{m}\\left(\\mathbf{x}_{i}\\right)}}_{\\text{Loss of each observation}}$$So $\\underset{h_{m}}{\\arg \\min } E$ can not only be seen as an optimization of the boosted model after $m$ steps on the initial dataset but also a weak learner on the weighted dataset. Second, it adds the weak learner with a coefficient $\\gamma_m$ that expresse the performances of this weak learner: the better a weak learner performs, the more it contributes to the strong learner. For example, in binary classification case: $\\gamma_m=\\frac{1}{2} \\ln \\left(\\frac{\\sum_{y_{i}=h_{m}\\left(\\mathbf{x}_{i}\\right)} w_{i}^{(m)}}{\\sum_{y_{i} \\neq h_{m}\\left(\\mathbf{x}_{i}\\right)} w_{i}^{(m)}}\\right)$, in which the larger amount of misclassified data is the smaller $\\lambda_m$ is. Gradient BoostingDiffer from Adaboost, in Gradient Boosting, the decision tree model is trained on the pseudo-residuals, rather than residuals. Algorithm Setting:The idea is to apply a steepest descent step to this minimization problem (functional gradient descent, which regards the entire function as a variable). We would update the model in accordance with the following equations: $$ r_{im}=\\nabla_{F_{m-1}} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)\\right)\\\\ {F_{m}(x)=F_{m-1}(\\mathbf{x})-\\gamma_{m} \\sum_{i=1}^{n}r_{im}} $$, where $r_{im}$ is the **pseudo-residuals** and the model is **initialized with a constant value** $F_{0}(\\mathbf{x})=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, \\gamma\\right)$. Generic Gradient Boosting Method: Initialize model with a constant value $F_{0}(\\mathbf{x})=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, \\gamma\\right)$. For $m=1$ to $M$, a weak learner $h_{m}(\\mathbf{x})$ is fitted to pseudo-residuals $\\left{\\left(\\mathbf{x}i, r{i m}\\right)\\right}_{i=1}^{n}$. Compute multiplier $\\gamma _{m}$ using line search:$$\\gamma_{m}=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)+\\gamma h_{m}\\left(\\mathbf{x}_{i}\\right)\\right)$$ Update the model: $F_{m}(\\mathbf{x})=F_{m-1}(\\mathbf{x})+\\gamma_{m} h_{m}(\\mathbf{x})$ Compared to Adaboost:Gradient boosting uses instead a gradient descent approach and can more easily be adapted to large number of loss functions. Thus, gradient boosting can be considered as a generalization of Adaboost to arbitrary differentiable loss functions. Regularization/Parameters of Boosted TreesSeveral so-called regularization techniques reduce this overfitting effect by constraining the fitting procedure. Iterations: Namely, the number of trees in the model. Increasing it reduces the error on training set, but may lead to overfitting. Shrinkage / Learning Rate: Shrinkage consists in modifying the update rule as follows:$$F_{m}(\\mathbf{x})=F_{m-1}(\\mathbf{x})+\\nu \\cdot \\gamma _{m}h_{m}(\\mathbf{x}),\\quad 0 Empirically using small learning rates (such as $\\nu &lt;0.1$) improves models’ generalization ability (increase bias but decrease variance). However, it comes at the price of increasing computational time both during training and predicting, since lower learning rate requires more iterations. Sub-Sampling: Motivated by the bagging method, a decision tree can be fit on a subsample of the training set drawn at random without replacement. Sampling ratio ($f$) introduces randomness into the algorithm and help prevent overfitting acting as a kind of regularization. $0.5\\leq f\\leq 0.8$ (typically set to $0.5$) leads to good results. Also, like in bagging, subsampling allows one to define an out-of-bag error, which helps avoid the need for an independent validation dataset, but often underestimate actual performance improvement and the optimal number of iterations. Minimum Sample Size for Splitting Trees: Ignore any splits that lead to nodes containing fewer than this number of training set instances, which reduces variance. Sampling Features: Randomly choosing small subsets of features for different trees/layers/nodes, as in the case of Random Forest. Penalize Complexity of Tree:Penalize model complexity of the learned model. The model complexity can be defined as: The proportional number of leaves in the learned trees. $\\ell _{2}$ penalty on the leaf values (score). The joint optimization of loss and model complexity corresponds (equals) to a post-pruning algorithm to remove branches that fail to reduce the loss by a threshold. XgboostThe biggest drawback of the gradient boosting trees is that the algorithm is quite sequential and hence is very slow. XGBoost provides an optimized implementation of gradient boosted trees. It uses various tricks like regularization, column block for parallel learning, and cache-aware access to accelerate the speed. Algorithm Setting:The objective loss of Xgboost and its Second-order Taylor approximation is as following: $$\\begin{split}\\tilde{\\mathcal{L}}&=\\underbrace{\\sum_{i=1}^{n} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)+h_{m}\\left(\\mathbf{x}_{i}\\right)\\right)}_{Original loss}+\\underbrace{\\eta T+\\sum_{j=1}^{T}\\frac{1}{2} \\lambda\\left\\|\\omega_{j}\\right\\|^{2}}_{Regularization}\\\\&\\approx\\sum_{i=1}^{n}\\left[L\\left(y_{i}, F_{m-1}\\right)+\\frac{\\partial{L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)\\right)}}{\\partial{F_{m-1}}} h_{m}\\left(\\mathbf{x}_{i}\\right) + \\frac{1}{2} \\frac{\\partial^2{L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)\\right)}}{\\partial F_{m-1}^2}h_{m}^{2}\\left(\\mathbf{x}_{i}\\right)\\right]\\\\&+ \\eta T+\\sum_{j=1}^{T}\\frac{1}{2} \\lambda\\left\\|\\omega_{j}\\right\\|^{2}\\\\&= \\sum_{i=1}^{n}\\left[L\\left(y_{i}, F_{m-1}\\right)+g^1_i h_{m}\\left(\\mathbf{x}_{i}\\right) + \\frac{1}{2} g^2_i h_{m}^{2}\\left(\\mathbf{x}_{i}\\right)\\right]+ \\eta T+\\sum_{j=1}^{T}\\frac{1}{2} \\lambda\\left\\|\\omega_{j}\\right\\|^{2}\\end{split}$$, where $\\omega_j$ is the score in leaf nodes of weak learner $h_m$, and assume $I_{j}=\\left\\{i | q\\left(\\mathbf{x}_{i}\\right)=j\\right\\}$ as the instance set of leaf $j$: $$\\sum_{i=1}^{n}h_{m}\\left(\\mathbf{x}_{i}\\right)=\\sum_{j=1}^{T}\\left(\\sum_{i \\in I_{j}} 1\\right) w_{j}$$ By setting derivative of loss to 0, we can get the optimal leaf scores for $h_m$ : $$ w_{j}^{*}=-\\frac{\\sum_{i \\in I_{j}} g^1_{i}}{\\sum_{i \\in I_{j}} g^2_{i}+\\lambda} $$ But there could be an infinite number of possible trees with these optimal leaf scores, detailed method is needed to specify the tree. Compared to GBDT: From the perspective of optimization, GBDT adopts fastest descent method to get the optimal solution of loss function, while Xgboost uses the analytic method to expand the loss function to its second-order approximation and find the analytic solution. The parallelism of Xgboost is different from random forest. Xgboost can only carry out the next iteration after the previous iteration. The parallelism of xgboost is on the feature level. ReferenceBlog: Understanding Boosted Trees ModelsBlog: Ensemble methods: bagging, boosting and stackingStackExchange: How can we explain the fact that “Bagging reduces the variance while retaining the bias” mathematically?Blog: XGBoost原理和底层实现剖析PPT: Introduction to Boosted TreePaper: XGBoost: A Scalable Tree Boosting System","link":"/2019/11/24/tree-based-learning-algorithms-part-2/"}],"tags":[],"categories":[]}