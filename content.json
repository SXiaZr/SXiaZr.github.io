{"pages":[],"posts":[{"title":"","text":"","link":"/2019/10/17/Docker/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/08/22/hello-world/"},{"title":"Google Cloud for Deep Learning - Part 1","text":"Google Cloud is a suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products. Google compute engine instance is a virtual machine (VM) hosted on Google Cloud where you could run our GPU-enabled deep learning project. What the most important is that a $300 trial credit is offered by google to try this cloud services! The series of passages aiming to use the Google Cloud for deep learning has two parts: The first part is about creating a VM instance on the Google cloud and the second part is about installing some deep learning related configurations (jupyter notebook and GPUs). Let’s take a step to the Part 1! Create a VM InstanceStep 1: Create a Google Account with CreditYou should activate your free trial to get $300 credit for the trial. Your payment information will be required but you won’t be charged unless you manually upgrade to a paid account. So don’t be too worried about the autocharge! Step 2 : Create a New ProjectClick the “project button“ in the top left cornor (right to the “Google Cloud Platform“ icon) to create a new project. Step 3: Create a VM InstanceClick on the “three lines button“ (menu) on the top left corner and then click on “Compute Engine“. There some tips for the new VM instance: Select a region with zone that has GPU resources, e.g. “us-west1-a”. Customize your machine type (CPU, GPU) as you want, which can be adjusted after the creation. Usually select “Ubuntu 16.04 LTS“ as the image in your boot disk. Tick both “http“ and “https“ under the firewall options to allow the network traffic. Expand the drop-down menu “Management, security, disks …“, choose the “disk“ tab and untick “Delete boot disk when instance is deleted“. You can also add an additonal disk if you want. IMPORTANT : DON’T FORGET TO STOP YOUR GPU INSTANCE, OTHERWISE GCP WILL KEEP CHARGING. Step 4: Request for Increase in QuotaYou probably get an error “Quota ‘GPUS_ALL_REGIONS’ exceeded. Limit: 0.0 globally“, which requires you to increase the quota. Click “Quotas“ in “IAM &amp; Admin“, send a quota increase request for “Compute Engine API GPUs (all regions)“ to at least set the limit to 1. An email will be sent automatically and your application will be processed in following days. If you want to use mutiple GPUs in one VM instance, you need to increase the quota for a specific kind of GPUs in the same zone as your VM instance as well, such as “Compute Engine API NVIDIA K80 GPUs“ in “us-west1-a”. Step 5: Connecting to the VM via SSHThree of those methods will be mentioned briefly. Method 1: SSH from the browser Method 2: Log in via SSH password verification Log in using method 1, switch to root and reset the passward: 12$ sudo -i $ sudo passwd root Edit the SSH configuration file: 123$ vim /etc/ssh/sshd_config# Eneter :wq to quit with change # Eneter :q! to quit without change and change things as following: 12PermitRootLogin yesPasswordAuthentication yes Restart SSH and activate new setting: 1$ service sshd restart Connect to Google Cloud using passward set above. 12$ ssh user@host# First time: Type yes if any notification host here should be the external-ip as below: ### Method 3 :Log in via the local private key file Generate SSH key file (in macOS): 1$ ssh-keygen -t rsa Copy the public key: 123$ cat ~/.ssh/id_rsa.pub# ...45UVp1 user@computer-name.local# Change user@computer-name.local as the user you want to login, e.g. root Paste it to “Compute Engine“ → “Metadata“, Google will write the public key to ~/.ssh/authorized_keys: Use the corresponding private to login and enter passphrase for key: 1$ ssh -i id_rsa user@host ReferenceBlog: Running Jupyter Notebook on Google Cloud Platform in 15 minBlog: Mac使用SSH登录Google Cloud PlatformBlog: Google Cloud Platform SSH 连接配置","link":"/2019/10/17/google-cloud-for-deep-learning-part-1/"},{"title":"Google Cloud for Deep Learning - Part 2","text":"The series of passages to use the Google Cloud for deep learning has two parts: The first part is about creating a VM instance on the Google cloud and the second part is about installing some deep learning related configurations (jupyter notebook and GPUs). Now, Let’s move to the Part 2! Configure the Deep Learning EnviromentStep 1: Install GPU Drivers and CUDAAfter you create an instance with one or more GPUs, your system requires device drivers so that your applications can access the device. Install CUDA, which includes the NVIDIA driver. Following steps to install CUDA and the associated drivers for NVIDIA® GPUs. Run command line to create a script: 1$ vim installNvidia.sh Paste the following script to install CUDA: 12345678910111213#!/bin/bashecho \"Checking for CUDA and installing.\"# Check for CUDA and try to install.if ! dpkg-query -W cuda-10-0; then # The 16.04 installer works with 16.10. curl -O http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.0.130-1_amd64.deb dpkg -i ./cuda-repo-ubuntu1604_10.0.130-1_amd64.deb apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub apt-get update apt-get install cuda-10-0 -yfi# Enable persistence modenvidia-smi -pm 1 Run command line to execute the script: 1$ bash installNvidia.sh Verify that the driver installed and initialized properly. 1$ nvidia-smi The output of the command looks similar to the following: 123456789+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.92 Driver Version: 410.92 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla P100-PCIE... On | 00000000:00:04.0 Off | 0 || N/A 34C P0 26W / 250W | 0MiB / 16276MiB | 0% Default |+-------------------------------+----------------------+----------------------+ Step 2: Install cuDNN The NVIDIA CUDA® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. IMPORTANT: If you want to use pytorch with an NVIDIA GPU…We ship with everything in-built (pytorch binaries include CUDA, CuDNN, NCCL, MKL, etc.) When using pytorch, you just need to install the NVIDIA drivers and the binaries will come with the other libs. If you want to build from source, you would need to install CUDA, cuDNN etc. The cuDNN can be installed as the following: Check the version of your CUDA and download the corresponding version of cuDNN which requires a NVIDIA account. 1234# Check the version of CUDA$ cat /usr/local/cuda/version.txt# Check the version of cuDNN$ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 Download it to the local and copy it to the remote: Or use wget to download it. It’s a bit more tricky, you have to start the downloading and copy the downloading link from the browser downloader. Then decompress downloaded compressed file and get a folder cuda: 1$ tar -zxvf file Copy files to finish the installation 12$ sudo cp cuda/lib64/* /usr/local/cuda/lib64/$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ Step 3: Install Anaconda with Jupyter Notebook Copy the bash (.sh file) installer link: Use wget to download the bash installer: 1$ wget https://repo.continuum.io/archive/Anaconda3&lt;release&gt;.sh Run the bash script to install Anaconda3: 1$ bash Anaconda3-5.2.0-Linux-x86_64.sh source the .bashrc file to add Anaconda to your PATH. Now that Anaconda3 is added to PATH, source the .bashrcfile to load the new PATH environment variable. 1$ source ~/.bashrc To verify the installation is complete, open Python from the command line: 1$ python Step 4: Local Access to Jupyte Service Set firewall rules in “VPC network“ → “Firewall rules“ → “Create firewall rules“. Set Source IP ranges as 0.0.0.0/0 to allow all IPs to access. Set “Protocols and Ports“ to “tcp:port“, which the notebook server will listen on (e.g. 8888). Create a Jupyter configuration file if it doesn’t exist: 1$ jupyter notebook --generate-config Add a few lines to your Jupyter configuration file: 1$ vim .jupyter/jupyter_notebook_config.py 1234567c.NotebookApp.ip = '*'# The port the notebook server will listen on.c.NotebookApp.port = 8888# Whether to open in a browser after starting.c.NotebookApp.open_browser = False# Whether to allow the user to run the notebook as root.c.NotebookApp.allow_root = True Run the jupyter notebook: 12$ jupyter notebook# Then paste returned URL to the browser: http://host(static-ip):8888/?token=...ccf7bb Small Tips:Modifying Existing InstancesYou can modify resources (CPU/GPUs) on your existing instances, but you must first stop the instance and change its host maintenance. At the top of the instance details page, click “Stop“ to stop the instance. After the instance stops running, click “Edit“ to change the instance properties. Make External IP Address as StaticBy default, the external IP address is dynamic and we need to make it static to make our life easier. “VPC network“ →”External IP addresses“, and change the “Type“ from “Ephemeral“ to “Static“ Useful Script for Uninstallation12$ sudo apt-get --purge remove cuda # uninstall cuda $ sudo apt autoremove Create Conda Environment With conda, you can create, export, list, remove, and update environments that have different versions of Python and/or packages installed in them. 123# Create and activate the environment$ conda create --name myenv python=3.6$ source activate myenv Transfer Files Between the Local and RemoteUse scp command From the local to the remote: 1234$ scp local_file remote_username@remote_ip:remote_folder$ scp local_file remote_username@remote_ip:remote_file# Add -r parameter to move the entire folder$ scp -r local_folder remote_username@remote_ip:remote_folder From the remote to the local: just reverse the path of file. Use FTP application Use FileZilla® as an example, remember that you should use sftp://external-ip as the host. ReferenceDoc: Compute Engin DocmentationBlog: Running Jupyter Notebook on Google Cloud Platform in 15 minBlog: Installing Anaconda on LinuxBlog: Linux 教程","link":"/2019/10/21/google-cloud-for-deep-learning-part-2/"},{"title":"Optimization Algorithm in Deep Learning","text":"An optimization algorithm is a procedure which is executed iteratively by comparing various solutions until an optimum or a satisfactory solution is found. Here, we introduce some optimization algorithms commonly used in deep learning. Gradient Descent Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point as following:$$w = w - \\eta \\nabla Q(w) $$ Batch Gradient Descent Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated. Batch gradient descent decreases update frequency but takes much more time per iteration. Batch gradient descent has a more stable cost gradient and the cost goes down on every iteration. However, it may result in premature convergence of the model to a local minimum. Batch gradient descent requires the entire training dataset in memory and available to the algorithm, which is memory-consuming. Stochastic Gradient Descent Stochastic gradient descent is a variation of the gradient descent algorithm that calculates the error and updates the model for each example in the training dataset. It can be regarded as a stochastic approximation of batch gradient descent optimization, since it replaces the actual gradient by an estimate. Stochastic gradient descent increases model update frequency and can result in faster learning on some problems. Stochastic gradient descent can be extremely noisy, which may cause the model error/cost to jump around (higher variance). The noisy update process can allow the model to avoid local minima, but it also makes it hard to settle on a minimum (oscillate around the region of the minimum). Stochastic gradient descent loses the speed up from vectorization (matrix computation). Mini-Batch Gradient Descent Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients. Mini-batch gradient descent seeks to find a balance between the robustness of batch gradient descent and the efficiency of stochastic gradient descent.$$w = w - \\eta \\nabla Q(w) = w - \\eta \\sum_{i=1}^n \\nabla Q_i(w)/n$$ The cost of mini-batch gradient descent should trend downwards with noises (oscillations). Since some easy mini-batches might have a lower cost, while hard mini-batches (with some mislabeled examples) might have a higher cost. The larger step size can be adopted compared to stochastic gradient descent since the gradient is more stable. The batch size is a hyper-parameter that can be tuned as a power of two that fits the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, and so on. Gradient Descent with Momentum Stochastic gradient descent with momentum remembers the update $\\delta \\omega$ at each iteration, and determines the next update as a linear combination of the gradient and the previous update: RMSpropAdamAdaGrad title: Optimization Algorithm in Deep Learning date: 2019-10-17 00:00:00Gradient Descent Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point as following:$$\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n \\nabla F(\\mathbf{x}_n),\\ n \\ge 0$$ Batch gradient descent Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated. With batch gradient descent on every iteration you go through the entire training set and you’d expect the cost to go down on every single iteration. The decreased update frequency results in a more stable error gradient and may result in a more stable convergence on some problems Fewer updates to the model means this variant of gradient descent is more computationally efficient than stochastic gradient descent. The more stable error gradient may result in premature convergence of the model to a less optimal set of parameters. Commonly, batch gradient descent is implemented in such a way that it requires the entire training dataset in memory and available to the algorithm.Model updates, and in turn training speed, may become very slow for large datasets. As stochastic gradient descent won’t ever converge, it’ll always just kind of oscillate and wander around the region of the minimum. But it won’t ever just head to the minimum and stay there. In practice, the mini-batch size you use will be somewhere in between. Stochastic Gradient Descent Stochastic gradient descent is a variation of the gradient descent algorithm that calculates the error and updates the model for each example in the training dataset. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). But a huge disadvantage to stochastic gradient descent is that you lose almost all your speed up from vectorization.Because, here you’re processing a single training example at a time. The way you process each example is going to be very inefficient. Mini-batch gradient descentAnd It’s not guaranteed to always head toward the minimum but it tends to head more consistently in direction of the minimum than the consequent descent. And then it doesn’t always exactly convert or oscillate in a very small region. If that’s an issue you can always reduce the learning rate slowly. Anything from 64 up to maybe 512 are quite typical. And because of the way computer memory is layed out and accessed, sometimes your code runs faster if your mini-batch size is a power of 2. One last tip is to make sure that your mini batch。 All of your X{t}, Y{t} that that fits in CPU/GPU memory. Try a few different powers of two and then see if you can pick one that makes your gradient descent optimization algorithm as efficient as possible. Gradient descent with momentum gradient descent with momentum RMSpropAdam Adam In the typical implementation of Adam, you do implement bias correction. So, this algorithm has a number of hyper parameters. The learning with hyper parameter alpha is still important and usually needs to be tuned. So you just have to try a range of values and see what works. A common choice really the default choice for ß1 is 0.9. So this is a moving average, weighted average of dw right this is the momentum light term. The hyper parameter for ß2, the authors of the Adam paper, inventors of the Adam algorithm recommend 0.999. And then Epsilon, the choice of epsilon doesn’t matter very much. But the authors of the Adam paper recommended it 10 to the minus 8. Adam stands for Adaptive Moment Estimation. So ß1 is computing the mean of the derivatives. This is called the first moment. And ß2 is used to compute exponentially weighted average of the ²s and that’s called the second moment. Second-Order MethodsReferenceCoursera: Deep Learning SpecializationBlog: A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size","link":"/2019/10/16/optimization-algorithm-in-deep-learning/"},{"title":"Normalization in Deep Learning","text":"Normalization refers to a process that makes something more normal or regular. In statistics, normalization means adjusting values measured on different scales to a common scale, or adjustments to bring the entire probability distributions of adjusted values into alignment. Normalization in deep learning can make the model to train efficiently. Why does Normalization work?Intuition 1: Input Normalization $\\frac{X - \\mu}{\\sigma}$ Make training less sensitive to the scale of features. After optimization, the feature with a large scale will always be weighted with a small value ($\\omega_{small}$), and vise versa ($\\omega_{large}$). Because of the large scale of the feature, a tiny change in the small weight ($\\omega_{small}$) will change the prediction by a lot compared to the same change in the large weight ($\\omega_{large}$). It means that setting $\\omega_{small}$ correctly might dominate the optimization process and the feature with a large scale is of more importance which actually makes no sense. Make optimization well-conditioned. The rough intuition is that the cost function will be more round and easier to optimize when features are all on similar scales. Cost function of the unnormalized features can be a very elongated bowl, whereas if we normalize the features, then the cost function will look more symmetric. Having an elongated cost function, we might have to use a very small learning rate and gradient descent might need a lot of steps to oscillate back and forth before it finally gets to the minimum. If we have more spherical contours, then gradient descent can pretty much go straight to the minimum and we can take much larger steps with gradient descent without much oscillation. Intuition 2: Covariate Shift Covariate shift refers to the problem that the distribution of the input values changes, but the concept (model) being learned remains stationary. In deep learning, the basic idea behind normalization is to limit the covariate shift, which allows the model to learn on a more stable distribution, and would thus accelerate the training of the network. Batch NormalizationIn deep learning, we are particularly concerned with the change in the distribution of the inputs to the hidden nodes within a network. A neural network changes the weights of each layer over the training, which means that the activations of each layer change as well. Since the activations of a previous layer are the inputs of the next layer, each layer in the neural network is faced with a situation where the input distribution changes with each step (covariate shift). Batch Normalization at Training Time What batch normalization does is, especially from the perspective of the later layers of the neural network, it limits the earlier layers to not get to shift around much, by restricting them to have the same mean and variance. It weakens the coupling between what the early layers and the later layers. For a layer of the network with d-dimensional input, $x = (x^{(1)},…,x^{(d)})$, each dimension is then normalized separately as following: $$\\hat{x}_i^{(k)} = \\frac{x_i^{(k)}-\\mu_B^{(k)}}{\\sqrt{\\sigma_B^{(k)^2}+\\epsilon}}$$where $\\mu_B^{(k)}$ and $\\sigma_B^{(k)^2}$ are the per-dimension mean and variance, respectively. $\\epsilon$ is added in the denominator for numerical stability and is an arbitrarily small constant. The resulting normalized $\\hat{x}^{(k)}$ have zero mean and unit variance, to restore the representation power of the network or better take advantage of the nonlinearity (if activations are normalized), a transformation step then follows as:$$y_i^{(k)} = \\gamma^{(k)} \\hat{x}_{i}^{(k)} +\\beta^{(k)}$$where the parameters $\\gamma^{(k)}$and $\\beta^{(k)}$ are subsequently learnt in the optimization process and can convert the mean and variance to any value that the network desires. Formally, the transform $BN_{\\gamma,\\beta}: x_{1…m} \\rightarrow y_{1…m}$ is denoted as the Batch Normalizing Transform. There are some debates in deep learning about whether weshould normalize the value before the activation function or after it. In practice, normalizing before the activation function is done much more often. Regularization can be motivated as a technique to improve the generalizability of a learned model. Batch normalization has a slight regularization effect. The mean and variance are a little bit noisy because they are estimated on a mini-batch. Similar to dropout, batch normalization adds small noise to the hidden layers and therefore has a very slight regularization effect. We don’t particularly use to batch normalization as a regularization but use it as a way to speed up learning. Batch Normalization at Test Time At the test time, we use the $\\gamma$ and $\\beta$ learned from training, the $\\mu$ and $\\sigma^2$ are estimated from the training set. Running the whole training set to get $\\mu$ and $\\sigma^2$ could be memory-consuming. So we keep a running average of the $\\mu$ and $\\sigma^2$ for each layer as we train the neural network across different mini-batches to get an estimation of them. In practice, the moving average people usually use here is exponentially weighted average. An exponential moving average (EMA), also known as an exponentially weighted moving average (EWMA), is a first-order filter that applies weighting factors which decrease exponentially. The EMA for a series $Y$ is calculated recursively: $$S_t = \\begin{cases} \\alpha \\cdot Y_1, &amp; t = 1 \\\\ \\alpha \\cdot Y_t + (1 - \\alpha) \\cdot S_{t-1}, &amp; t &gt; 1\\end{cases}$$Where: $\\alpha$ represents the degree of weighting decrease, a constant smoothing factor between 0 and 1. $Y_t$ and $S_t$ are the values at a time period $t$ respectively. This formula can also be expressed in technical analysis terms as follows: How the EMA steps towards the latest datum point:$$\\text{EMA}_\\text{today} = \\text{EMA}_\\text{yesterday} + \\alpha \\left[\\text{price}_\\text{today} - \\text{EMA}_\\text{yesterday}\\right]$$ How the weighting factor on each data point $p_1$, $p_2$, etc., decreases exponentially by expanding out $\\text{EMA}_\\text{yesterday}$ :$$\\begin{split}\\text{EMA}_\\text{today}= &amp; { \\alpha \\left[p_1 + (1 - \\alpha) p_2 + (1 - \\alpha)^2 p_3 + (1 - \\alpha)^3 p_4 + \\cdots \\right] } \\\\= &amp;\\frac{p_1 + (1 - \\alpha) p_2 + (1 - \\alpha)^2 p_3 + (1 - \\alpha)^3 p_4 + \\cdots}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + (1 - \\alpha)^3 + \\cdots}\\end{split}$$since $1/\\alpha = 1 + (1 - \\alpha) + (1 - \\alpha)^2 + \\cdots$ (weighted average). A couple of things about choosing $\\alpha$: A higher $\\alpha$ discounts older observations faster so that the EMA will adapt more quickly to the current changes. When compute EMA, we can think of $S_t$ as approximately averaging over $\\frac{1}{\\alpha}$ number of data. So with a low value of $\\alpha$, the EMA is much smoother and also has more latency. (Always set to 0.1 in deep learning) Bias correction is a technical modification that makes EMA estimate more accurate, especially during its initial phase. The EMA is very low when the iteration just starts off and we will get a much lower value that is not a very good estimate of the first data. In order to fix the problem, we could instead of taking $S_t$, take $\\frac{S_t}{1-(1-a)^t}$, namely bias correction. It will make the EMA larger when $t$ is small and make almost no difference when $t$ is large enough. Weight NormalizationThe Limitations of Batch NormalizationThe key limitation of batch normalization is that it is dependent on the mini-batch. It puts a lower limit on the batch size. Ideally, we want to use the global mean and variance to normalize the inputs to a layer. However, it’s too costly and the mean and variance are simply estimates on mini-batch, which means that they contain a certain amount of noises. Smaller mini-batch sizes increase the variance of these estimates. It can be a problem in settings such as online learning and reinforcement learning which is highly sensitive to noise. It is difficult to apply to recurrent connections in recurrent neural network. In a recurrent neural network, the recurrent activations of each time-step will have different statistics. This means that we have to fit a separate batch normalization layer for each time-step and it forces us to store the statistics for each time-step during training. Theory of Weight Normalization What weight normalization does is it separates the norm of the weight vector from its direction. In an effort to speed up the convergence of optimization procedure, it proposes to reparameterize each weight vector $\\omega$ in terms of a parameter vector $\\boldsymbol{v}$ and a scalar parameter $g$ in the following way: $$\\omega = \\frac{g}{||\\boldsymbol{v}||}\\boldsymbol{v}$$It then optimizes both$\\boldsymbol{v}$ and $g$ using gradient descent. This reparameterization has the effect of fixing the Euclidean norm of the weight vector $\\omega$: we now have $||\\omega||=g$, independent of the parameters $\\boldsymbol{v}$. We, therefore, call this reparameterization weight normalization. Layer NormalizationTheory of Layer Normalization The key feature of layer normalization is that it normalizes the inputs across the features. The equations of layer normalization and batch normalization are similar:$$\\hat{x}_i^{(k)} = \\frac{x_i^{(k)}-\\mu_L}{\\sqrt{\\sigma_L^2+\\epsilon}}$$where $\\mu_L$ and $\\sigma_L^2$ are the mean and variance across the features, respectively. ReferenceQuora: Why do we normalize the dataCoursera: Deep Learning SpecializationBlog: An Intuitive Explanation of Why Batch Normalization Really WorksBlog: Weight Normalization and Layer Normalization ExplainedBlog: An Overview of Normalization Methods in Deep Learning","link":"/2019/10/14/normalization-in-deep-learning/"}],"tags":[],"categories":[]}