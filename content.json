{"pages":[],"posts":[{"title":"","text":"","link":"/2019/10/17/Docker/"},{"title":"Google Cloud for Deep Learning - Part 1","text":"Google Cloud is a suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products. Google compute engine instance is a virtual machine (VM) hosted on Google Cloud where you could run our GPU-enabled deep learning project. What the most important is that a $300 trial credit is offered by google to try this cloud services! The series of passages aiming to use the Google Cloud for deep learning has two parts: The first part is about creating a VM instance on the Google cloud and the second part is about installing some deep learning related configurations (jupyter notebook and GPUs). Let’s take a step to the Part 1! Create a VM InstanceStep 1: Create a Google Account with CreditYou should activate your free trial to get $300 credit for the trial. Your payment information will be required but you won’t be charged unless you manually upgrade to a paid account. So don’t be too worried about the autocharge! Step 2 : Create a New ProjectClick the “project button“ in the top left cornor (right to the “Google Cloud Platform“ icon) to create a new project. Step 3: Create a VM InstanceClick on the “three lines button“ (menu) on the top left corner and then click on “Compute Engine“. There some tips for the new VM instance: Select a region with zone that has GPU resources, e.g. “us-west1-a”. Customize your machine type (CPU, GPU) as you want, which can be adjusted after the creation. Usually select “Ubuntu 16.04 LTS“ as the image in your boot disk. Tick both “http“ and “https“ under the firewall options to allow the network traffic. Expand the drop-down menu “Management, security, disks …“, choose the “disk“ tab and untick “Delete boot disk when instance is deleted“. You can also add an additonal disk if you want. IMPORTANT : DON’T FORGET TO STOP YOUR GPU INSTANCE, OTHERWISE GCP WILL KEEP CHARGING. Step 4: Request for Increase in QuotaYou probably get an error “Quota ‘GPUS_ALL_REGIONS’ exceeded. Limit: 0.0 globally“, which requires you to increase the quota. Click “Quotas“ in “IAM &amp; Admin“, send a quota increase request for “Compute Engine API GPUs (all regions)“ to at least set the limit to 1. An email will be sent automatically and your application will be processed in following days. If you want to use mutiple GPUs in one VM instance, you need to increase the quota for a specific kind of GPUs in the same zone as your VM instance as well, such as “Compute Engine API NVIDIA K80 GPUs“ in “us-west1-a”. Step 5: Connecting to the VM via SSHThree of those methods will be mentioned briefly. Method 1: SSH from the browser Method 2: Log in via SSH password verification Log in using method 1, switch to root and reset the passward: 12$ sudo -i $ sudo passwd root Edit the SSH configuration file: 12$ vim /etc/ssh/sshd_config# Eneter :wq to quit with change; Eneter :q! to quit without change and change things as following: 12PermitRootLogin yesPasswordAuthentication yes Restart SSH and activate new setting: 1$ service sshd restart Connect to Google Cloud using passward set above. 12$ ssh user@host# First time: Type yes if any notification host here should be the external-ip as below: Method 3 :Log in via the local private key file Generate SSH key file (in macOS): 1$ ssh-keygen -t rsa Copy the public key: 123$ cat ~/.ssh/id_rsa.pub# ...45UVp1 user@computer-name.local# Change user@computer-name.local as the user you want to login, e.g. root Paste it to “Compute Engine“ → “Metadata“, Google will write the public key to ~/.ssh/authorized_keys: Use the corresponding private key to login and enter passphrase for key: 1$ ssh -i ~/.ssh/id_rsa user@host Small Tips:Modifying Existing InstancesYou can modify resources (CPU/GPUs) on your existing instances, but you must first stop the instance and change its host maintenance. At the top of the instance details page, click “Stop“ to stop the instance. After the instance stops running, click “Edit“ to change the instance properties. ReferenceBlog: Running Jupyter Notebook on Google Cloud Platform in 15 minBlog: Mac使用SSH登录Google Cloud PlatformBlog: Google Cloud Platform SSH 连接配置","link":"/2019/10/17/google-cloud-for-deep-learning-part-1/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/08/22/hello-world/"},{"title":"Google Cloud for Deep Learning - Part 2","text":"The series of passages to use the Google Cloud for deep learning has two parts: The first part is about creating a VM instance on the Google cloud and the second part is about installing some deep learning related configurations (jupyter notebook and GPUs). Now, Let’s move to the Part 2! Configure the Deep Learning EnviromentStep 1: Install GPU Drivers and CUDAAfter you create an instance with one or more GPUs, your system requires device drivers so that your applications can access the device. Install CUDA, which includes the NVIDIA driver. Following steps to install CUDA and the associated drivers for NVIDIA® GPUs. Run command line to create a script: 1$ vim installNvidia.sh Paste the following script to install CUDA: 12345678910111213#!/bin/bashecho \"Checking for CUDA and installing.\"# Check for CUDA and try to install.if ! dpkg-query -W cuda-10-0; then # The 16.04 installer works with 16.10. curl -O http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.0.130-1_amd64.deb dpkg -i ./cuda-repo-ubuntu1604_10.0.130-1_amd64.deb apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub apt-get update apt-get install cuda-10-0 -yfi# Enable persistence modenvidia-smi -pm 1 Run command line to execute the script: 1$ bash installNvidia.sh Verify that the driver installed and initialized properly. 1$ nvidia-smi The output of the command looks similar to the following: 123456789+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.92 Driver Version: 410.92 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla P100-PCIE... On | 00000000:00:04.0 Off | 0 || N/A 34C P0 26W / 250W | 0MiB / 16276MiB | 0% Default |+-------------------------------+----------------------+----------------------+ Step 2: Install cuDNN The NVIDIA CUDA® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. IMPORTANT: If you want to use pytorch with an NVIDIA GPU…We ship with everything in-built (pytorch binaries include CUDA, CuDNN, NCCL, MKL, etc.) When using pytorch, you just need to install the NVIDIA drivers and the binaries will come with the other libs. If you want to build from source, you would need to install CUDA, cuDNN etc. The cuDNN can be installed as the following: Check the version of your CUDA and download the corresponding version of cuDNN which requires a NVIDIA account. 1234# Check the version of CUDA$ cat /usr/local/cuda/version.txt# Check the version of cuDNN$ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 Download it to the local and copy it to the remote: Or use wget to download it. It’s a bit more tricky, you have to start the downloading and copy the downloading link from the browser downloader. Then decompress downloaded compressed file and get a folder cuda: 1$ tar -zxvf file Copy files to finish the installation 12$ sudo cp cuda/lib64/* /usr/local/cuda/lib64/$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ Step 3: Install Anaconda with Jupyter Notebook Copy the bash (.sh file) installer link: Use wget to download the bash installer: 1$ wget https://repo.continuum.io/archive/Anaconda3&lt;release&gt;.sh Run the bash script to install Anaconda3: 1$ bash Anaconda3-5.2.0-Linux-x86_64.sh source the .bashrc file to add Anaconda to your PATH. Now that Anaconda3 is added to PATH, source the .bashrcfile to load the new PATH environment variable. 1$ source ~/.bashrc To verify the installation is complete, open Python from the command line: 1$ python Step 4: Local Access to Jupyte Service Set firewall rules in “VPC network“ → “Firewall rules“ → “Create firewall rules“. Set Source IP ranges as 0.0.0.0/0 to allow all IPs to access. Set “Protocols and Ports“ to “tcp:port“, which the notebook server will listen on (e.g. 8888). Create a Jupyter configuration file if it doesn’t exist: 1$ jupyter notebook --generate-config Add a few lines to your Jupyter configuration file: 1$ vim .jupyter/jupyter_notebook_config.py 1234567c.NotebookApp.ip = '*'# The port the notebook server will listen on.c.NotebookApp.port = 8888# Whether to open in a browser after starting.c.NotebookApp.open_browser = False# Whether to allow the user to run the notebook as root.c.NotebookApp.allow_root = True Run the jupyter notebook: 12$ jupyter notebook# Then paste returned URL to the browser: http://host(static-ip):8888/?token=...ccf7bb Small Tips:Make External IP Address as StaticBy default, the external IP address is dynamic and we need to make it static to make our life easier. “VPC network“ →”External IP addresses“, and change the “Type“ from “Ephemeral“ to “Static“ Useful Script for Uninstallation12$ sudo apt-get --purge remove cuda # uninstall cuda $ sudo apt autoremove Create Conda Environment With conda, you can create, export, list, remove, and update environments that have different versions of Python and/or packages installed in them. 123# Create and activate the environment$ conda create --name myenv python=3.6$ source activate myenv Transfer Files Between the Local and RemoteUse scp command From the local to the remote: 1234$ scp local_file remote_username@remote_ip:remote_folder$ scp local_file remote_username@remote_ip:remote_file# Add -r parameter to move the entire folder$ scp -r local_folder remote_username@remote_ip:remote_folder From the remote to the local: just reverse the path of file. Use FTP application Use FileZilla® as an example, remember that you should use sftp://external-ip as the host. ReferenceDoc: Compute Engin DocmentationBlog: Running Jupyter Notebook on Google Cloud Platform in 15 minBlog: Installing Anaconda on LinuxBlog: Linux 教程","link":"/2019/10/21/google-cloud-for-deep-learning-part-2/"},{"title":"Model Selection & Bias–Variance Tradeoff","text":"In this passage, I summarize some basic concepts in Machine Learning, which mainly include model validation methods in model selection and the interpretation in bias–variance tradeoff. Probabilistic SetupIn the following, we set up a general probabilistic view on the data. Assume that the data is generated as (Data Generation Model) $$y=f(\\mathbf{x})+\\varepsilon$$, where $f$ is some arbitrary and unknown function and $\\mathbf{x}$ is generated according to some ﬁxed but unknown distribution $\\mathcal{D}_{\\mathbf{x}}$ . $\\varepsilon$ is an additive noise with distribution $\\mathcal{D}_{\\varepsilon}$, which is independent from sample to sample and independent from the data. Assume $\\varepsilon$ has zero mean (otherwise this constant can be absorbed into $f$). There is an unknown underlying joint distribution $\\mathcal{D}$ on pairs $(\\mathbf{x}, y)$, with range $\\mathcal{X} \\times \\mathcal{Y}$. The data set $\\mathcal{S}$ we see consists of independent and identically distributed(i.i.d.) samples from $\\mathcal{D}$: $$S=\\left\\{ \\left(\\mathbf{x}_{n}, y_{n}\\right) \\text {i.i.d.} \\sim \\mathcal{D}\\right\\}_{n=1}^{N}$$ Write $f_{\\mathcal{S}} = \\mathcal{A}(\\mathcal{S})$, where $\\mathcal{A}$ denotes the learning algorithm, $f_{\\mathcal{S}}$ is the resulting prediction function or model which depends on the data subset $\\mathcal{S}$ we are given. If we want to indicate that $f_{\\mathcal{S}}$ also depends on hyper-parameters of the model, we can add a subscript to write $f_{\\mathcal{S, \\lambda}}$. Model Selection Model (actually hyper-parameters) is selected by the result of VALIDATION. Given a model $f_{\\mathcal{S}}$, how can we assess if $f_{\\mathcal{S}}$ is any good? Theoretically, the best way is to compute the expected error over all possible samples $(\\mathbf{x}, y)$ chosen from $\\mathcal{D}$: $$L_{\\mathcal{D}}(f_{\\mathcal{S}})=\\mathbb{E}_{\\mathcal{D}}[\\ell(y, f_{\\mathcal{S}}(\\mathbf{x}))]$$ , where $\\ell(\\cdot, \\cdot)$ is our loss function. The quantity $L_{\\mathcal{D}}(f_{\\mathcal{S}})$ is called generalization error. Training, Validation &amp; Test SetTraining Set: A set of data used for learning, to fit the weights of the model. We would use the training set to find the “optimal” weights with the back-propagation algorithm. (Get optimal training error) Validation Set: A set of data used to tune the hyper-parameters of the model. We would use the validation set to find the “optimal” hyper-parameters or determine a stopping point for the back-propagation algorithm. (Get optimal validation error) Test Set: A set of data used only to assess the performance of the fully-trained model. After assessing the final model on the test set, YOU MUST NOT tune the model any further! Why separate test and validation sets?The error estimate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model. Generalization Error V.S. Training ErrorGeneralization error is the quantity we are fundamentally interested in, but we cannot compute it since $\\mathcal{D}$ is unknown. Since we have given a data subset $\\mathcal{S}$, it is natural to compute the equivalent empirical quantity: $$L_{\\mathcal{S}}\\left(f_{\\mathcal{S}}\\right)=\\frac{1}{|\\mathcal{S}|} \\sum_{\\left(\\mathbf{x}_{n}, y_{n}\\right) \\in \\mathcal{S}} \\ell\\left(y_{n}, f_{\\mathcal{S}}\\left(\\mathbf{x}_{n}\\right)\\right)$$ This is often called the training error, but $L_{\\mathcal{S}}(f_{\\mathcal{S}})$ might not be close to $L_{\\mathcal{D}}(f_{\\mathcal{S}})$ for the sake of overﬁtting. Generalization Error V.S. Test ErrorValidating model on the same data we trained it on will result in overfitting, so we could split the data into a training and a test set (a.k.a. validation set), call them $\\mathcal{S}_{\\text {train}}$ and $\\mathcal{S}_{\\text {test}}$, respectively. From the training set, we learn the model $\\mathcal{S}_{\\text {train}}$ and then compute the error on the test set: $$L_{\\mathcal{S}_{\\text {test}}}(f_{\\mathcal{S}_{\\text {train}}})=\\frac{1}{\\left|\\mathcal{S}_{\\text {test }}\\right|} \\sum_{\\left(y_{n}, \\mathbf{X}_{n}\\right) \\in \\mathcal{S}_{\\text {test}}}\\ell\\left(y_{n}, f_{\\mathcal{S}_{\\text {train }}}\\left(\\mathbf{x}_{n}\\right)\\right)$$, where $L_{\\mathcal{S}_{\\text {test}}}(f_{\\mathcal{S}_{\\text {train}}})$ is called **test error (empirical error)**. Indeed, the expectation of test error equals generalization error: $$L_{\\mathcal{D}}\\left(f_{\\mathcal{S}_{\\text {train }}}\\right)=\\mathbb{E}_{\\mathcal{S}_{\\text {test }} \\sim \\mathcal{D}}\\left[L_{\\mathcal{S}_{\\text {test }}}\\left(f_{\\mathcal{S}_{\\text {train }}}\\right)\\right]$$**But for a particular test set $\\mathcal{S}_{\\text {test}}$, how far are these apart?** According to lemma: Chernoff bound. Let $\\Theta_{1}, \\ldots, \\Theta_{N}$ be a sequence of i.i.d. random variables with mean $\\mathbb{E}[\\Theta]$ and range $[a, b]$. Then, for any $\\varepsilon&gt; 0$,$$\\mathbb{P}\\left[\\left|\\frac{1}{N} \\sum_{n=1}^{N} \\Theta_{n}-\\mathbb{E}[\\Theta]\\right|\\geq \\varepsilon \\right] \\leq 2 e^{-2N\\varepsilon^{2}(b-a)^2}$$ we claim that: $$\\mathbb{P}\\left[\\left|L_{\\mathcal{D}}(f)-L_{\\mathcal{S}_{\\text {test}}}(f)\\right| \\geq \\sqrt{\\left.\\frac{(b-a)^{2} \\ln (2 / \\delta)}{2\\left|\\mathcal{S}_{\\text {test }}\\right|}\\right)}\\right] \\leq \\delta$$, from which we could have the **insight** that the difference decreases as $\\mathcal{O}(1 / \\sqrt{\\left|S_{\\text {test }}\\right|})$. So the more data points we have, the more conﬁdent that the **empirical loss we measure is close to the true loss**. Cross Validation Splitting the data once into two parts (one for training and one for testing) is not the most efficient way to use the data. Cross-validation is a better way. Methods: See more K-fold Cross-Validation: Randomly partition the data into K groups. Each time leave out exactly one of the K groups for testing and use the remaining K-1 groups for training. Stratified K-fold: It is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set. Nested Cross-Validation: Always used for time series data, where particular care must be taken in splitting the data in order to prevent data leakage. It means to evaluate the model for time series data on the “future” observations. Nested Cross-Validation is a variation of k-fold which returns first k folds as train set and the (k+1) th fold as test set. Outcomes Cross-validation returns an unbiased estimate of the generalization error and its variance. We can average the K validation errors as the validation result. The purpose of cross-validation is model checking, not model building. Once we have used cross-validation to select the better performing model, we train that model on all the data. Cross-validation will not be used in complex deep learning model with a large dataset, since it’s too computationally expensive. Advantages Use All Data: Splitting the data once into two parts is not the most efficient way to use the data. It only gives predictions on part of the data (test set). If the test set we pick happens to contain a bunch of points that are particularly easy (or hard) to predict, we will not come up with a rational estimate of the model’s ability to learn and predict. Prevent Data Leakage: Work with Dependent/Grouped Data: For example, there are 3 speakers and 500 recordings for each speaker. If we do a random split, our training and test set will share the same speaker saying the same words, which will boost our algorithm performance but fail when tested on a new speaker. The proper way to do it is to split the speakers, i.e., use 2 speakers for training and the third for testing (cross-validation on the speakers level). Cross-validation can be used in models stacking, where we can’t train both our models on the same dataset because then, our second model will learn on ground truth information that our first model already seen. Bias–Variance Tradeoff The bias–variance tradeoff is the property of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples and vice versa. The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). Ideally, one wants to choose a model that both accurately captures the regularities in its training data (high variance), but also generalizes well to unseen data (high bias). There is an inherent bias-variance tradeoff when we perform the model selection. Error DecompositionFollowing Probabilistic Setup and test error defined above, we look at the mean squared error of the prediction function $f_{\\mathcal{S}}$ for a ﬁxed element $\\mathbf{x}_0$. Imagine that we are running the experiment many times: we create $\\mathcal{S}_{\\text {train }}$, learn the model $f_{\\mathcal{S}_{\\text {train }}}$, and then evaluate the performance by computing the mean squared error for $\\mathbf{x}_0$ ($y_0=f(\\mathbf{x}_0)+\\varepsilon$). $$ \\begin{split} & \\mathbb{E}_{\\mathcal{S}_{\\text {train }} \\sim \\mathcal{D}, \\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)+\\varepsilon-f_{\\mathcal{S}_{\\text {train }}}\\left(\\mathbf{x}_{0}\\right)\\right)^{2}\\right] \\\\ = &\\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]+\\mathbb{E}_{\\mathcal{S}_{\\text {train } \\sim \\mathcal{D}}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-f_{\\mathcal{S}_{\\text {train }}\\left(\\mathbf{x}_{0}\\right)}\\right)^{2}\\right] \\\\ &+2\\cdot \\underbrace{\\mathbb{E}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]}_{0}\\cdot\\mathbb{E}_{\\mathcal{S}_{\\text {train } \\sim \\mathcal{D}}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-f_{\\mathcal{S}_{\\text {train }}\\left(\\mathbf{x}_{0}\\right)}\\right)\\right] \\\\ = &\\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]+\\mathbb{E}_{\\mathcal{S}_{\\text {train } \\sim \\mathcal{D}}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-f_{\\mathcal{S}_{\\text {train }}\\left(\\mathbf{x}_{0}\\right)}\\right)^{2}\\right] \\\\ = & \\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]+\\mathbb{E}_{\\mathcal{S} \\sim \\mathcal{D}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-\\mathbb{E}_{\\mathcal{S}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]\\right) + \\left(\\mathbb{E}_{\\mathcal{S}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]-f_{\\mathcal{S}}\\left(\\mathbf{x}_{0}\\right)\\right)^2\\right] \\\\ =& \\underbrace{\\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}[\\varepsilon]}_{\\text {noise variance }} \\\\ & + \\underbrace{\\left(f\\left(\\mathbf{x}_{0}\\right)-\\mathbb{E}_{\\mathcal{S}_{\\text {train }}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}_{\\text {train }}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]\\right)^{2}}_{\\text {bias }}\\\\ & + \\mathbb{E}_{\\mathcal{S}_{\\text {train }} \\sim \\mathcal{D}}[\\underbrace{\\left(\\mathbb{E}_{\\mathcal{S}_{\\text {train }}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}_{\\text {train }}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]-f_{\\mathcal{S}_{\\text {train }}}\\left(\\mathbf{x}_{0}\\right)\\right)^{2}}_{\\text { variance }}] \\end{split} $$ Each of the three terms above is nonnegative, so each of them is a lower bound on the true error for the input $\\mathbf{x}_0$. $\\varepsilon$ is an irreducible error. The bias term is the square of the difference between the actual value $f\\left(\\mathbf{x}_{0}\\right)$ and the expected prediction $\\mathbb{E}_{S^{\\prime} \\sim \\mathcal{D}}\\left[f_{S^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]$. The variance term is the variance of the prediction function. Approaches Simple model, more data, less features, underfitting → Large bias Complex model, less data, more features, overfitting → Large variance Dimensionality reduction and feature selection can decrease variance by simplifying models. Learning algorithms typically have some tunable hyper-parameters that control bias and variance. For example: Linear and generalized linear models can be regularized to decrease variance at the cost of increasing bias. (It’s the essence of regulation to avoid overfitting) In artificial neural networks, the variance increases and the bias decreases as the number of hidden units increase. In k-nearest neighbor models, a high value of k (simple model) leads to high bias and low variance. In decision trees, the depth of the tree determines the variance. Ensemble learning is also one way of resolving the trade-off. For example, boosting combines many “weak” (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines “strong” learners in a way that reduces their variance. ReferenceCourse: Machine Learning@EPFLWikipedia: Bias–variance tradeoffBlog: 5 Reasons why you should use Cross-Validation in Data Science ProjectsStack Exchange: How to choose a predictive model after k-fold cross-validation?Zhihu: 求大神通俗易懂的解释下交叉验证cross-validation？","link":"/2019/10/31/model-selection-&-bias–variance-tradeoff/"},{"title":"Normalization in Deep Learning","text":"Normalization refers to a process that makes something more normal or regular. In statistics, normalization means adjusting values measured on different scales to a common scale, or adjustments to bring the entire probability distributions of adjusted values into alignment. Normalization in deep learning can make the model to train efficiently. Why does Normalization work?Intuition 1: Input Normalization $\\frac{X - \\mu}{\\sigma}$ Make training less sensitive to the scale of features. After optimization, the feature with a large scale will always be weighted with a small value ($\\omega_{small}$), and vise versa ($\\omega_{large}$). Because of the large scale of the feature, a tiny change in the small weight ($\\omega_{small}$) will change the prediction by a lot compared to the same change in the large weight ($\\omega_{large}$). It means that setting $\\omega_{small}$ correctly might dominate the optimization process and the feature with a large scale is of more importance which actually makes no sense. Make optimization well-conditioned. The rough intuition is that the cost function will be more round and easier to optimize when features are all on similar scales. Cost function of the unnormalized features can be a very elongated bowl, whereas if we normalize the features, then the cost function will look more symmetric. Having an elongated cost function, we might have to use a very small learning rate and gradient descent might need a lot of steps to oscillate back and forth before it finally gets to the minimum. If we have more spherical contours, then gradient descent can pretty much go straight to the minimum and we can take much larger steps with gradient descent without much oscillation. Intuition 2: Covariate Shift Covariate shift refers to the problem that the distribution of the input values changes, but the concept (model) being learned remains stationary. In deep learning, the basic idea behind normalization is to limit the covariate shift, which allows the model to learn on a more stable distribution, and would thus accelerate the training of the network. Batch NormalizationIn deep learning, we are particularly concerned with the change in the distribution of the inputs to the hidden nodes within a network. A neural network changes the weights of each layer over the training, which means that the activations of each layer change as well. Since the activations of a previous layer are the inputs of the next layer, each layer in the neural network is faced with a situation where the input distribution changes with each step (covariate shift). Batch Normalization at Training Time What batch normalization does is, especially from the perspective of the later layers of the neural network, it limits the earlier layers to not get to shift around much, by restricting them to have the same mean and variance. It weakens the coupling between what the early layers and the later layers. For a layer of the network with d-dimensional input, $x = (x^{(1)},…,x^{(d)})$, each dimension is then normalized separately as following: $$\\hat{x}_i^{(k)} = \\frac{x_i^{(k)}-\\mu_B^{(k)}}{\\sqrt{\\sigma_B^{(k)^2}+\\epsilon}}$$where $\\mu_B^{(k)}$ and $\\sigma_B^{(k)^2}$ are the per-dimension mean and variance, respectively. $\\epsilon$ is added in the denominator for numerical stability and is an arbitrarily small constant. The resulting normalized $\\hat{x}^{(k)}$ have zero mean and unit variance, to restore the representation power of the network or better take advantage of the nonlinearity (if activations are normalized), a transformation step then follows as:$$y_i^{(k)} = \\gamma^{(k)} \\hat{x}_{i}^{(k)} +\\beta^{(k)}$$where the parameters $\\gamma^{(k)}$and $\\beta^{(k)}$ are subsequently learnt in the optimization process and can convert the mean and variance to any value that the network desires. Formally, the transform $BN_{\\gamma,\\beta}: x_{1…m} \\rightarrow y_{1…m}$ is denoted as the Batch Normalizing Transform. There are some debates in deep learning about whether weshould normalize the value before the activation function or after it. In practice, normalizing before the activation function is done much more often. Regularization can be motivated as a technique to improve the generalizability of a learned model. Batch normalization has a slight regularization effect. The mean and variance are a little bit noisy because they are estimated on a mini-batch. Similar to dropout, batch normalization adds small noise to the hidden layers and therefore has a very slight regularization effect. We don’t particularly use to batch normalization as a regularization but use it as a way to speed up learning. Batch Normalization at Test Time At the test time, we use the $\\gamma$ and $\\beta$ learned from training, the $\\mu$ and $\\sigma^2$ are estimated from the training set. Running the whole training set to get $\\mu$ and $\\sigma^2$ could be memory-consuming. So we keep a running average of the $\\mu$ and $\\sigma^2$ for each layer as we train the neural network across different mini-batches to get an estimation of them. In practice, the moving average people usually use here is exponentially weighted average. An exponential moving average (EMA), also known as an exponentially weighted moving average (EWMA), is a first-order filter that applies weighting factors which decrease exponentially. The EMA for a series $Y$ is calculated recursively: $$S_t = \\begin{cases} \\alpha \\cdot Y_1, &amp; t = 1 \\\\ \\alpha \\cdot Y_t + (1 - \\alpha) \\cdot S_{t-1}, &amp; t &gt; 1\\end{cases}$$Where: $\\alpha$ represents the degree of weighting decrease, a constant smoothing factor between 0 and 1. $Y_t$ and $S_t$ are the values at a time period $t$ respectively. This formula can also be expressed in technical analysis terms as follows: How the EMA steps towards the latest datum point:$$\\text{EMA}_\\text{today} = \\text{EMA}_\\text{yesterday} + \\alpha \\left[\\text{price}_\\text{today} - \\text{EMA}_\\text{yesterday}\\right]$$ How the weighting factor on each data point $p_1$, $p_2$, etc., decreases exponentially by expanding out $\\text{EMA}_\\text{yesterday}$ :$$\\begin{split}\\text{EMA}_\\text{today}= &amp; { \\alpha \\left[p_1 + (1 - \\alpha) p_2 + (1 - \\alpha)^2 p_3 + (1 - \\alpha)^3 p_4 + \\cdots \\right] } \\\\= &amp;\\frac{p_1 + (1 - \\alpha) p_2 + (1 - \\alpha)^2 p_3 + (1 - \\alpha)^3 p_4 + \\cdots}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + (1 - \\alpha)^3 + \\cdots}\\end{split}$$since $1/\\alpha = 1 + (1 - \\alpha) + (1 - \\alpha)^2 + \\cdots$ (weighted average). A couple of things about choosing $\\alpha$: A higher $\\alpha$ discounts older observations faster so that the EMA will adapt more quickly to the current changes. When compute EMA, we can think of $S_t$ as approximately averaging over $\\frac{1}{\\alpha}$ number of data. So with a low value of $\\alpha$, the EMA is much smoother and also has more latency. (Always set to 0.1 in deep learning) Bias correction is a technical modification that makes EMA estimate more accurate, especially during its initial phase. The EMA is very low when the iteration just starts off and we will get a much lower value that is not a very good estimate of the first data. In order to fix the problem, we could instead of taking $S_t$, take $\\frac{S_t}{1-(1-a)^t}$, namely bias correction. It will make the EMA larger when $t$ is small and make almost no difference when $t$ is large enough. Weight NormalizationThe Limitations of Batch NormalizationThe key limitation of batch normalization is that it is dependent on the mini-batch. It puts a lower limit on the batch size. Ideally, we want to use the global mean and variance to normalize the inputs to a layer. However, it’s too costly and the mean and variance are simply estimates on mini-batch, which means that they contain a certain amount of noises. Smaller mini-batch sizes increase the variance of these estimates. It can be a problem in settings such as online learning and reinforcement learning which is highly sensitive to noise. It is difficult to apply to recurrent connections in recurrent neural network. In a recurrent neural network, the recurrent activations of each time-step will have different statistics. This means that we have to fit a separate batch normalization layer for each time-step and it forces us to store the statistics for each time-step during training. Theory of Weight Normalization What weight normalization does is it separates the norm of the weight vector from its direction. In an effort to speed up the convergence of optimization procedure, it proposes to reparameterize each weight vector $\\omega$ in terms of a parameter vector $\\boldsymbol{v}$ and a scalar parameter $g$ in the following way: $$\\omega = \\frac{g}{||\\boldsymbol{v}||}\\boldsymbol{v}$$It then optimizes both$\\boldsymbol{v}$ and $g$ using gradient descent. This reparameterization has the effect of fixing the Euclidean norm of the weight vector $\\omega$: we now have $||\\omega||=g$, independent of the parameters $\\boldsymbol{v}$. We, therefore, call this reparameterization weight normalization. Layer NormalizationTheory of Layer Normalization The key feature of layer normalization is that it normalizes the inputs across the features. The equations of layer normalization and batch normalization are similar:$$\\hat{x}_i^{(k)} = \\frac{x_i^{(k)}-\\mu_L}{\\sqrt{\\sigma_L^2+\\epsilon}}$$where $\\mu_L$ and $\\sigma_L^2$ are the mean and variance across the features, respectively. ReferenceQuora: Why do we normalize the dataCoursera: Deep Learning SpecializationBlog: An Intuitive Explanation of Why Batch Normalization Really WorksBlog: Weight Normalization and Layer Normalization ExplainedBlog: An Overview of Normalization Methods in Deep Learning","link":"/2019/10/14/normalization-in-deep-learning/"},{"title":"Optimization Algorithm in Deep Learning","text":"An optimization algorithm is a procedure which is executed iteratively by comparing various solutions until an optimum or a satisfactory solution is found. Here, we introduce some optimization algorithms commonly used in deep learning. Gradient Descent Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point as following:$$w = w - \\eta \\nabla Q(w) $$ Batch Gradient Descent Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated. Batch gradient descent decreases update frequency but takes much more time per iteration. Batch gradient descent has a more stable cost gradient and the cost goes down on every iteration. However, it may result in premature convergence of the model to a local minimum. It is an estimate of the function gradient. Batch gradient descent requires the entire training dataset in memory and available to the algorithm, which is memory-consuming. Stochastic Gradient Descent Stochastic gradient descent is a variation of the gradient descent algorithm that calculates the error and updates the model for each example in the training dataset. It can be regarded as a stochastic approximation of batch gradient descent optimization, since it replaces the actual gradient by an estimate. Stochastic gradient descent increases model update frequency and can result in faster learning on some problems. Stochastic gradient descent can be extremely noisy, which may cause the model error/cost to jump around (higher variance). It is an unbiased estimate of the full gradient. The noisy update process can allow the model to avoid local minima, but it also makes it hard to settle on a minimum (oscillate around the region of the minimum). Stochastic gradient descent loses the speed up from vectorization (matrix computation). Mini-Batch Gradient Descent Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients. Mini-batch gradient descent seeks to find a balance between the robustness of batch gradient descent and the efficiency of stochastic gradient descent.$$w = w - \\eta \\nabla Q(w) = w - \\eta \\sum_{i=1}^n \\nabla Q_i(w)/n$$ The cost of mini-batch gradient descent should trend downwards with noises (oscillations). Since some easy mini-batches might have a lower cost, while hard mini-batches (with some mislabeled examples) might have a higher cost. The larger step size can be adopted compared to stochastic gradient descent since the gradient is more stable. The batch size is a hyper-parameter that can be tuned as a power of two that fits the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, and so on. Adaptive Learning Rate DescentAdaGrad AdaGrad (adaptive gradient descent) algorithm is a modified stochastic gradient descent with per-parameter learning rate. AdaGrad has a base learning rate $\\eta$, but this is multiplied with the elements of a vector ($G_{j, j}$) which is the diagonal of the matrix:$$G = \\sum_{\\tau=1}^t g_\\tau g_\\tau^T$$ where $g_\\tau = \\nabla Q_i(w)$, the stochastic gradient at iteration $\\tau$. The diagonal is given by $G_{j,j} = \\sum_{\\tau=1}^t g_{\\tau,j}^2$. The formula for an update is now:$$w = w - \\eta, diag(G)^{-\\frac{1}{2}} \\circ g$$, where the $\\circ$ is the element-wise product. It can also be written as per-parameter updates $w_j = w_j - \\frac{\\eta}{\\sqrt{G_{j,j}}} g_j.$ AdaGrad increases the learning rate for sparser parameters and decreases the learning rate for ones that are less sparse. This strategy often improves convergence performance over sparse data. The intuition behind is that: with the number of updates increases, it is getting closer to the optimal solution, the learning rate supposed to be down to avoid oscillations. In AdaGrad, the learning rate will shrink with large updates or frequent updates.RMSProp RMSProp (Root Mean Square Propagation) is also a method in which the learning rate is adapted for each of the parameters. The idea is to divide the learning rate by an exponentially weighted average (detailed described in my previous blog) of the magnitudes of recent gradients. The formula for an update is shown below:$$\\begin{cases}v_w=\\gamma v_w+(1-\\gamma)(\\nabla Q(w))^2 \\\\w=w-\\frac{\\eta}{\\sqrt{v_w+\\epsilon}}\\nabla Q(w)\\end{cases}$$ where, $\\gamma$ is the forgetting factor, $(\\nabla Q(w))^2$ is component-wise squared and $\\epsilon$ is set to ${10}^{-8}$ to avoid blow up. The intuition of RMSProp is quite similar to Adagrad. The advantage of using the moving average instead of the accumulation of $(\\nabla Q(w))^2$ is that the learning rate can not only be gradually adapted smaller but larger if the feature becomes sparse in recent updates. Gradient Descent with Momentum Stochastic gradient descent with momentum remembers the update $\\Delta \\omega$ at each iteration, and determines the next update as a linear combination of the gradient and the previous update. In one sentence, the basic idea is to compute an exponentially weighted average of your gradients, and then use that gradient to update your weights instead. Stochastic gradient descent with momentum remembers the update $w$ at each iteration:$$\\begin{cases}{m_{w}=\\beta m_{w}+(1-\\beta) \\nabla Q(w)} \\\\ \\tilde{m}_ w = \\frac{m_{w}}{1-\\beta^{t}} \\\\{w=w-\\alpha \\tilde{m}_{w}}\\end{cases}$$where $t$ is the iteration, $\\alpha$ is the learning rate, $\\beta$ is the forgetting factor always set to 0.9 (average approximately over last 10 gradients). The name momentum stems from an analogy to momentum in physics. Unlike in classical stochastic gradient descent, it tends to keep traveling in the same direction, preventing oscillations. Momentum averages out oscillations of its gradients. The oscillations in some directions will tend to be averaged out and the gradients will get closer to zero. Adam Adam (Adaptive Moment Estimation ) combines RMSprop together with momentum and is an update to the RMSProp optimizer. Given parameters $w^{(t)}$ and a loss function $Q^{(t)}$, where $t$ indexes the current training iteration (indexed at 0), Adam’s parameter update is given by: $$\\begin{cases}m_w ^ {(t+1)} \\leftarrow \\beta_1 m_w ^ {(t)} + (1 - \\beta_1) \\nabla _w Q ^ {(t)}\\\\v_w ^ {(t+1)} \\leftarrow \\beta_2 v_w ^ {(t)} + (1 - \\beta_2) (\\nabla _w Q ^ {(t)} )^2\\\\\\hat{m}_w = \\frac{m_w ^ {(t+1)}}{1 - (\\beta_1) ^{t+1}}\\\\\\hat{v}_w = \\frac{ v_w ^ {(t+1)}}{1 - (\\beta_2) ^{t+1}}\\\\w ^ {(t+1)} \\leftarrow w ^ {(t)} - \\eta \\frac{\\hat{m}_w}{\\sqrt{\\hat{v}_w} + \\epsilon}\\end{cases}$$ where $\\epsilon$ is a small scalar used to prevent division by 0, and $\\beta_1$ and $\\beta_2$ are the forgetting factors for gradients and second moments of gradients, respectively. Squaring and square-rooting is done elementwise. Learning rate $\\eta$ is important and usually we need to try a range of values and see what works. A common choice for $\\beta_1$ is 0.9 and the hyper parameter $\\beta_2$ is recommend 0.999. $\\eta$ is always set to $10^{-8}$ and bias correction is implemented.Second-Order Methods… to be continued.ReferenceWiki: Stochastic gradient descentCoursera: Deep Learning SpecializationBlog: A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size","link":"/2019/10/16/optimization-algorithm-in-deep-learning/"}],"tags":[],"categories":[]}