{"pages":[],"posts":[{"title":"算法例题：经典题型","text":"一. nSum问题1. 2Sum：Leetcode 1. Two Sum有且只有一个答案，例如：[1,2,4]，3 12345678910class Solution(object): def twoSum(self, nums, target): table_dict = {} # 基于hash table，t=O(n)，s=O(n) for index, number in enumerate(nums): complement = target - number if complement in table_dict: return (index, table_dict[complement]) else: table_dict[number] = index 有多个答案，给出非重复答案，例如：[1,1,2,3,4]，5 12345678910111213141516171819202122class Solution(object): def twoSum(self, nums, target): # 因为有重复答案，故返回元素，而非index res, nums = [], sorted(nums) # t=O(nlgn)，s=O(1) left, right = 0, len(nums) - 1 while left &lt; right: left_val, right_val = nums[left], nums[right] two_sum = left_val + right_val if two_sum &lt; target: while left &lt; right and left_val == nums[left]: # 跳过重复数字，并避免溢出 left += 1 elif two_sum &gt; target: while left &lt; right and right_val == nums[right]: right -= 1 else: res.append([nums[left], nums[right]]) while left &lt; right and left_val == nums[left]: # 若left + right = target, 则同时移动left和right left += 1 while left &lt; right and right_val == nums[right]: right -= 1 return res 2. 3Sum：Leetcode 15. 3Sum123456789101112131415161718class Solution(object): def threeSum(self, nums): def three_sum_target(nums, target): # t = O(nlgn) + O(n^2) left, length = 0, len(nums) while left &lt; length - 2: # twoSum至少需要两个元素 left_val = nums[left] res_two = twoSum(nums[left + 1:], target - left_val) for arr in res_two: arr.append(nums[left]) res.append(arr) while left &lt; length - 2 and nums[left] == left_val: # 跳过首个元素的重复值 left += 1 res, nums = [], sorted(nums) three_sum_target(nums, 0) return res 3. nSum：Leetcode 18. 4Sum1234567891011121314151617181920212223242526272829303132333435class Solution(object): def fourSum(self, nums, target): def n_sum(nums, target, n): # nums已排序 res = [] if n == 2: left, right = 0, len(nums) - 1 while left &lt; right: left_val, right_val = nums[left], nums[right] if left_val + right_val &lt; target: while left &lt; right and nums[left] == left_val: left += 1 elif left_val + right_val &gt; target: while left &lt; right and nums[right] == right_val: right -= 1 else: res.append([nums[left], nums[right]]) while left &lt; right and nums[left] == left_val: left += 1 while left &lt; right and nums[right] == right_val: right -= 1 return res if n &gt; 2: left, right = 0, len(nums) - (n - 1) # 至少剩下n-1个元素 while left &lt; right: left_val = nums[left] n_sum_res = n_sum(nums[left + 1:], target - left_val, n - 1) # 递归 for arr in n_sum_res: arr.append(nums[left]) res.append(arr) while left &lt; right and nums[left] == left_val: left += 1 return res nums = sorted(nums) return n_sum(nums, target, n) 主要思路：Repeatedly finding the maximum element (considering ascending order) from unsorted part and putting it at the end；t=O(n^2), s=O(1)；确定一个位置的顺序，仅需一次交换。 三. ReferenceBlog: twoSum问题的核心思想Blog: 一个方法团灭 3Sum 4Sum 100Sum 问题","link":"/2020/07/24/classic-problems/"},{"title":"数学：凸优化与拉格朗日乘子法","text":"一. 凸优化（Convex optimaztion） 凸函数（Convex funcitons）：Let $f$ be a function defined over some domain $\\operatorname{dom} f$. $f$ is said to be convex, iff$$\\forall \\mathbf{a}, \\mathbf{b} \\in \\operatorname{dom} f, \\forall k \\in[0 , 1], f(k \\mathbf{a}+(1-k) \\mathbf{b}) \\leq k f(\\mathbf{a})+(1-k) f(\\mathbf{b})$$性质1: 凸函数的任一极小点为其全局极小点。性质2: 凸的充要条件：函数二阶可微（$\\nabla^{2} f(\\mathbf{x})$存在），且$\\nabla^{2} f(\\mathbf{x})$在定义域内半正定（$\\forall \\mathbf{w}, \\mathbf{w}^T\\mathbf{A}\\mathbf{w}\\geq 0$)，即$\\nabla^{2} f(\\mathbf{x}) \\succeq 0$。 凸优化，或叫凸最小化，研究凸函数的最小化的问题。 二. 拉格朗日乘数法（Lagrangian）拉格朗日乘数法，是多元函数在其变量受到一个或多个条件的约束时，寻找其极值的方法，它将有$d$个变量与$k$个约束条件的最优化问题，转换为有$d+k$个变量的无约束优化问题求解。 等式约束： $\\begin{array}{ll}\\min & f(\\mathbf{x}) \\\\ s.t. & h(\\mathbf{x})=0\\end{array}$ 设$f(\\mathbf{x})$与$h(\\mathbf{x})$连续、可导，$f(\\mathbf{x})$在$h(\\mathbf{x})=0$约束下取极值时，两者的等高线在极值处相切，即有共线的法向量，又因为梯度向量是等高线的法线，假设$\\mathbf{x}^{\\star}$为满足约束条件的最优解，可得$\\nabla f(\\mathbf{x}^{\\star})+\\lambda \\nabla g(\\mathbf{x}^{\\star})=\\mathbf{0}$。 定义Lagrangian函数：$L(\\mathbf{x}, \\lambda)=f(\\mathbf{x})+\\lambda h(\\mathbf{x})$，Lagrangian无约束优化问题最优解（最大、最小化）在其导数为0处取到，即： $$\\begin{aligned} \\nabla_{\\mathbf{x}} L&=\\frac{\\partial L}{\\partial \\mathbf{x}}=\\nabla f(\\mathbf{x}^{\\star})+\\lambda \\nabla g(\\mathbf{x}^{\\star})=\\mathbf{0} \\quad \\text{(stationary equation)}\\\\\\nabla_{\\lambda} L&=\\frac{\\partial L}{\\partial \\lambda}=g(\\mathbf{x})=0\\end{aligned}$$ 所得上式，恰为原等式约束优化问题最优点需满足的条件（曲线相切、等式约束）。Lagrangian无约束优化与原等式约束优化，在相同处取到的最优解。 不等式约束： $\\begin{array}{ll}\\min & f(\\mathbf{x}) \\\\ s.t. & g(\\mathbf{x})\\leq0\\end{array}$ 假设$\\mathbf{x}^{\\star}$为满足约束条件的最优解： $g(\\mathbf{x}^{\\star})&lt;0$，最优解位于可行域的内部，此时约束条件无效，约束优化问题退化为无约束优化问题，$\\lambda=0$。 $g(\\mathbf{x}^{\\star})=0$，最优解位于可行域的边界，此时约束条件有效。又因为梯度方向指向函数上升最快的方向，故$\\nabla_{g}(\\mathbf{x}^{\\star})$应该指向可行域的外部，$\\nabla_{f}(\\mathbf{x}^{\\star})$应指向内部(否则，内部有更小值），故$\\nabla f(\\mathbf{x}^{\\star})=-\\lambda \\nabla g(\\mathbf{x}^{\\star})$，$\\lambda&gt;0$。 故综上，可得$\\lambda \\geq 0$（dual feasibility）、$\\lambda g(\\mathbf{x}^{\\star}) = 0$（complementary slackness）。 多个等式与不等式约束推广至多个等式与不等式约束的情况，一般的约束最优化问题可写成如下形式(原问题，primal problem)： $$\\begin{array}{ll}\\min & f(\\mathbf{x}) \\\\ s.t. & g_{i}(\\mathbf{x}) \\leq 0, \\quad i=1, \\ldots, m \\\\ & h_{i}(\\mathbf{x})=0, \\quad i=1, \\ldots, p\\end{array}$$ ，其对应的Lagrangian函数为：$$L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu})=f(\\mathbf{x})+\\sum_{i=1}^{m} \\lambda_{i} g_{i}(\\mathbf{x})+\\sum_{i=1}^{p} \\mu_{i} h_{i}(\\mathbf{x})$$，设$\\mathbf{x}^{\\star}$、$\\boldsymbol{\\lambda}^{\\star}$、$\\boldsymbol{\\mu}^{\\star}$为满足约束条件的最优解，其满足“KKT条件”： primal feasibility：$g\\left(\\mathbf{x}^{\\star}\\right) \\leq 0, h\\left(\\mathbf{x}^{\\star}\\right)=0$ dual feasibility：$\\lambda^{\\star} \\geq 0$ complementary slackness：$\\lambda_{i}^{\\star} g_{i}\\left(\\mathbf{x}^{\\star}\\right)=0 \\quad \\forall i=1, \\ldots, m \\quad$ stationarity: $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)+\\sum_{i=1}^{m} \\lambda_{i}^{\\star} \\nabla g_{i}\\left(\\mathbf{x}^{\\star}\\right)+\\sum_{i=1}^{p} \\mu_{i}^{\\star} \\nabla h_{i}\\left(\\mathbf{x}^{\\star}\\right)=0$ 三. 原问题与对偶问题（Primal/Dual）所以可得，$$ \\text{Primal}= \\min_{\\mathbf{x}}f(\\mathbf{x})=\\min_\\mathbf{x}\\max_{\\boldsymbol{\\lambda} \\succeq 0, \\boldsymbol{\\mu}} L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = p^{\\star}$$显然，由于$h\\left(\\mathbf{x}\\right)=0$、$\\lambda g\\left(\\mathbf{x}\\right)\\geq 0$，有$\\max_{\\boldsymbol{\\lambda} \\succeq 0, \\boldsymbol{\\mu}} L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu})=f(\\mathbf{x})$，即前述所说的“Lagrangian无约束优化与原约束优化，在相同处取到的最优解”。 定义对偶函数$\\Gamma(\\boldsymbol{\\lambda}, \\boldsymbol{\\mu})=\\inf _{\\mathbf{x} \\in \\mathbb{D}} L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) \\leq L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu})\\leq f(\\mathbf{x})$，其代表了主问题最优值的下界（由$\\boldsymbol{\\lambda}$、$\\boldsymbol{\\mu}$决定），对偶问题（dual problem）：$$\\text{Dual} = \\max _{\\boldsymbol{\\lambda}\\succeq0, \\boldsymbol{\\mu}} \\Gamma(\\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = \\max_{\\boldsymbol{\\lambda} \\succeq 0, \\boldsymbol{\\mu}} \\min_\\mathbf{x} L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = d^{\\star}$$， 给出了基于对偶函数能获得的最好下界。 对偶性由于$d^{\\star}$是主问题的一个下界，故有$d^{\\star} \\leq p^{\\star}$，这称为”弱对偶性“ (weak duality)，若$d^{\\star} = p^{\\star}$， 则称为”强对偶性“ (strong duality)。 对偶性与KKT条件： 对于一般（或非凸优化）优化问题：当强对偶性成立时，原问题、对偶问题在相同点（相同的$\\mathbf{x}^{\\star}$、$\\boldsymbol{\\lambda}^{\\star}$、$\\boldsymbol{\\mu}^{\\star}$）处取得最优值，其满足KKT条件）。KKT是最优性的必要条件，此时，满足KKT条件的的点，不一定为最优点。 对于凸优化问题（$f(\\mathbf{x})$、$g(\\mathbf{x})$为凸函数，$h(x)$为仿射函数）：且满足Slater条件（可行域中至少有一点使不等式约束$g(\\mathbf{x})&lt; 0$严格成立），强对偶性成立，那么KKT条件是最优性的充要条件。此时，可以通过求解KKT条件，来求解优化问题。 无论原问题（Primal problem）是什么形式，对问题（Dual problem）总是一个凸优化问题。 于是，对于难以求解的原问题，可以通过优化（或随机代入）对偶问题来得到原问题的下界估计。在强对偶性成立时，可以通过求解对偶问题，来获得原问题的最优值（$d^{\\star} = p^{\\star}$）。在SVM中，便是如此这样做的。 四. ReferenceZhihu: Karush-Kuhn-Tucker (KKT)条件Zhihu@戏言玩家/徐廷霆回复: 如何理解拉格朗日乘子法？Blog: 凸优化（八）——Lagrange对偶问题Blog: Convex functionsBlog: “支持向量机系列” 的番外篇一: Duality","link":"/2020/06/11/compact-convex-optimization/"},{"title":"Data Leakage","text":"Data leakage happens when your training data contains information about the target, but similar information will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production. There are two main types of leakage: target leakage and train-test contamination. Leaky PredictorsThis occurs when your predictors include data that will not be available at the time you make predictions. To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Train-Test ContaminationValidation is meant to be a measure of how the model does on data that it hasn’t considered before. You can corrupt this process if you leak data from the validation set into the training set. This is called train-test contamination. If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps (e.g. fitting an imputer for missing values, normalization) before train-test split. Negative examples as below: “In order to avoid problems with concept drift, I assigned articles to training or test alternating between the two. Performance on the test set was astonishingly good even when it was obvious that the model was over fitting. As it turned out, the data set had lots of near-duplicate articles so many articles in the training set also appeared in the test set. This duplication is coupled with an incautious way of splitting the training data wreaked havoc on my results.” – Ted Dunning from Quora “For example, there are 3 speakers and 500 recordings for each speaker. If we do a random split, our training and test set will share the same speaker saying the same words, which will boost our algorithm performance but fail when tested on a new speaker. The proper way to do it is to split the speakers, i.e., use 2 speakers for training and the third for testing.” – Dima Shulga from Medium ResolutionHere are several strategies to find and eliminate data leakage: Exploratory data analysis (EDA) can be a powerful tool for identifying data leakage. If the performance of your algorithm is too good to be true, data leakage may be the reason. Perform early in-the-field testing of algorithms. Any significant data leakage would be reflected as a difference between estimated and realized out-of-sample performance. This is perhaps the best approach in identifying data leakage, but it can also be challenging to isolate the cause of such performance discrepancy as data leakage since the cause actually could be classical over-fitting, sampling bias, etc. ReferenceBlog: Discriminative Modeling vs Generative ModelingKaggle: What is Data LeakageBlog: Ask a Data Scientist: Data Leakage","link":"/2019/11/09/data-leakage/"},{"title":"Entropy in Information Theory","text":"Entropy (information theory), also called Shannon entropy, a measure of the unpredictability or information content of a message source. Huffman Coding TreeTo shorten the length of symbol encoding, people naturally hope that the higher the frequency of symbols, the shorter its encoding, so as to save the storage space. Huffman coding is a kind of data compression (encoding) algorithm based on a special binary tree (Huffman coding tree) and can solve the issue above. It assigns codes to characters (or any discrete source symbols) such that the length of the code depends on the relative frequency or weight of the corresponding character. The output from Huffman’s algorithm can be viewed as a variable-length code table. The algorithm derives this table (encoding) from the estimated probability or frequency for each possible symbol value as below: Given a set of symbols and their weights (usually proportional to probabilities). Find a binary tree with minimum weighted path length (path length $\\times$ leaf-node weight) from the root, which will put the symbol with the low weight far away from root and vice versa. Select two adjacent nodes with the lowest sum of weight and create a new parent node whose weight is this sum. Do this process iteratively until the only root node is created. After building the Huffman coding tree, each symbol can be: Encoding: We can encode the symbol following the path from the root to its leaf-node. As a common convention, bit ‘0’ represents following the left child, and bit ‘1’ represents following the right child. Encoding a string can be done by replacing each letter in the string with its binary code (the Huffman code). Decoding: Decoding an encoded string can be done by looking at the bits in the coded string from left to right until a letter decoded. EntropyGenerally, entropy refers to disorder or uncertainty (a large entropy represent a great disorder), and the entropy used in information theory can reflect the informative extent of the event. When the data source produces a low-probability event, the event carries more “information” than when the high-probability event. Obviously, it has the same inner logic as Huffman Coding, so the probability of the event can be encoded by Huffman Coding Tree, and from it, we can compute the amount of information (information entropy) the event carries. Information entropy is defined as the expected rate (bits), which is actually the expected length of the Huffman Coding Tree path (one edge in path represents 1 bit coding). The measure of information entropy is the negative logarithm of the probability mass function for the value:$$S=-\\sum _{i}P_{i}\\underbrace{\\log {P_{i}}}_\\text{Length of Huffman Tree Path}=-\\operatorname {E} _{P}[\\log P]\\geq0$$, where ${E} _{P}[X]=\\sum _{i}P_{i}X_{i}$ is the expectation defined by the probability $P$. For example: A discrete random variable $X$ whose distribution $Q$ has three events: $A: \\frac{1}{2}$, $\\quad B: \\frac{1}{4}$, $\\quad C: \\frac{1}{4}$. So the events $A, B,C$ can be encoded by Huffman Tree as below: The information entropy of $X$ is:$$H(Q)=-\\frac{1}{2} \\log _{2}\\left(\\frac{1}{2}\\right)-\\frac{1}{4} \\log _{2}\\left(\\frac{1}{4}\\right)-\\frac{1}{4} \\log _{2}\\left(\\frac{1}{4}\\right)=1.5$$ Cross-EntropyIn information theory, the cross entropy between two probability $p$ and $q$ (over the same underlying set of events) measures the average number of bits needed to identify an event drawn from the set if a coding scheme is optimized for an estimated probability distribution $Q$, rather than the true distribution $P$. Shortly, the cross-entropy is the expected rate (bits) if the Huffman Tree is constructed according to the estimated distribution rather than the true distribution. Continue the example in Entropy: The actual distribution is $A: \\frac{1}{2}$, $\\quad B: \\frac{1}{4}$, $\\quad C: \\frac{1}{4}$, while the estimated distribution is $A: \\frac{1}{4}, \\quad B: \\frac{1}{4}, \\quad C: \\frac{1}{2}$, the expected rate (bits) will thus be computed as:$$H(P, Q)=-\\frac{1}{2} \\log _{2}\\left(\\frac{1}{4}\\right)-\\frac{1}{4} \\log _{2}\\left(\\frac{1}{4}\\right)-\\frac{1}{4} \\log _{2}\\left(\\frac{1}{2}\\right)=1.75$$ KL-DivergenceThe KL-Divergence (Kullback–Leibler divergence, also called relative entropy) is a measure of how one probability distribution is different from a reference probability distribution. KL-Divergence is actually the expected extra bit length if a code that is optimal for a given (wrong) distribution $Q$ is used, compared to using a code based on the true distribution $P$. Thus, it is the difference between Cross-Entropy and Entropy:$$D_{\\mathrm{KL}}(P | Q) = H(P, Q) - H(Q)\\geq0$$ Cross-Entropy minimizationCross-entropy minimization is frequently used as a loss function in optimization. It has the following properties: When comparing a distribution $P$ against a fixed reference distribution $Q$, cross-entropy and KL divergence are identical up to an additive constant (since $Q$ is fixed). Both of them take on their minimal values when $P=Q$, which is $0$ for KL divergence, and $H(P)$ for cross-entropy. Both of them are continuous and differentiable which benefits the gradient computation. ReferenceBlog: 详细图解哈夫曼Huffman编码树Blog: 数据结构和算法Implementation——Huffman树和Huffman编码Blog: Huffman codingBlog: 熵与信息增益Blog: KL散度与交叉熵区别与联系Zhihu@erwin: 如何通俗的解释交叉熵与相对熵?","link":"/2019/11/30/entropy-in-information-theory/"},{"title":"Google Cloud for Deep Learning - Part 1","text":"Google Cloud is a suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products. Google compute engine instance is a virtual machine (VM) hosted on Google Cloud where you could run our GPU-enabled deep learning project. What the most important is that a $300 trial credit is offered by google to try this cloud services! The series of passages aiming to use the Google Cloud for deep learning has two parts: The first part is about creating a VM instance on the Google cloud and the second part is about installing some deep learning related configurations (jupyter notebook and GPUs). Let’s take a step to the Part 1! Create a VM InstanceStep 1: Create a Google Account with CreditYou should activate your free trial to get $300 credit for the trial. Your payment information will be required but you won’t be charged unless you manually upgrade to a paid account. So don’t be too worried about the autocharge! Step 2 : Create a New ProjectClick the “project button“ in the top left cornor (right to the “Google Cloud Platform“ icon) to create a new project. Step 3: Create a VM InstanceClick on the “three lines button“ (menu) on the top left corner and then click on “Compute Engine“. There some tips for the new VM instance: Select a region with zone that has GPU resources, e.g. “us-west1-a”. Customize your machine type (CPU, GPU) as you want, which can be adjusted after the creation. Usually select “Ubuntu 16.04 LTS“ as the image in your boot disk. Tick both “http“ and “https“ under the firewall options to allow the network traffic. Expand the drop-down menu “Management, security, disks …“, choose the “disk“ tab and untick “Delete boot disk when instance is deleted“. You can also add an additonal disk if you want. IMPORTANT : DON’T FORGET TO STOP YOUR GPU INSTANCE, OTHERWISE GCP WILL KEEP CHARGING. Step 4: Request for Increase in QuotaYou probably get an error “Quota ‘GPUS_ALL_REGIONS’ exceeded. Limit: 0.0 globally“, which requires you to increase the quota. Click “Quotas“ in “IAM &amp; Admin“, send a quota increase request for “Compute Engine API GPUs (all regions)“ to at least set the limit to 1. An email will be sent automatically and your application will be processed in following days. If you want to use mutiple GPUs in one VM instance, you need to increase the quota for a specific kind of GPUs in the same zone as your VM instance as well, such as “Compute Engine API NVIDIA K80 GPUs“ in “us-west1-a”. Step 5: Connecting to the VM via SSHThree of those methods will be mentioned briefly. Method 1: SSH from the browser Method 2: Log in via SSH password verification Log in using method 1, switch to root and reset the passward: 12$ sudo -i $ sudo passwd root Edit the SSH configuration file: 12$ vim /etc/ssh/sshd_config# Eneter :wq to quit with change; Eneter :q! to quit without change and change things as following: 12PermitRootLogin yesPasswordAuthentication yes Restart SSH and activate new setting: 1$ service sshd restart Connect to Google Cloud using passward set above. 12$ ssh user@host# First time: Type yes if any notification host here should be the external-ip as below: Method 3 :Log in via the local private key file Generate SSH key file (in macOS): 1$ ssh-keygen -t rsa Copy the public key: 123$ cat ~/.ssh/id_rsa.pub# ...45UVp1 user@computer-name.local# Change user@computer-name.local as the user you want to login, e.g. root Paste it to “Compute Engine“ → “Metadata“, Google will write the public key to ~/.ssh/authorized_keys: Use the corresponding private key to login and enter passphrase for key: 1$ ssh -i ~/.ssh/id_rsa user@host Small Tips:Modifying Existing InstancesYou can modify resources (CPU/GPUs) on your existing instances, but you must first stop the instance and change its host maintenance. At the top of the instance details page, click “Stop“ to stop the instance. After the instance stops running, click “Edit“ to change the instance properties. ReferenceBlog: Running Jupyter Notebook on Google Cloud Platform in 15 minBlog: Mac使用SSH登录Google Cloud PlatformBlog: Google Cloud Platform SSH 连接配置","link":"/2019/10/17/google-cloud-for-deep-learning-part-1/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/08/22/hello-world/"},{"title":"Independent and Identically Distributed","text":"DefinitionIn probability theory and statistics, a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. In machine learning theory, i.i.d. assumption is often made for training datasets to imply that all samples stem from the same generative process ($y=f(\\mathbf{x})+\\varepsilon$), which means $\\left(\\mathbf{x}_{n}, y_{n}\\right)$ are i.i.d. samples according to $\\mathcal{D}$ ($S=\\left\\{ \\left(\\mathbf{x}_{n}, y_{n}\\right) \\text {i.i.d.} \\sim \\mathcal{D}\\right\\}_{n=1}^{N}$) and that the generative process is assumed to have no memory of past generated samples. Independent means the joint distribution can “splits up” into n individual marginals, one for each random variable:$$f_{X_{1}, \\ldots, X_{n}}\\left(x_{1}, \\ldots, x_{n}\\right)=f_{X_{1}}\\left(x_{1}\\right) \\times \\cdots \\times f_{X_{n}}\\left(x_{n}\\right)$$ Identical means they all have the same identical probability mass functions. Or in coin flip case, each random variable shares the same parameter p.$$f_{X_{1}}\\left(x_{1}\\right) \\times \\cdots \\times f_{X_{n}}\\left(x_{n}\\right)=\\left[p^{x_{1}}(1-p)^{1-x_{1}}\\right] \\times \\cdots \\times\\left[p^{x_{n}}(1-p)^{1-x_{n}}\\right]$$ ExplanationThe assumption (or requirement) that observations be i.i.d. tends to simplify the underlying mathematics of many statistical methods (e.g. maximizing the log-likelihood, minimizing the empirical risk, bootstrap aggregation, random splitting in model assessment, see more). In practical applications of statistical modeling, however, the assumption may or may not be realistic. To partially test how realistic the assumption is on a given data set, the correlation can be computed. In many problems, data is required to be sampled from the same distribution because it is hoped that the model trained with the training data set can be reasonably applied to the test set. Feature engineering is actually a way to remove noises and expose the latent mutual distribution. *Note about the Distribution (statistical model):In statistical classification, including machine learning, two main approaches are called the generative approach and the discriminative approach. Given an observable variable $X$ and a target variable $Y$, a generative model is a statistical model of the joint probability distribution on $X \\times Y$, $P(X, Y)$. (E.g. Naive Bayes, Gaussian Mixture Model) A discriminative model is a model of the conditional probability of the target $Y$, given an observation $x$, symbolically, $P(Y | X=x)$. (E.g. kNN, Logistic regression, SVM, Neural networks) Contrast: A generative algorithm models how the data was generated in order to categorize a signal. It predicts the most possible known label for the unknown variable using Bayes Rules (It tries to learn $p(X,Y)$ which can be transformed into $p(Y|X)$ later). A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal. So, discriminative algorithms try to learn $p(Y|X)$ directly from the data. ExamplesThe following are examples or applications of i.i.d. random variables: A sequence of fair or loaded dice rolls is i.i.d. A sequence of fair or unfair coin flips is i.i.d. The following examples data samples do not satisfy i.i.d. assumption: A medical dataset where multiple samples are taken from multiple patients, it is very likely that samples from same patients may be correlated. Samples drawn from time dependent processes, for example, year-wise census data. ReferenceStackExchange: On the importance of the i.i.d. assumption in statistical learningStackExchange: What does it mean by independently and identically distributed random variables? Zhihu: 独立同分布 independent and identically distributed","link":"/2019/11/06/independent-and-identically-distributed/"},{"title":"机器学习：集成学习（Ensemble Learning）","text":"一. 概念集成学习（Ensemble Learning）：结合弱学习器（weak learners），降低偏差（bias）或方差（variance），以获得（泛化）性能显著优越的强学习器（strong learner）。 同质学习器（Homogeneous learners）：用相同的学习算法，以同一参数，在不同数据集（采样或加权差异）上训练得到的学习器。 异质学习器（Heterogeneous learners）：用不相同的学习算法，在相同数据集上训练得到的学习器。 经验法则（Rule of Thumb） 要获得好的集成，即比最好的弱学习器更好的性能，弱学习器应“好而不同”，具有“准确性”和“多样性”（学习器间具有差异）。 弱学习器的选择应与集成方法一致，若选择“低偏差，高方差”的基本学习器，则应使用倾向于“减少方差”的集成方法，反之亦然。 二. Bagging（Bootstrap AGGregatING）Bagging通常使用同质弱学习器，并进行并行训练/学习，在预测时按如下方式结合： $$\\hat{f}_{b a g}=\\left\\{\\begin{array}{ll}{\\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}^{b}(x)} & {\\text { Averaging for Regression Problems }} \\\\ {\\underset{b=1 \\ldots B}{\\arg \\max } \\hat{f}^{b}(x)} & {\\text { Voting for Classification Problems }}\\end{array}\\right.$$ 假设$Y_{1}, Y_{2}, \\ldots, Y_{n}$代表了$n$个弱学习器的预测结果（方差为$\\sigma^{2}$的随机变量），若训练集独立同分布，则为$Y_{1}, Y_{2}, \\ldots, Y_{n}$独立同分布。预测结果均值的方差为$\\operatorname{Var}\\left(\\frac{Y_{1}+Y_{2}+\\ldots+Y_{n}}{n}\\right)=\\operatorname{Var}\\left(\\frac{Y_{1}}{n}\\right)+\\ldots+\\operatorname{Var}\\left(\\frac{Y_{n}}{n}\\right)=\\frac{\\sigma^{2}}{n}$，方差从$\\sigma^{2}$降低为$\\frac{\\sigma^{2}}{n}$。 通常，多个独立同分布的训练集难以获得（潜在分布未知），通过自助法（bootstrapping）在原始数据集中有放回地随机采样（random sampling with replacement），可以获得多个近似独立同分布的训练集。自助法的使用通常需满足以下假设： 1）原始数据集的样本数量$N$应足够大，足以描绘数据的真实分布，以便采样的数据也能很好地近似真实分布（代表性）。 2）$N$相对于训练集数目$B$应足够大，以使训练集之间的相关性不大（独立性）。 包外误差（Out-of-Bag Error）：使用未在训练集中出现的样本作为“测试集”，测试获得的误差。Bagging可以通过计算OOB误差来进行评估，而不需要交叉验证。 随机森林法（Random Forest）：以方差较大的决策树（深，并在训练过程中引入了随机属性选择）为基学习器，构建Bagging集成，以产生方差较低的输出，详见随机森林法。 三. BoostingBoosting通常使用同质弱学习器，并进行串流训练/学习，在预测时加权结合： $$\\hat{F}(\\mathbf{x})=\\sum_{i=1}^{M} \\gamma_{i} h_{i}(\\mathbf{x})+\\text { const. }$$ Boosting提高稳定性，减小偏差，它所包含的基学习器应是低方差、高偏差。 Boosting中的弱学习器按顺序，拟合加权的原始数据集。拟合过程是适应性（adaptive）的，不能被先前拟合的弱学习器，较好处理的数据，将被分配较大的权重，在下一个弱学习器训练过程中受到更多关注，详见提升树。 四. Stacking四. ReferenceBlog: Model Ensemble Blog：机器学习：树型模型（Tree-Based Learning Algorithms）","link":"/2020/06/14/interview-ensemble/"},{"title":"机器学习：支持向量机（Support Vector Machine）","text":"一. 概念支持向量机（Support Vector Machine，SVM）是经典的二分类模型，其直观地定义为特征空间上，间隔最大（最鲁棒）的线性分类器。SVM学习的优化目标是最大化间隔，可以转化为凸二次规划（convex quadratic programming）问题。在线性模型的基础上，引入核函数（核技巧）可以得到非线性模型。 二. 硬间隔(Hard margin)SVM试图找到位于两类样本正中间的超平面，因为其鲁棒性最好，泛化能力最强。 上图实线即划分超平面。在线性模型中，超平面可通过方程$\\mathbf{w}^T\\mathbf{x}+b=0$描述，其中$\\mathbf{w}$是超平面的法向量（决定超平面方向），$b$为位移项（决定超平面和原点之间的距离）。 假设超平面能将所有训练样本正确分类（硬间隔，hard margin），那么对于所有标记为+1的点，有$\\mathbf{w}^T\\mathbf{x}_i+b&gt;0$，对所有标记为-1的点，有$\\mathbf{w}^T\\mathbf{x}_i+b&lt;0$。若超平面存在，那么可以对$\\mathbf{w}$和$b$进行线性放缩，使得任意$\\left(\\mathbf{x}_i, y_{i}\\right) \\in D$： $$\\mathbf{w}^T\\mathbf{x}_i+b\\geq+1,\\quad y_i = +1 \\\\ \\mathbf{w}^T\\mathbf{x}_i+b\\leq-1,\\quad y_i = -1$$ 使上式等号成立的训练样本称为支持向量（support vector），其为距离超平面最近的样本点，距离为$\\gamma=\\frac{1}{\\Vert\\mathbf{w}\\Vert}$（样本主空间中任意点$\\mathbf{x}$，到超平面的距离为$r=\\frac{\\left|\\mathbf{w}^{T} \\mathbf{x}+b\\right|}{\\Vert\\mathbf{w}\\Vert}$）。定义间隔（margin）为两个异类支持向量到超平面的距离之和，为$\\gamma=\\frac{2}{\\Vert\\mathbf{w}\\Vert}$。 证明：设$\\mathbf{x}_0$为$\\mathbf{x}$在超平面$(\\mathbf{w}, b)$上的投影（向量）。由上图可知，$\\mathbf{x} = \\mathbf{x_0} + \\mathbf{r}$，且$\\mathbf{r}=\\frac{\\mathbf{w}}{\\Vert\\mathbf{w}\\Vert} \\cdot r$，代入可得$\\mathbf{x_0} = \\mathbf{x} - \\frac{\\mathbf{w}}{\\Vert \\mathbf{w} \\Vert} \\cdot r$。又因为点$\\mathbf{x_0}$在超平面上，所以有$\\mathbf{w}^T\\mathbf{x_0}+b=0$。消去$\\mathbf{x_0}$，得到$$\\mathbf{w}^T\\mathbf{x_0}+b= \\mathbf{w}^T(\\mathbf{x} - \\frac{\\mathbf{w}}{\\Vert \\mathbf{w} \\Vert} \\cdot r)+b= 0 \\Rightarrow r = \\frac{\\mathbf{w}^T\\mathbf{x}+b}{\\Vert \\mathbf{w} \\Vert}$$，取距离为正值（法向量可能为图示反向），即$r = \\frac{|\\mathbf{w}^T\\mathbf{x}+b|}{\\Vert \\mathbf{w} \\Vert}$。 SVM的目标是找到具有最大间隔（maximum margin）的划分超平面，即$\\max_{\\mathbf{w},b} \\frac{2}{\\Vert \\mathbf{w} \\Vert}$，将目标函数写成最小化（$\\max_{\\mathbf{w},b} \\frac{2}{\\Vert \\mathbf{w} \\Vert} \\Rightarrow \\min_{\\mathbf{w},b} \\frac{1}{2}\\Vert \\mathbf{w} \\Vert^{2}$），可得支持向量机的基本型： $$\\min_{\\mathbf{w},b} \\frac{1}{2} \\Vert \\mathbf{w} \\Vert^2 \\\\\\text{s.t.} \\quad y_i(\\mathbf{w}^T\\mathbf{x}_i+b) \\geq 1, \\quad i=1,2,...,m$$ 三. SVM与凸优化由上式可看出，SVM为凸二次规划 (convex quadratic programming) 问题，可以通过随机梯度下降（stochastic gradient descent）求解最优值。 另外，也可以使用拉格朗日乘子法（详见），通过对偶问题 (此时，强对偶性成立)求解，其降低了计算开销，并为核函数$\\phi(\\mathbf{x})$的使用提供基础。其拉格朗日函数如下：$$L(\\mathbf{w},b,\\mathbf{a}) = \\frac{1}{2} \\Vert \\mathbf{w} \\Vert^2 + \\sum_{i=1}^m a_i(1-y_i(\\mathbf{w}^T\\mathbf{x}_i+b))$$其中，$\\mathbf{a} = (a_1;a_2;…;a_m)$为拉格朗日乘子。将拉格朗日函数分别对原变量$\\mathbf{w}$、$b$求偏导，并令导数为0，可得$ \\mathbf{w}^{\\star} = \\sum_{i=1}^m a_i y_i \\mathbf{x}_i$、$0 = \\sum_{i=1}^m a_i y_i$（本质为KKT条件）。由于强对偶性成立，原问题与对偶问题有相同的最优解，故代入拉格朗日函数（$\\min_{\\mathbf{w}, b} = \\mathbf{w}^{\\star}, b^{\\star}$，消去$\\mathbf{w}$、$b$），可得对偶问题：$$ \\max_{\\mathbf{a}} \\sum_{i=1}^m a_i - \\frac{1}{2} \\sum_{i=1}^m\\sum_{j=1}^m a_i a_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j \\\\ \\text{s.t.} \\quad \\sum_{i=1}^m a_i y_i = 0, \\quad a_i \\geq 0, \\quad i=1,2,...,m $$只要求出对偶问题的解$\\mathbf{a}^{\\star}$，即可推出$ \\mathbf{w}^{\\star} = \\sum_{i=1}^m a_i y_i \\mathbf{x}_i$和$b^{\\star}$(如下)，从而得到预测模型：$$\\begin{split}f(\\mathbf{x}) &amp;= \\mathbf{w^{\\star}}^T \\mathbf{x} + b^{\\star}\\&amp;= \\sum_{i=1}^m a_i^{\\star} y_i \\mathbf{x}_i^T \\mathbf{x} + b^{\\star}\\end{split}$$ 为什么称为支持向量机？由于强对偶性成立，故最优点$\\mathbf{a}^{\\star}$、$\\mathbf{w}^{\\star}$、$b^{\\star}$满足KKT条件： $$\\left\\{\\begin{array}{l}y_i(\\mathbf{w^{\\star}}^T\\mathbf{x}_i+b) - 1 = y_{i} f\\left(\\mathbf{x}_{i}\\right)-1 \\geq 0 , \\quad i=1,2,...,m\\\\ a_{i}^{\\star} \\geq 0 , \\quad i=1,2,...,m\\\\ a_{i}\\left(y_{i} f\\left(\\mathbf{x}_{i}\\right)-1\\right)=0, \\quad i=1,2,...,m\\\\ \\mathbf{w}^{\\star} = \\sum_{i=1}^m a_i y_i \\mathbf{x}_i, \\quad 0 = \\sum_{i=1}^m a_i y_i \\end{array}\\right.$$ 由第三条（complementary slackness）可得，对任何一个样本$\\mathbf{x}_i$而言： 若$y_i f(\\mathbf{x}_i) \\neq 1$，则拉格朗日乘子$a_i$为0，此时样本$\\mathbf{x}_i$对预测模型（$\\sum_{i=1}^m a_i^{\\star} y_i \\mathbf{x}_i^T \\mathbf{x} + b^{\\star}$）无影响； 若$y_i f(\\mathbf{x}_i) = 1$，此时样本$\\mathbf{x}_i$位于最大间隔边界上，是支持向量。将支持向量表示为$(\\mathbf{x}_s, y_s)$，则$y_{s}\\left( \\mathbf{w^{\\star}}^T \\mathbf{x}_{s}+b^{\\star}\\right)=1$，故理论上，可选取任意支持向量获得$b^{\\star}$ (均值$b^{\\star}=\\frac{1}{|S|} \\sum_{s \\in S}\\left(\\frac{1}{y_{s}}-\\mathbf{w^{\\star}}^T \\mathbf{x}_{s}\\right)$更为鲁棒)。 它揭示了SVM的一个重要性质：最终模型只与支持向量有关。 如何求解$\\boldsymbol{a}$？为了提高优化速度，可以采用坐标下降法(coordinate descent)，其是一种非梯度优化算法，与通过梯度的最速下降不同，在坐标下降法中，优化方向是固定的。 原理： 多变量函数$F(\\boldsymbol{a})$，每次沿一个坐标方向进行一维搜索，求得该方向上，函数的局部极小值； 在整个过程中，循环使用不同的坐标方向（例如，线性空间的一组基$\\mathbf{e}_1, \\mathbf{e}_2, \\dots, \\mathbf{e}_n$），循环最小化各方向上的函数值，即： $$\\mathbf{x}^{k+1}_i = \\underset{y\\in\\mathbb R}{\\operatorname{arg\\,min}}\\; f(a^{k+1}_1,...,a^{k+1}_{i-1},y,a^k_{i+1},...,a^k_n)$$ 因而，从初始的猜测值$\\boldsymbol{a}_0$，以求$F$的局部最优值，可以迭代获得$\\boldsymbol{a}_0, \\boldsymbol{a}_1, \\boldsymbol{a}_2, \\dots$的序列。 四. 核函数核函数（kernel function）将样本从原始空间映射($f(\\mathbf{x})=\\mathbf{w}^T \\phi(\\mathbf{x})+b$)到更高维空间，使样本在该特征空间内线性可分，此时，对偶问题如下： $$ \\max_{\\mathbf{a}} \\sum_{i=1}^m a_i - \\frac{1}{2} \\sum_{i=1}^m\\sum_{j=1}^m a_i a_j y_i y_j \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j) \\\\ \\text{s.t.} \\quad \\sum_{i=1}^m a_i y_i = 0, \\quad a_i \\geq 0, \\quad i=1,2,...,m $$ 其中，$\\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j)$是样本在高维空间的内积，计算困难。为了避开计算上的复杂，可设想这样的核函数: $$\\kappa\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\left\\langle\\phi\\left(\\mathbf{x}_{i}\\right), \\phi\\left(\\boldsymbol{x}_{j}\\right)\\right\\rangle=\\phi\\left(\\mathbf{x}_{i}\\right)^T\\phi\\left(\\mathbf{x}_{j}\\right)$$ 其将样本在高维空间的内积，通过在低维空间中计算$\\kappa\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)$来获得，节约了映射过程和高维向量内积的计算开销。 通过核函数，上述对偶问题变为： $$ \\max_{\\mathbf{a}} \\sum_{i=1}^m a_i - \\frac{1}{2} \\sum_{i=1}^m\\sum_{j=1}^m a_i a_j y_i y_j \\kappa\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right) \\\\ \\text{s.t.} \\quad \\sum_{i=1}^m a_i y_i = 0, \\quad a_i \\geq 0, \\quad i=1,2,...,m $$ 函数模型变为：$$\\begin{aligned} f(\\mathbf{x}) &=\\mathbf{w^{\\star}}^T \\phi(\\mathbf{x})+b^{\\star} =\\sum_{i=1}^{m} a_{i}^{\\star} y_{i} \\phi\\left(\\mathbf{x}_{i}\\right)^T \\phi(\\mathbf{x})+b^{\\star} \\\\ &=\\sum_{i=1}^{m} a_{i}^{\\star} y_{i} \\kappa\\left(\\mathbf{x}, \\mathbf{x}_{i}\\right)+b^{\\star} \\end{aligned}$$ ，若己知合适映射 cþ(.) 的具体形式，则可写出核函数 κ(. ， .).但在现实 任务中我们通常不知道 cþ(.) 是什么形式， 需注意的是，在不知道特征映射的形 式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地走义了 这个特征空间.于是，”核函数选择”成为支持向量机的最大变数. 核函数是一个把特征从低维空间映射到高维空间并保证计算开销与低维度空间相当的做法。 也可以说是既要马儿跑又要马儿不吃草的神奇操作。 核函数有很多，常用的有rbf，高斯核， polynomial 核等等。 核函数的研究在学界也是一个曾经一度很热的领域。 这里不提各种核的用法，想简单推导一下核的原理，顺便展现dual 形式求解的价值。 五. 软间隔与正则化六. 支持向量回归七. ReferenceBook: 机器学习，周志华Zhihu: SVM（二）：从凸优化聊到SVM对偶式的推导","link":"/2020/06/10/interview-SVM/"},{"title":"数据结构：队列","text":"队列也称伫列，具有线性存储结构。栈的特点是先入先出，其只允许在后端（rear）进行插入，在前端（front）进行删除，如下图所示。 应用场景：当需要按照一定的顺序来处理数据，而该数据的数据量在不断地变化时，需要队列，即BFS。 一. 栈的设计与实现1. 用栈实现队列: Leetcode 232. Implement Queue using Stacks123456789101112131415161718192021class MyQueue(object): def __init__(self): self.stack_push = [] self.stack_pop = [] def push(self, x): # t = O(n) while self.stack_pop: # 将stack_pop中的元素装回stack_push self.stack_push.append(self.stack_pop.pop()) self.stack_pop.append(x) # 入栈新的元素 while self.stack_push: self.stack_pop.append(self.stack_push.pop()) def pop(self): # t = O(1) return self.stack_pop.pop() def peek(self): # t = O(1) return self.stack_pop[-1] def empty(self): return len(self.stack_pop) == 0 主要思路：需要用两个栈，其中一个来反转元素顺序，另一个用来存储元素最终顺序。 二. 优先级队列（Priority Queues）普通队列先进先出，而在优先队列中，元素被赋予优先级，特点是最高级先出（first in, largest out）。通常采用堆（heap）来实现。 堆（heap）是一种特别的完全树数据结构，若满足：任意母节点的值恒小于等于子节点的值，此堆称为最小堆（min heap）；反之，若任意母节点的值恒大于等于子节点的值，此堆称为最大堆（max heap）。二叉堆（binary heap）（特殊的完全二叉树）是最为常见的一种堆，如下图所示： 完全二叉树:在二叉树中，若除最后一层外的其余层均满，且最后一层满，或仅在右边缺少连续若干节点。满二叉树:指每层结点个数都是最大的二叉树。 1. 堆的数组表示二叉堆（特殊完全二叉树）一般用数组来表示，若数组的下标基于0，那么下标为i的节点的左、右子节点分别为：left(i) = 2i + 1、right(i) = 2i + 2，父节点为：parent(i) = floor((i - 1)/2)，如下图所示： 2. 插入/添加元素 堆的基本操作：上浮（sift up），比较当前节点与父节点，若不满足堆性质，则交换。 1234def sift_up(i, array): while (i &gt; 0 and array[parent(i)] &lt; array[i]): # 若到堆顶，则不再上浮 array[parent(i)], array[i] = array[i], array[parent(i)] i = parent(i) 主要思路：插入/添加元素基于，基本操作上浮（sift up）；在数组的最末尾插入新节点，然后自下而上，进行上浮操作（调整子节点与父节点）；时间复杂度为（树高）t(n)=O(logn)。 3. 删除根节点 堆的基本操作：下沉（sift down），比较当前节点与其两个子节点（上浮仅需与父节点比较），把较小的子节点和当前节点交换。 12345678def sift_down(i, array, end): while left(i) &lt;= end: # 若到堆底，则不再下沉 if right(i) &lt; end: smaller_child_index = left(i) if array[left(i)] &lt; array[right(i)] else right(i) else: smaller_child_index = left(i) if array[smaller_child_index] &lt; array[i]: array[smaller_child_index], array[i] = array[i], array[smaller_child_index] 主要思路：删除根节点基于，基本操作下沉（sift down）；删除根节点，并把堆中最后的节点移填在根节点处；再从上而下，进行下沉操作（调整父节点与它的子节点）；时间复杂度为（树高）t(n)=O(logn)。 4. heapify(堆化)，build the heap123def heapify(array): for i in range(floor(n/2-1), -1, -1) sift_down(i) 主要思路：将n个元素逐一插入到空堆中，t(n)=O(logn)；优化方法，从最后一个非子叶节点（最后一个节点的父节点floor(n/2-1))）开始，自下而上进行下沉操作，t(n)=O(n)；合并两个heap1与heap2, 即heapify(heap1+heap2)。 5. Heapsort 12345678def heapsort(array) heapify(array) end = len(array) - 1 while end &gt; 0: array[end], array[0] = array[0], array[end] end -= 1 sift_down(0, array, end) 主要思路：构造初始堆（heapify）；首尾交换，断尾（因为尾节点已排序）；首部下沉；重复，直至所有节点排序；t(n)=O(nlgn)，s(n)=1（inplace）。 6. Heap in Python123456789101112131415import heapqheap = [(1, \"a\"), (2, \"b\")]heapq.heapify(heap) # 根据第一个元素heapify，得到min heapheapq.heapify(-heap) # 若想得到max heapheapq.heappush(heap, item) # 插入值heapq.heappop(heap) # 弹出最小（root）元素n_largest=heapq.nlargest(n, heap) # 返回n个最大值n_smallest=heapq.nsmallest(n, heap)def heapsort(heap): heapq.heapify(heap) return [heapq.heappop(heap) for i in range(len(heap))] 用例：Leetcode 23. Merge k Sorted Lists, Leetcode 347. Top K Frequent Elements 四. ReferenceWiki: 队列Blog: 栈和队列Blog: 二叉堆详解实现优先级队列Blog: 我理解的数据结构（七）—— 堆和优先队列（Heap And PriorityQueue）Blog: Priority Queues","link":"/2020/07/14/queue/"},{"title":"算法思维：回溯法","text":"回溯法（backtracking）是暴力搜索法中的一种。回溯法采用试错的思想，它尝试分步解决问题，当它发现现有的分步不能得到解时，将取消上一步的计算，用其它的可能的分步，再次尝试求解。在最坏的情况下，回溯法会导致指数时间复杂度，例如 t=O(n!) 或 t=O(2^n)，即枚举的时间复杂度。回溯法的代码框架如下： 12345678910result = []def backtracking(path, choices): if condition:# 结束情况 result.append(path) return for i in range(len(choices)): # 做选择，减少可能选择 backtrack(path + choices[i], choices[:i]+choices[i+1:]) # 可以合并写为 # 撤销选择，恢复可能选择 回溯法，本质是决策树的遍历过程（root代表初始状态，其余每个节点代表一个选择），通过for循环里面的递归来实现。 一. 子集、组合、排列问题1. 子集问题Leetcode 78. Subsets 123456789101112class Solution(object): def subsets(self, nums): res = [] def backtracking(path, choices): res.append(path) if not choices: # End case return for i in range(len(choices)): backtracking(path + [choices[i]], choices[i+1:]) # 因为下一次，只尝试更大的可能路径 backtracking([], nums) return res Leetcode 90. Subsets II 123456789101112131415class Solution(object): def subsetsWithDup(self, nums): def backtracking(path, choices): res.append(path) for i in range(len(choices)): if i &gt; 0 and choices[i] == choices[i-1]: pass else: backtracking(path + [choices[i]], choices[i+1:]) res = [] nums = sorted(nums) # 排序，让相同的元素聚集，是去重的有效操作 backtracking([], nums) return res 主要思路：将backtracking过程想象成决策树，没有重复子集，即要求决策树每层不得有同值元素；排序，让相同的元素聚集，是去重的有效操作。 2. 组合问题：Leetcode 77. Combinations、Leetcode 47. Permutations II1234567891011121314class Solution(object): def combine(self, n, k): res = [] def backtracking(path, choices, k): if k == 0: # 两种不同情况下的end case res.append(path) return if not choices: return for i in range(len(choices)): backtracking(path+[choices[i]], choices[i+1:], k-1) backtracking([], [i+1 for i in range(n)], k) return res 3. 排列问题：Leetcode 46. Permutations123456789101112class Solution(object): def permute(self, nums): res = [] def backtracking(path, choices): if not choices: res.append(path) return for i in range(len(choices)): backtracking(path+[choices[i]], choices[:i]+choices[i+1:]) backtracking([], nums) return res 二. 其他1. N皇后问题：Leetcode 51. N-Queens123456789101112131415161718192021222324252627282930313233343536373839class Solution(object): def solveNQueens(self, n): def increment(row, col): # 坐标的步进 col += 1 return row, col def is_valid(path, x, y): # 仅需与已访问的位置比较，因为这些位置才可能有Q for i in range(x): # 上侧检验 if path[i][y] == \"Q\": return False i, j = x, y while i &gt;= 0 and j &gt;= 0: # 左上 if path[i][j] == \"Q\": return False i, j = i - 1, j - 1 i, j = x, y while i &gt;= 0 and j &lt; n: # 右上 if path[i][j] == \"Q\": return False i, j = i - 1, j + 1 return True def backtracking(path, row, col): if row == n: res.append([\"\".join(i) for i in path]) return while col &lt; n: # 坐标步进，分析坐标点 if is_valid(path, row, col): path[row][col] = \"Q\" row_, col_ = row + 1, 0 backtracking(path, row_, col_) path[row][col] = \".\" row, col = increment(row, col) return res, board = [], [[\".\"] * n for _ in range(n)] backtracking(board, 0, 0) return res 主要思路：在(n,n)的棋盘中放入n个皇后，每行有且仅有一个皇后；故回溯算法应逐行，对每行的元素进行枚举判断。 2. 数独问题：Leetcode 37. Sudoku Solver1234567891011121314151617181920212223242526272829303132333435363738394041class Solution(object): def solveSudoku(self, board): def increment(row, col): # 坐标的步进 col += 1 if col == n: row, col = row + 1, 0 return row, col def is_valid(row, col, num): # 有效性验证 for i in range(n): if board[i][col] == num: return False for j in range(n): if board[row][j] == num: return False for i in range(row // 3 * 3, row // 3 * 3 + 3): for j in range(col // 3 * 3, col // 3 * 3 + 3): if board[i][j] == num: return False return True def backtracking(row, col): if row == n: # 当步进到第n行，说明寻到了结果；找到结果后，通过标志变量，立刻结束迭代 self.solved = True return if board[row][col] == \".\": for i in range(1, 10): # for循环来，进行填写选择 if is_valid(row, col, str(i)) and not self.solved: # 找到结果后，不必再更新上一次的尝试 board[row][col] = str(i) row_, col_ = increment(row, col) backtracking(row_, col_) if not self.solved: # 找到结果后，不必再将上一次的尝试恢复（保留上一次的尝试） board[row][col] = \".\" else: row_, col_ = increment(row, col) # 遇到本身填写的数字，跳过 backtracking(row_, col_) return self.solved, n = False, len(board) backtracking(0, 0) 主要思路：回溯法在二维数据上的应用，基本涉及坐标的步进和有效性验证；找到结果后，通过标志变量，立刻结束迭代。 12345678910111213141516def 2D_backtracking(): def increment(row, col): # 坐标的步进 def is_valid(row, col): # 有效性验证 def backtracking(row, col) if row == n: # 超出范围，返回 return if is_valid(row, col): for choice in choices: make_choice(row, col, choice) # 选择 row_, col_ = increment(row, col) backtracking(row_, col_) cancel_choice(row, col, choice) # 撤销选择 return 其他用例：Leetcode 17. Letter Combinations of a Phone NumberLeetcode 22. Generate ParenthesesLeetcode 39. Combination SumLeetcode 93. Restore IP AddressesLeetcode 131. Palindrome Partitioning 三. ReferenceWiki: 回溯法Blog: 回溯法Blog: 回溯算法解题套路框架Blog: 回溯算法最佳实践：解数独","link":"/2020/07/16/back-track/"},{"title":"算法思维：二分查找","text":"二分查找（binary search），也称对数搜索（logarithmic search），是在有序数组中查找某特定元素的搜索算法，其时间复杂度为O(nlgn)。 搜索过程从数组中间元素开始，若中间元素是查找的元素，则搜索过程结束； 若特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找； 重复上述步骤，直至找到特定元素，或某一步骤数组为空，代表找不到。 应用场景：二分查找要求表中元素按关键字有序排列。 一. 二叉搜索的实现1. 基本的二分搜索1234567891011def binary_search(nums, target): left, right = 0, len(nums) - 1 while left &lt;= right: mid = left + (right - left) // 2 # 防止left和right太大，相加溢出；mid指针偏向前半段 if nums[mid] == target: return mid elif nums[mid] &lt; target: left = mid + 1 else: right = mid - 1 return -1 # 未寻到目标 主要思路：算法的搜索区间为闭区间[left, right]；若mid不是目标时，需搜索[left, mid-1]或[mid+1, right]；因为mid已被搜索过，应从搜索区间中除去；结束时left=right+1，left (left=len(nums)), right (right=0)都有可能越界。 2. 寻找左侧边界的二分搜索例如，在有序数组nums=[2,2,2,3]中，搜索目标2的左边界；若搜索1，算法会返回0；搜索4，算法返回4。所以算法可以理解为nums中小于目标的元素个数。 1234567891011121314def binary_search_left_border(nums, target): left, right = 0, len(nums) - 1 while left &lt;= right: mid = left + (right-left) // 2 if nums[mid] == target: right = mid - 1 # 即便误排了唯一目标，结束时left=right+1，也会返回正确值 elif nums[mid] &lt; target: left = mid + 1 else: right = mid - 1 if left &gt;= len(nums) or nums[left] != target: # 分别检测left=len(nums)和left=0时的情况；若需要小于目标的元素个数，可以直接返回left return -1 else: return left 主要思路：基本同上，主要区别在于，找到目标时，不立即返回，而是缩小搜索区间的上界right；while退出时，left=right+1，当目标比nums中所有元素都大（或小）时，返回left=len(nums)（或0），所以在代码最后应有检测判断。 3. 寻找右侧边界的二分搜索1234567891011121314def binary_search_right_border(nums, target): left, right = 0, len(nums) - 1 while left &lt;= right: mid = left + (right-left) // 2 if nums[mid] == target: left = mid + 1 # 即便误排了唯一目标，结束时right=left-1，也会返回正确值 elif nums[mid] &lt; target: left = mid + 1 else: right = mid - 1 if right &lt; 0 or nums[right] != target: # 分别检测right=-1和right=len(nums)-1时的情况 return -1 else: return right 主要思路：基本同上，主要区别在于，找到目标时，不立即返回，而是增大搜索区间的下界left；while退出时，right=left-1，当目标比nums中所有元素都大（或小）时，返回right=len(nums)-1（或-1），所以在代码最后应有检测判断。例子：Leetcode 704. Binary Search，Leetcode 34. Find First and Last Position of Element in Sorted Array。 二. 二分搜索的应用1. 旋转排序数组搜索旋转排序数组，Leetcode 33. Search in Rotated Sorted Array 123456789101112131415161718192021class Solution(object): def search(self, nums, target): left, right = 0, len(nums) - 1 while left &lt;= right: mid = left + (right - left) // 2 if nums[mid] == target: return mid if nums[mid] &lt; nums[right]: if nums[mid] &lt; target &lt;= nums[right]: left = mid + 1 else: right = mid - 1 elif nums[mid] &gt; nums[right]: if nums[left] &lt;= target &lt; nums[mid]: right = mid - 1 else: left = mid + 1 else: # mid=right，当且仅当left=right=mid, 而前方已判断，nums[mid]!=target return -1 return -1 搜索旋转排序数组ii，包含重复元素，Leetcode 81. Search in Rotated Sorted Array II，[参考] 123456789101112131415161718192021222324class Solution(object): def search(self, nums, target): left, right = 0, len(nums) - 1 while left &lt;= right: mid = left + (right - left) // 2 if nums[mid] == target: return True if nums[mid] &lt; nums[right]: if nums[mid] &lt; target &lt;= nums[right]: left = mid + 1 else: right = mid - 1 elif nums[mid] &gt; nums[right]: if nums[left] &lt;= target &lt; nums[mid]: right = mid -1 else: left = mid + 1 else: # nums[mid] = nums[right]，有两种情况： # 3333[30123], 非单调区间；3333[33333]，单调区间。 if nums[right] == target: return True right -= 1 寻找旋转排序数组中的最小值，Leetcode 153. Find Minimum in Rotated Sorted Array，[参考] 12345678910111213141516171819class Solution(object): def findMin(self, nums): if len(nums) == 1 or nums[0] &lt; nums[-1]: return nums[0] left, right = 0, len(nums) - 1 # pivot旋转题的关键在于与队首、队尾的比较 while left &lt;= right: if left == right: break mid = left + (right - left) // 2 if nums[mid] &lt; nums[right]: right = mid elif nums[mid] &gt; nums[right]: left = mid + 1 else: # mid=right，当且仅当left=right=mid，不会运行至此 pass return nums[left] 主要思路：旋转排序数组的二分搜索，关键在于通过mid元素与队首、队尾元素的比较，确定mid元素的位置（pivot点前，或后）；注意边界问题。代码框架如下： 12345678910111213141516def func(nums): left, right = 0, len(nums) - 1 while left &lt;= right: (optional) if left == right: # 若不希望，left=right之后，再有变化，可在此处break mid = left + (right - left) // 2 # -------------------- 包含/不包含重复元素 -------------------- # # 此处注意，应与nums[right]比较(不知道为什么)； # 大概是因为right，永远处于后半段中。例如，4567[12「3」] -&gt; 4567[「1」23][]中为后半段，「」中为right） # left， 却会从前半段，跳入后半段。例如：[「4」567]123 -&gt; [4567]「1」23，[]中为前半段，「」中为left） if nums[mid] &lt; nums[right]: # [mid, right]为单调区间 # 收缩left，或right；注意收缩后区间，是否应包括mid elif nums[mid] &gt; nums[right]: # [left, mid]为单调序列 # 收缩left，或right；注意收缩后区间，是否应包括mid else: # mid=right，当且仅当left=right=mid right = right - 1 # 若有重复元素；此操作仅缩小了一个数的范围，而非一半，所以时间复杂度会上升。 相似用例：Leetcode 154. Find Minimum in Rotated Sorted Array II，[参考] 2. 其他Leetcode 74. Search a 2D MatrixLeetcode 35. Search Insert Position 四. ReferenceWiki: 二分查找Blog: 二分查找详解Blog: 二分搜索","link":"/2020/07/14/binary-search/"},{"title":"数据结构：二叉树","text":"二叉树（Binary tree）是每个节点最多只有两个分支（即不存在分支度大于2的节点）的树结构。 一. 二叉树的遍历/搜索 树的遍历，指不重复地访问树的所有节点的过程，具体的访问操作可能是检查节点值、更新节点值等。 定义节点类如下： 12345class TreeNode(): def __init__(self, x): self.val = x self.left = None self.right = None 1.深度优先按照根节点，相对于左右子节点的访问顺序，可进一步划分如下。 1.1 前序遍历(Pre-Order Traversal) 定义：父节点 -&gt; 左/右子树 -&gt; 右/左子树。 递归方法：简洁123456def preoder_traversal(root): if root is None: return print(root.val) # 父节点访问 preoder_traversal(root.left) # 遍历左子树 preoder_traversal(root.right) # 遍历右子树 非递归方法：避免栈溢出123456789101112def preoder_traversal(root): if root is None: return stack = [] while stack or root is not None: while root is not None: print(root.val) # 父节点访问 stack.append(root) # 保存父节点，以便稍后再访问 root = root.left # 遍历左子树 root = stack.pop() root = root.right # 遍历右子树 1.2 中序遍历(In-Order Traversal) 定义：左/右子树 -&gt; 父节点 -&gt; 右/左子树, 即访问父节点时，其左子树已访问。在每个父节点，检查左子树是否完全遍历，若已遍历，访问父节点，否则遍历左子树。 递归方法123456def inoder_traversal(root): if root is None: return inoder_traversal(root.left) # 遍历左子树 print(root.val) # 父节点访问 inoder_traversal(root.right) # 遍历右子树 非递归方法123456789101112def inorder_traversal(root): if root is None: return stack = [] while stack or root is not None: while root is not None: stack.append(root) # 保存父节点，以便稍后再访问 root = root.left # 遍历左子树 root = stack.pop() print(root.val) # 父节点访问 root = root.right # 遍历右子树 1.3 后序遍历(Post-OrderTraversal) 定义：左/右子树 -&gt; 右/左子树 -&gt; 父节点, 即访问父节点时，其左、右子树已访问。在每个父节点，检查左、右子树是否完全遍历，若已遍历，访问父节点，否则遍历左、右子树。 递归方法123456def postorder_traversal(root): if root is None: return postorder_traversal(root.left) # 遍历左子树 postorder_traversal(root.right) # 遍历右子树 print(root.val) # 父节点访问 非递归方法1234567891011121314def postorder_traversal(root): if root is None: return stack = [] # 暂存未被访问的父节点 while stack or root is not None: while root is not None: stack.append(root) root = root.left # 遍历左子树 if root.right is None: # 父节点需在，右子树完成遍历后，pop root = stack.pop() print(root.val) # 父节点访问 else: root = root.right # 遍历右子树 2.广度优先二叉树的广度优先遍历又称按层次遍历. 1234567891011121314def layer_traversal(root): if root is None: return stack = [root] while stack: size = len(stack) # 记录当前层的节点个数 for i in range(size): node = stack.pop(0) print(node.val) # 访问节点 if node.left is not None: stack.append(node.left) if node.right is not None: stack.append(node.right) 二. 二叉搜索树 二叉搜索树（Binary Search Tree，BST）是一种常用的的二叉树，其定义为：二叉树中，任意节点的值大于等于左子树所有节点的值，小于等于右子树的所有节点的值；其最左/右侧的节点为树中的最大/小值。上述二叉树代码框架, 皆适用于用二叉搜索树. 综合二叉搜索树“左小右大”的特性，可简化遍历/搜索代码框架如下， 以preorder-traversal为例： 123456789def BST(root, target): if root is None: return if root.val == target: return print(root.val) # 或其他访问操作 if root.val &lt; target: # 不必再继续遍历更小的左子树 BST(root.right, target) if root.val &gt; target: BST(root.left, target) # 不必再继续遍历更大的左子树 以下例举几个较为的特殊的例子。 1.判断BST的合法性需要注意的是，根节点需要做的不只是和左、右子节点比较，而需要与整个左、右子树所有节点比较。所以根节点需保存其子树的上下限值。 123456789def isBST(root, maximum, minimum): if root is None: return True visit = root.val &lt; maximum and root.val &gt; minimum # 访问操作 traversal_left = isBST(root.left, root.val, minimum) # 遍历左子树 traversal_right = isBST(root.right, maximum, root.val) # 遍历右子树 return visit and traversal_left and traversal_right 2.在BST中删除一个数BST中树的删除分三种情况：1）删除无子树的末端节点；2）删除仅有一个子树的节点；3）删除有两个子树的节点。 12345678910111213141516171819202122def deleteBST(root, target): def get_min(node): while node.left: node = node.left return node if root is None: return None # 查无此数 if root.val == target: if root.left == None: # 涵盖了1), 2)两种情况 return root.right # 因为root为被删除的点，需在上层替换，故需要返回 if root.right == None: return root.left node = get_min(root.right) # 3):用左子树中最大值，或右子树中最小的值来替换 root.val = node.val root.right = deleteBST(root.right, root.val) # 删除用于替换的节点 if root.val &gt; target: root.left = deleteBST(root.left, target) if root.val &lt; target: root.right = deleteBST(root.right, target) return root 三. 用例Leetcode 98. Validate Binary Search TreeLeetcode 103. Binary Tree Zigzag Level Order TraversalLeetcode 104. Maximum Depth of Binary TreeLeetcode 102. Binary Tree Level Order TraversalLeetcode 107. Binary Tree Level Order Traversal IILeetcode 110. Balanced Binary TreeLeetcode 124. Binary Tree Maximum Path SumLeetcode 236. Lowest Common Ancestor of a Binary TreeLeetcode 450. Delete Node in a BSTLeetcode 701. Insert into a Binary Search Tree Leetcode中的两种常用写法node传参：适用于，返回结果为node形式12345678910class Solution(object): def node_munipulation(self, root): if root is None: return None # Do something here root.left = self.node_munipulation(root.left) root.right = self.node_munipulation(root.right) return root 全局变量： 适用于其他情况12345678910class Solution(object): self.flag = False def node_munipulation(self, root): if root is None: return # Do something here self.flag = True self.node_munipulation(root.left) self.node_munipulation(root.right) 四. ReferenceWiki: 树的遍历Blog: 学习数据结构和算法的框架思维Blog: 二叉树Blog: 二叉搜索树操作集锦","link":"/2020/06/30/binary-tree/"},{"title":"Google Cloud for Deep Learning - Part 2","text":"The series of passages to use the Google Cloud for deep learning has two parts: The first part is about creating a VM instance on the Google cloud and the second part is about installing some deep learning related configurations (jupyter notebook and GPUs). Now, Let’s move to the Part 2! Configure the Deep Learning EnviromentStep 1: Install GPU Drivers and CUDAAfter you create an instance with one or more GPUs, your system requires device drivers so that your applications can access the device. Install CUDA, which includes the NVIDIA driver. Following steps to install CUDA and the associated drivers for NVIDIA® GPUs. Run command line to create a script: 1$ vim installNvidia.sh Paste the following script to install CUDA: 12345678910111213#!/bin/bashecho \"Checking for CUDA and installing.\"# Check for CUDA and try to install.if ! dpkg-query -W cuda-10-0; then # The 16.04 installer works with 16.10. curl -O http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.0.130-1_amd64.deb dpkg -i ./cuda-repo-ubuntu1604_10.0.130-1_amd64.deb apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub apt-get update apt-get install cuda-10-0 -yfi# Enable persistence modenvidia-smi -pm 1 Run command line to execute the script: 1$ bash installNvidia.sh Verify that the driver installed and initialized properly. 1$ nvidia-smi The output of the command looks similar to the following: 123456789+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.92 Driver Version: 410.92 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla P100-PCIE... On | 00000000:00:04.0 Off | 0 || N/A 34C P0 26W / 250W | 0MiB / 16276MiB | 0% Default |+-------------------------------+----------------------+----------------------+ Step 2: Install cuDNN The NVIDIA CUDA® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. IMPORTANT: If you want to use pytorch with an NVIDIA GPU…We ship with everything in-built (pytorch binaries include CUDA, CuDNN, NCCL, MKL, etc.) When using pytorch, you just need to install the NVIDIA drivers and the binaries will come with the other libs. If you want to build from source, you would need to install CUDA, cuDNN etc. The cuDNN can be installed as the following: Check the version of your CUDA and download the corresponding version of cuDNN which requires a NVIDIA account. 1234# Check the version of CUDA$ cat /usr/local/cuda/version.txt# Check the version of cuDNN$ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 Download it to the local and copy it to the remote: Or use wget to download it. It’s a bit more tricky, you have to start the downloading and copy the downloading link from the browser downloader. Then decompress downloaded compressed file and get a folder cuda: 1$ tar -zxvf file Copy files to finish the installation 12$ sudo cp cuda/lib64/* /usr/local/cuda/lib64/$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ Step 3: Install Anaconda with Jupyter Notebook Copy the bash (.sh file) installer link: Use wget to download the bash installer: 1$ wget https://repo.continuum.io/archive/Anaconda3&lt;release&gt;.sh Run the bash script to install Anaconda3: 1$ bash Anaconda3-5.2.0-Linux-x86_64.sh source the .bashrc file to add Anaconda to your PATH. Now that Anaconda3 is added to PATH, source the .bashrcfile to load the new PATH environment variable. 1$ source ~/.bashrc To verify the installation is complete, open Python from the command line: 1$ python Step 4: Local Access to Jupyte Service Set firewall rules in “VPC network“ → “Firewall rules“ → “Create firewall rules“. Set Source IP ranges as 0.0.0.0/0 to allow all IPs to access. Set “Protocols and Ports“ to “tcp:port“, which the notebook server will listen on (e.g. 8888). Create a Jupyter configuration file if it doesn’t exist: 1$ jupyter notebook --generate-config Add a few lines to your Jupyter configuration file: 1$ vim .jupyter/jupyter_notebook_config.py 1234567c.NotebookApp.ip = '*'# The port the notebook server will listen on.c.NotebookApp.port = 8888# Whether to open in a browser after starting.c.NotebookApp.open_browser = False# Whether to allow the user to run the notebook as root.c.NotebookApp.allow_root = True Run the jupyter notebook: 12$ jupyter notebook# Then paste returned URL to the browser: http://host(static-ip):8888/?token=...ccf7bb Small Tips:Make External IP Address as StaticBy default, the external IP address is dynamic and we need to make it static to make our life easier. “VPC network“ →”External IP addresses“, and change the “Type“ from “Ephemeral“ to “Static“ Useful Script for Uninstallation12$ sudo apt-get --purge remove cuda # uninstall cuda $ sudo apt autoremove Create Conda Environment With conda, you can create, export, list, remove, and update environments that have different versions of Python and/or packages installed in them. 123# Create and activate the environment$ conda create --name myenv python=3.6$ source activate myenv Transfer Files Between the Local and RemoteUse scp command From the local to the remote: 1234$ scp local_file remote_username@remote_ip:remote_folder$ scp local_file remote_username@remote_ip:remote_file# Add -r parameter to move the entire folder$ scp -r local_folder remote_username@remote_ip:remote_folder From the remote to the local: just reverse the path of file. Use FTP application Use FileZilla® as an example, remember that you should use sftp://external-ip as the host. ReferenceDoc: Compute Engin DocmentationBlog: Running Jupyter Notebook on Google Cloud Platform in 15 minBlog: Installing Anaconda on LinuxBlog: Linux 教程","link":"/2019/10/21/google-cloud-for-deep-learning-part-2/"},{"title":"数据结构：链表","text":"链表（Linked list）是一种线性表，但并非按线性顺序存储数据。其利用不连续的内存块，在每块内存中，通过存储下一块内存的指针来构造线性存储结构。链表主要分为单向链表、双向链表与循环链表。 一. 链表删除删除链表中的节点，主要针对于单向链表（singly-linked list）， 单向链表类定义如下： 1234class ListNode(object): def __init__(self, val=0, next=None): self.val = val self.next = next 1. 删除指定节点：Leetcode 203: Remove Linked List Elements1234567891011class Solution(object): def removeElements(self, head, val): dummy = pointer = ListNode(0, head) while pointer.next: # 若pointer.next为删除点，需要将pointer.next指向pointer.next.next # 那么在一个循环中，需要知道pointer与pointer.next；此时就需要dummy node if pointer.next.val == val: pointer.next = pointer.next.next # pointer.next已更新，pointer不需要再移动 else: pointer = pointer.next return dummy.next 技巧：dummy node， 即虚拟节点dummy-&gt;head。链表题通常需要返回头节点，加入dummy节点使得头节点与其他节点没有区别， 适用于链表修改/删除后头节点发生变化的情况。dummy node对于空节点（None）的输入也有很好的防错效果，例如Leetcode 83. Remove Duplicates from Sorted List。 主要思路：引入dummy node； 以一对nodes（node, node.next）为单位遍历数组；pointer.next更新后，pointer不需再移动。 2. 删除重复节点：82. Remove Duplicates from Sorted List II12345678910111213141516class Solution(object): def deleteDuplicates(self, head): dummy1 = ListNode(None, head) pointer = dummy2 = ListNode(None, dummy1) temp_val = None while pointer.next and pointer.next.next: # 若pointer.next和pointer.next.next为删除点，需要将pointer.next指向pointer.next.next.next # 涉及到三个节点 if pointer.next.val == pointer.next.next.val: temp_val = pointer.next.val # 先找到重复节点的值 while pointer.next and pointer.next.val == temp_val: # 用while遍历所有重复节点，较快 pointer.next = pointer.next.next # pointer.next已更新，pointer不需要再移动 else: pointer = pointer.next return dummy1.next 主要思路：因为一次需要考虑三个节点，所以需要引入两个dummy nodes；确定重复节点的值，用while遍历，直至删除所有重复点；pointer.next更新后，pointer不需再移动。 二. 链表反转反转链表中全部或部分节点，主要针对于单向链表。 1. 反转部分链表：92. Reverse Linked List II123456789101112131415161718192021222324class Solution(object): def reverseBetween(self, head, m, n): cur = dummy = ListNode(None, head) prev = None for _ in range(m): # 利用循环分段遍历 prev = cur cur = cur.next node_in = prev # 确定前驱节点 node_reverse_start = cur prev = None for _ in range(m, n+1): temp_next = cur.next cur.next = prev prev = cur # 先存储prev，再更新cur cur = temp_next node_out = cur # 确定后续节点 node_reverse_end = prev node_in.next = node_reverse_end node_reverse_start.next = node_out return dummy.next 主要思路：head变化，需引入dummy节点；分段过程，通过分段循环来遍历；反转目标区间节点；前驱结点（m-1，m）与后续结点（n+1，n）相拼接，此处涉及四个节点。相似例子有：Leetcode 24. Swap Nodes in Pairs、Leetcode 25. Reverse Nodes in k-Group、Leetcode 25. Reverse Nodes in k-Group。 主要模版为： 123456789101112131415161718192021222324class Solution(object): def reverse(self, head, k): dummy = ListNode(None, head) # 创建dummy节点 prev, cur = dummy, head # 创建prev, cur节点，用以两两呼唤 node_start = head while cur: node_in = node_start # 前驱节点，上一组的node_start是下一组的node_in node_start = cur # 反转目标区间节点 prev = None for _ in range(k): temp_next = cur.next cur.next = prev prev = cur cur.next = temp_next node_out = cur # 后续节点 node_end = prev # 拼接 node_start.next = node_out node_in.next = node_end 三. 链表中的环1. 链表中环的第一个节点：Leetcode 142. Linked List Cycle II1234567891011# Solution 1: Hash table: t = O(n), s = O(n)class Solution(object): def detectCycle(self, head): dic = {} while head: if head not in dic: dic[head] = True else: return head head = head.next return head 技巧：缓存，即缓存节点信息。用哈希表（dict）的键名，或数组（list）来缓存，节点的值或节点对象（引用/指针）；可解决绝大部分链表题，但需要O(n)的空间复杂度, 有时需多次遍历链表。常见用例：Leetcode 138. Copy List with Random Pointer。 12345678910111213141516171819202122# Solution 2: Slow and fast pointersclass Solution(object): def detectCycle(self, head): if not head: return None slow, fast = head, head.next # Detect if it has cycle first try: while slow != fast: slow = slow.next fast = fast.next.next except: return None # If it has a cycle, head -&gt; entry point = meet point -&gt; end + 1 slow = slow.next while slow != head: slow = slow.next head = head.next return head 技巧：快慢指针。快指针步进为2，慢指针步进为1；若存在环，快慢指针终将相交，否则快指针将优先到达终点。 四. 链表排序1. 重排链表Leetcode 143. Reorder List123456789101112131415161718192021222324252627# Solution 1: Recursive methodclass Solution(object): def reorderList(self, head): if not head: return None pointer = head self.flag = True # 用于在满足条件时，停止递归 def recursion(node): if not node: return head node_head = recursion(node.next) if self.flag: if node != node_head and node_head.next != node: temp_next = node_head.next node_head.next = node node.next = temp_next return temp_next else: self.flag = False node.next = None return None recursion(head) return head 主要思路：节点的访问顺序是从后向前，可以考虑使用递归方法。 1234567891011121314151617181920212223242526272829303132333435# Solution 2: 拆分并合并class Solution(object): def reorderList(self, head): if not head: return None slow = fast = dummy = ListNode(None, head) while fast and fast.next: # 寻找中点 slow = slow.next fast = fast.next.next next_half = slow.next # 对半拆分 slow.next = None first_half = head prev, cur = None, next_half # 翻转后半 while cur: temp_next = cur.next cur.next = prev prev = cur cur = temp_next next_half = prev pointer = dummy # 重组两段 while first_half or next_half: if first_half: pointer.next = first_half pointer = pointer.next first_half = first_half.next if next_half: pointer.next = next_half pointer = pointer.next next_half = next_half.next return dummy.next 主要思路：使用快慢指针可以将链表拆（末节点以None结尾）成两半；反转后半段；将前后半段拼接。相似例子有：Leetcode 148. Sort List 2. 链表插入排序Leetcode 147. Insertion Sort List1234567891011121314151617class Solution(object): def insertionSortList(self, head): if not head: return None dummy = ListNode(None, None) # 无需连接至head cur = head while cur: pos = dummy while pos.next and pos.next.val &lt; cur.val: pos = pos.next pos_next, temp_next = pos.next, cur.next pos.next, cur.next = cur, pos_next # 自动以None结尾，因为dummy以None结尾 cur = temp_next return dummy.next 主要思路：常见的排序方法参考：Python different solutions (bubble, insertion, selection, merge, quick sorts).；为使排序后的数组以None结尾，创建后的dummy节点无需连接head。 技巧：综合上述两例，链表的拆分与拼接。拆分，末节点以None结尾完成拆分；拼接，合并/插入至新的dummy=ListNode()节点中。常见用例有：Leetcode 61. Rotate List，86. Partition List。 其他Leetcode 109. Convert Sorted List to Binary Search TreeLeetcode 234. Palindrome Linked List 六. ReferenceWiki: 链表Blog: LeetCode 总结 - 搞定 Linked List 面试题Blog: LeetCode 题目总结 - 链表Blog: 链表Blog: [Algorithm]链表Blog: Leetcode：刷完31道链表题的一点总结","link":"/2020/07/04/linked-list/"},{"title":"机器学习：树型模型（Tree-Based Learning Algorithms）","text":"一. 概念树型模型基于决策树与集成学习，该模型将特征空间划分为一组多面体，然后于多面体中拟合简单的模型（如常数）。本博客主要介绍Decision tree, Random forest, Boosted trees。 二. 决策树（Decision Tree）决策树的生成算法（ID3, C4.5, CART）是贪婪（greedy）的： 通常，自上而下地，在每一步中选择某变量的最佳分割（best split）。 此过程递归（DFS）进行，当节点上的子集达到预定的停止条件时，递归完成。 1. 划分选择随着划分不断进行，我们希望决策树的分支结点所包含的样本尽可能接近，即结点的“纯度” 越来越高，“纯度”可由以下指标描述： 信息增益（Information gain）信息熵越大，纯度越低，于ID3、C4.5中用于分类任务，定义如下:$$\\mathrm{H}(T)=\\mathrm{I}_{E}\\left(p_{1}, p_{2}, \\ldots, p_{J}\\right)=-\\sum_{i=1}^{J} p_{i} \\log _{2} p_{i} > 0$$, 其中$p_{1},p_{2},...$代表类别占比，总和为1。 那么，信息增益定义如下：$$\\begin{split}{IG(T, a)}&= \\underbrace{\\mathrm{H}(T)}_{\\text {Entropy (parent) }}-\\quad \\underbrace{\\mathrm{H}(T | a)}_{\\text {Weighted Sum of Entropy (Children) }} \\\\&=-\\sum_{i=1}^{J} p_{i} \\log _{2} p_{i}-\\sum_{a} p(a) \\sum_{i=1}^{J}-p(i | a) \\log _{2} p(i | a) \\end{split}$$ 基尼指数（Gini impurity）基尼指数，反映了从集合中随机抽取两个样本，其类别标记不一致的概率，在CART中用于分类任务，定义如下： $$\\operatorname{Gini}(T)=\\sum_{i=1}^{J} p_{i} \\sum_{k \\neq i} p_{k}=\\sum_{i=1}^{J} p_{i}\\left(1-p_{i}\\right) $$, 其中$J$代表类别数量，$i\\in \\{1,2,...,J\\}$。 基尼增益定义如下： $$\\begin{split}{IG(T, a)}&= \\underbrace{\\mathrm{Gini}(T)}_{\\text {Gini (parent) }}-\\quad \\underbrace{\\mathrm{Gini}(T | a)}_{\\text {Weighted Sum of Gini (Children) }} \\\\&=\\sum_{i=1}^{J} p_{i}\\left(1-p_{i}\\right)-\\sum_{a} p(a) \\sum_{i=1}^{J}-\\operatorname{P}(i | a) (1- \\operatorname{P}(i | a))\\end{split}$$ 方差下降（Variance reduction）方差反应了数据的离散程度，方差下降在CART中用于回归任务，定义如下：$$\\begin{split} I_{V}(N)=&\\frac{1}{|S|^{2}} \\sum_{i \\in S} \\sum_{j \\in S} \\frac{1}{2}\\left(x_{i}-x_{j}\\right)^{2}\\\\&-\\underbrace{\\left(\\frac{1}{\\left|S_{t}\\right|^{2}} \\sum_{i \\in S_{t}} \\sum_{j \\in S_{t}} \\frac{1}{2}\\left(x_{i}-x_{j}\\right)^{2}+\\frac{1}{\\left|S_{f}\\right|^{2}} \\sum_{i \\in S_{f}} \\sum_{j \\in S_{f}} \\frac{1}{2}\\left(x_{i}-x_{j}\\right)^{2}\\right)}_{\\text {Minimum: Largest Homogeneity}}\\end{split}$$ 2. 剪枝处理剪枝(pruning)是降低决策树的复杂度（complexity），是决策树学习算法对付“过拟合”的主要手段。剪枝分为以下两类： 预剪枝（Pre-pruning）运用了早停（early stopping）的思想，其在决策树生成过程中，对每个结点在划分前进行（验证集）性能评估，若该划分不能提升泛化性能，则停止划分，并将当前结点标记为叶结点。在预剪枝中，虽然些分支的当前划分不能提升泛化性能，但其后续划分却有可能导致性能显著提高；基于贪心的预剪枝有欠拟含的风险。 后剪枝（Post-pruning）先从训练集生成完整的决策树，然后自底向上地对非叶结点进行考察，若将子树替换为叶结点能提升泛化性能，则进行替换。后剪枝欠拟合风险小，但需完全生成决策树，且自底向上地对非叶结点进行逐一考察，因此其训练时间较长。 3. 连续与缺失值 连续值离散属性可直接根据可取值划分结点，连续属性采用二分法（C4.5）。假定连续属性$a$在样本集$D$上有$n$个不同的取值，将其从小到大排序，记为$\\left\\{a^{1}, a^{2}, \\ldots, a^{n}\\right\\}$。把区间$[a^{i}, a^{i+1})$的中位点$\\frac{a^{i}+a^{i+1}}{2}$作为候选划分点，然后像离散属性值一样，选取最优的划分点进行样本集合划分。 缺失值 在属性值缺失的情况下，进行划分属性选择?(C4.5)$\\tilde{D}$表示$D$中在属性$a$上没有缺失值的样本子集，需根据$\\tilde{D}$来判断属性$a$的优劣。假定为每个样本$x$贼予权重$\\omega_x$(初始为1)，并定义如下比例：$$\\begin{aligned} \\tilde{\\rho} &=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} w_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in D} w_{\\boldsymbol{x}}} \\\\ \\tilde{p}_{k} &=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}_{k}} w_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} w_{\\boldsymbol{x}}} \\quad(1 \\leqslant k \\leqslant|\\mathcal{Y}|) \\\\ \\tilde{r}_{v} &=\\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}^{v}} w_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} w_{\\boldsymbol{x}}} \\quad(1 \\leqslant v \\leqslant V) \\end{aligned}$$，即将原本基于全体样本的个数比例，转化为基于没有缺失值样本的加权比例。并将信息增益增广为$$\\begin{aligned} \\operatorname{Gain}(D, a) =\\tilde{\\rho} \\times \\operatorname{Gain}(\\tilde{D}, a) =\\tilde{\\rho} \\times\\left(\\operatorname{Ent}(\\tilde{D})-\\sum_{v=1}^{V} \\tilde{r}_{v} \\operatorname{Ent}\\left(\\tilde{D}^{v}\\right)\\right) \\end{aligned}$$ 若样本在划分属性上缺失，如何对样本进行划分?若样本$x$的划分属性$a$己知，则将其划入对应子结点，且权值不变。若未知，则$x$以不同的概率划入子结点，即其权值调整为$\\tilde{r}_{v} \\cdot w_{x}$。 三. 随机森林（Random Forest）随机森林以方差较大的决策树（深，并在训练过程中引入了随机属性选择）为基学习器，构建Bagging集成，以产生方差较低的输出。随机森林中基学习器的“多样性”，源自于样本扰动(Bootstrapping)与属性扰动。 特征采样（推荐采样数目$k=\\log _{2} d$）除了会使树去相关（decorrelate），还有以下好处： 在决策树拟合过程中，若少数特征占主导地位，许多其他的特征（例如，有用的局部的特征）将很少被选为分裂变量。特征重要性度量（feature importance）：在该特征上，由于拆分而产生的“纯度”指标降低的总量，在所有树上的平均值。 对缺失的特征更加稳健，具有缺失特征的数据，仍然可以进行回归或分类（仅用有限特征）。 四. 提升树（Boosted Trees）在Boosting中, 学习器${\\hat {F}}(\\mathbf{x})$可视为基学习器$h_{i}(\\mathbf{x})$的线性组合（加权和）: $$\\hat{F}(\\mathbf{x})=\\sum_{i=1}^{M} \\gamma_{i} h_{i}(\\mathbf{x})+\\text { const. }$$ 学习器${\\hat {F}}(\\mathbf{x})$的优化目标可定义为：$$\\hat{F}=\\underset{F}{\\arg \\min } \\mathbb{E}_{\\mathbf{x}, y}[L(y, F(\\mathbf{x}))]$$Boosting串行训练基学习器，每轮加入新的基学习器，使得损失函数最小，训练第$m$个基学习器如下：$$F_{m}(\\mathbf{x})=F_{m-1}(\\mathbf{x})+\\gamma_{m}h_m\\\\{F_{m}(\\mathbf{x})=F_{m-1}(\\mathbf{x})+\\underset{h_{m} \\in \\mathcal{H}}{\\gamma_{m}\\arg \\min }\\left[\\sum_{i=1}^{n} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)+h_{m}\\left(\\mathbf{x}_{i}\\right)\\right)\\right]}$$，$F_{0}(\\mathbf{x})$通常为常函数，即${F_{0}(\\mathbf{x})=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, \\gamma\\right)}$。上述是通用boosting算法。我们可以选择不同的弱学习器、损失函数、加权参数 $\\lambda$，可以得到不同的boosting方法。 1. AdaboostAdaboost以偏差较大的决策树（浅）为基学习器，构建Boosting集成，以产生偏差较低的输出。Adaboost使用指数损失函数（exponential loss） $E=\\sum_{i=1}^{N} e^{-y_{i} F_{m}\\left(\\mathbf{x}_{i}\\right)}=\\sum_{i=1}^{N} e^{-y_{i} F_{m-1}\\left(\\mathbf{x}_{i}\\right)} e^{-y_{i} \\gamma_{m} h_{m}\\left(\\mathbf{x}_{i}\\right)}=\\sum_{i=1}^{N} w_{i}^{(m)} e^{-y_{i} \\gamma_{m} h_{m}\\left(\\mathbf{x}_{i}\\right)}$，可以通过$\\underset{h_m}{\\arg \\min }E$确定第$m$轮的弱学习器，以$\\frac{d E}{d \\gamma_{m}}=0$确定$\\gamma_m$​。 Adaboost中弱学习器的训练，可视为对加权的原始数据的拟合（数据加权，指计算损失函数时，乘上权重）：$E=\\sum_{i=1}^{N} \\underbrace{e^{-y_{i} F_{m-1}\\left(\\mathbf{x}_{i}\\right)}}_{Weights} \\underbrace{e^{-y_{i} \\gamma_{m} h_{m}\\left(\\mathbf{x}_{i}\\right)}}_{\\text{Loss of each observation}}$。可以看出训练结果较差（损失函数大）的数据，在新一轮的训练中，将被赋予更大的权重。 2. Gradient Boosting Decision Tree (GBDT)GBDT使用最速下降法（steepest descent）最小化损失函数，其中决策树用于拟合梯度，并采用牛顿法（即，line search）确定步长$\\lambda_m$，具体如下： $$r_{im}=\\nabla_{F_{m-1}} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)\\right)\\\\ {F_{m}(x)=F_{m-1}(\\mathbf{x})-\\gamma_{m} \\sum_{i=1}^{n}r_{im}} $$，其中$r_{im}$（梯度）需被决策树拟合，$F_{m}(\\mathbf{x})$初始化为常函数：$F_{0}(\\mathbf{x})=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, \\gamma\\right)$，乘子（步长）$\\gamma _{m}$ 通过line search确定：$\\gamma_{m}=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)+\\gamma h_{m}\\left(\\mathbf{x}_{i}\\right)\\right)$。 与Adaboost相比，GBDT基于梯度下降法（最速下降法），可以更容易地适应大量的损失函数。 3. XgboostXgboost使用解析法，求得损失函数二阶近似，取最小值时的解析解，用解析解作为Gain来建立决策树，使损失函数最优。在Xgboost中, 学习器${\\hat {F}}(\\mathbf{x})$可视为基学习器$h_{i}(\\mathbf{x})$，权重为1的线性组合: $\\hat{F}(\\mathbf{x})=\\sum_{i=1}^{M}h_{i}(\\mathbf{x})+\\text { const. }$。用于确定每轮基学习器的优化目标（损失函数），增广了正则项（预测时，仅用${\\hat {F}}(\\mathbf{x})$，无需正则项）：$$\\begin{split} F_{m}(\\mathbf{x}) &= F_{m-1}(\\mathbf{x})+\\underset{h_{m} \\in \\mathcal{H}}{\\arg \\min }\\left[\\sum_{i=1}^{n} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)+h_{m}\\left(\\mathbf{x}_{i}\\right)\\right)\\right]+\\Omega(F_{m-1}\\left(\\mathbf{x}\\right)+h_{m}\\left(\\mathbf{x}\\right)) \\\\&= \\underset{h_{m} \\in \\mathcal{H}}{\\arg \\min }\\sum_{i=1}^{n}\\left[ L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)+h_{m}\\left(\\mathbf{x}_{i}\\right)\\right)\\right]+\\Omega(h_{m}\\left(\\mathbf{x}\\right)) +\\text { const. }\\end{split}$$，其中$\\Omega(h_{m}\\left(\\mathbf{x}\\right)) = \\gamma T+\\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_{j}^{2}$为正则项，$T$为决策树$h_{m}$中的叶子结点的数目，$w_{j}$为第$j$个叶子结点的权重（代表的分数，一般设为叶子节点中的样本均值），即用叶子结点的个数和叶子结点权重的平滑程度，来描述模型的复杂度。 由二阶泰勒展开$f(x+\\Delta x) \\simeq f(x)+f^{\\prime}(x) \\Delta x+\\frac{1}{2} f^{\\prime \\prime}(x) \\Delta x^{2}$可得，$$\\begin{split}Obj &= \\underset{h_{m} \\in \\mathcal{H}}{\\arg \\min }\\sum_{i=1}^{n}\\left[ L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)+h_{m}\\left(\\mathbf{x}_{i}\\right)\\right)\\right]+\\Omega(h_{m}\\left(\\mathbf{x}\\right)) +\\text { const. }\\\\ &\\simeq \\underset{h_{m} \\in \\mathcal{H}}{\\arg \\min }\\sum_{i=1}^{n}\\left[ L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)\\right)+\\nabla_{F_{m-1}} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)\\right)h_{m}\\left(\\mathbf{x}_{i}\\right) + \\frac{1}{2}\\nabla_{F_{m-1}}^2 L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)\\right)h_{m}^2\\left(\\mathbf{x}_{i}\\right)\\right]+\\Omega(h_{m}\\left(\\mathbf{x}\\right)) + \\text { const. } \\\\ &= \\underset{h_{m} \\in \\mathcal{H}}{\\arg \\min }\\sum_{i=1}^{n}\\left[ L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)\\right)+g_{m-1}(\\mathbf{x}_{i}) h_{m}\\left(\\mathbf{x}_{i}\\right) + \\frac{1}{2}k_{m-1}(\\mathbf{x}_{i}) h_{m}^2\\left(\\mathbf{x}_{i}\\right)\\right]+ \\Omega(h_{m}\\left(\\mathbf{x}\\right)) + \\text { const. } \\\\&= \\underset{h_{m} \\in \\mathcal{H}}{\\arg \\min }\\sum_{i=1}^{n}\\left[g_{m-1}(\\mathbf{x}_{i}) h_{m}\\left(\\mathbf{x}_{i}\\right) + \\frac{1}{2}k_{m-1}(\\mathbf{x}_{i}) h_{m}^2\\left(\\mathbf{x}_{i}\\right)\\right]+ \\gamma T+\\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_{j}^{2} + \\text { const. }\\\\&=\\underset{h_{m} \\in \\mathcal{H}}{\\arg \\min }\\sum_{j=1}^{T}\\left[\\left(\\sum_{i \\in I_{j}} g_{m-1}(\\mathbf{x}_{i})\\right) w_{j}+\\frac{1}{2}\\left(\\sum_{i \\in I_{j}} k_{m-1}(\\mathbf{x}_{i})+\\lambda\\right) w_{j}^{2}\\right]+\\gamma T + \\text { const. }, 其中h_{m}(x)=w_{q(x)} \\\\ &= \\underset{h_{m} \\in \\mathcal{H}}{\\arg \\min }\\sum_{j=1}^{T}\\left[G_j w_{j}+\\frac{1}{2}\\left(K_j+\\lambda\\right) w_{j}^{2}\\right]+\\gamma T + \\text { const. }, 二次函数\\end{split}$$ 假设树的结构确定，叶子结点的权重（代表的分数）为$w_{j}^{*}=-\\frac{G_{j}}{K_{j}+\\lambda}$时，优化目标取到最小值$Obj=-\\frac{1}{2} \\sum_{j=1}^{T} \\frac{G_{j}^{2}}{K_{i}+\\lambda}+\\gamma T$（也称“结构分数”，Structure Score，代表树结构所能达到的最小损失，结构分越小，该树的结构越好）。然而树的结构未知（无限可能），通过“结构分下降”贪婪地分裂叶子节点，以生产决策树。即，对每一个叶子结点计算：$$\\begin{aligned} \\text {Gain} &=\\text {Obj}_{\\text {split}}-\\text {Obj}_{\\text {split}} =\\frac{1}{2}\\left[\\frac{G_{L}^{2}}{K_{L}+\\lambda}+\\frac{G_{R}^{2}}{K_{R}+\\lambda}-\\frac{\\left(G_{L}+G_{R}\\right)^{2}}{K_{L}+K_{R}+\\lambda}\\right]-\\gamma\\left(T_{\\text {split}}-T_{\\text {unsplit}}\\right) \\end{aligned}$$ 预剪枝在$\\text {Gain} &gt; 0$时，分裂该叶子结点，分割点的选择同上述（决策树方法）；后剪枝在生成指定深度的决策树后，去除$\\text {Gain} &lt; 0$的叶子结点。对于类变量（categorical variables），可以根据取值进行多分裂，或者使用热独编码（one-hot embeding，每纬度为新变量）。 五. ReferenceBlog: Model Ensemble Blog: Tree-Based Learning Algorithms - Part 2 Blog: Tree-Based Learning Algorithms - Part 1 PPT: Introduction to Boosted Trees","link":"/2020/06/12/interview-tree/"},{"title":"机器学习：线性模型（Linear Model）","text":"一. 概念给定样本$\\mathbf{x} = (x_1, x_2, …,x_d)$，线性模型（linear model）通过属性的线性组合来进行预测，即：$$f(\\mathbf{x}) = w_1x_1 + w_2x_2 + … + w_dx_x + b$$一般写作向量形式：$f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} + b$，其中权重向量$\\mathbf{w}$和偏置项$b$为模型需要学习的参数。 线性模型有良好的可解释性，权重表达了对应属性的重要性。在线性模型的基础上，引入高维映射可以得到非线性模型。 二. 线性回归（Linear regression）“线性回归”试图学得线性模型（参数$\\mathbf{w}$和$b$），使其预测值尽可能准确：$f\\left(\\boldsymbol{x}_{i}\\right)=\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\simeq y_{i}$。 最小二乘法（Least square method）与均方误差（Mean square error）最小二乘法用于参数估计，其找到使所有样本与其欧氏距离之和最小的直线，是常见的线性回归方法。最小二乘法求解$\\mathbf{w}$和$b$，使均方误差（mean square error, MSE）最小，即使$E_{(\\mathbf{w}, b)}=\\frac{1}{m}\\sum_{i=1}^{m}\\left(y_{i}-\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}-b\\right)^{2}$最小。将$E_{(\\mathbf{w}, b)}$（凸函数，二阶导数在区间上非负）对$\\mathbf{w}$与$b$求导，可获得最优解的解析解。 三. 广义线性模型（Generalized linear model）除了让线性模型预测值逼近真实值$y$，还可以让其逼近$y$的衍生物，即：$$g(y) = \\mathbf{w^Tx} + b \\Rightarrow y = g^{-1}(\\mathbf{w^Tx} + b)$$，其认为真实值$y$在$g(y)$尺度上线性变化，$g(\\cdot)$ 称为联系函数（单调可微）。广义线性模型可实现非线性映射功能，例如，对数线性回归（log-linear regression，$g(y) = ln(y)$）、对数几率回归（logistic regression，$g^{-1}(z) = \\frac{1}{1+e^{-z}}$，$g(y) = \\ln{\\frac{y}{1-y}}$）。 对数几率回归（Logistic regression）逻辑回归用于分类任务，其标记是离散值，而线性模型的输出是实值，需要单调可微的联系函数将两者联系。使用对数几率函数（logistic function，近似单位阶跃函数，$g^{-1}(z) = \\frac{1}{1+e^{-z}}$）作为联系函数，代入得：$$y = \\frac{1}{1+ e^{-(\\mathbf{w^Tx} + b)}}$$，是一种Sigmoid函数（即，形似S的函数），也可写作：$$\\ln{\\frac{y}{1-y}} = \\mathbf{w^Tx} + b$$其中，$\\frac{y}{1-y}$ 称作几率（odds），$y$/$1-y$表示样本是正/反例的概率，几率表示该样本为正例的相对可能性，对几率取对数得到对数几率（log odds，也称logit）。所以，对数几率回归的实质，是让线性回归模型的预测值逼近（分类任务）真实标记的对数几率。对数几率回归，不仅可预测出类别，还能得到该预测的概率。 极大似然法（Maximum likelihood）与对数损失（Log loss）极大似然法用于参数估计，其找到使已观测的样本出现概率最大的模型参数（于线性模型中，即$\\mathbf{w}$和$b$）。极大似然法求解$\\mathbf{w}$和$b$，令所有样本概率$p=\\prod_{\\mathbf{x}} p(y^{True} \\mid \\mathbf{x})$最大，即使得对数损失（Log-loss）最小。对概率积求对数，得到对数似然（log-likelihood）： $$\\ell(\\mathbf{w},b) = \\sum_{i=1}^m \\ln p(y^{True}_i | \\mathbf{x_i}) = \\sum_{i=1}^m \\ln (y_i^{True}p(y_i^{True}=1|\\mathbf{x}) + (1-y_i^{True})p(y_i^{True}=0|\\mathbf{x}))$$其中，$p(y^{True}=1|\\mathbf{x}) = \\frac{e^{\\mathbf{w^Tx} + b}}{1+e^{\\mathbf{w^Tx} + b}}$，$p(y^{True}=0|\\mathbf{x}) = 1-p(y^{True}=1|\\mathbf{x}) = \\frac{1}{1+e^{\\mathbf{w^Tx} + b}}$分别代表样本被预测为第1类、第0类的概率。 对数损失（Log loss），定义如下：$$\\text{Log-loss} = -\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\left(y_{i}^{True} \\log p_{i}\\right)+\\left(1-y_{i}^{True}\\right) \\log \\left(1-p_{i}\\right)\\right)$$，其是交叉熵损失（Cross entropy loss = -$\\frac{1}{N} \\sum_{i}\\sum_{c=1}^{M} y_{i c}^{True} \\log \\left(p_{i c}\\right)$）在二分类时的特殊情况。所以说最大化似然函数等价于最小化对数损失函数。 对数几率回归为什么不使用均方误差？ $\\begin{aligned} g^{\\prime}(z) =\\frac{d}{d z} \\frac{1}{1+e^{-z}} =\\frac{1}{\\left(1+e^{-z}\\right)^{2}}\\left(e^{-z}\\right) =\\frac{1}{\\left(1+e^{-z}\\right)} \\cdot\\left(1-\\frac{1}{\\left(1+e^{-z}\\right)}\\right) =g(z)(1-g(z)) \\end{aligned}$，所以当$g(z)$接近0或1（优化目标）时，有梯度消失的问题。 均方误差与对数几率函数的组合函数（$M S E\\left(y, \\sigma\\left(X^{T} w\\right)\\right)$）是非凸的（non-convex，二阶导数在区间上可负），而对数误差与对数几率函数的组合函数是凸函数，详见参考。 四. 线性判别分析（Linear Discriminant Analysis）线性判别分析（Linear Discriminant Analysis，LDA）同样是一种线性学习方法，其将所有样例投影到一条直线上，使得同类样例投影尽可能近，不同类样例的投影尽可能远。 $\\mu_i$、$\\Sigma_i$分别表示第$i \\in{0,1}$类示例的均值向量、协方差矩阵($\\Sigma_{i j}=\\operatorname{cov}\\left(X_{i}, X_{j}\\right)=\\mathrm{E}\\left[\\left(X_{i}-\\mu_{i}\\right)\\left(X_{j}-\\mu_{j}\\right)^{\\mathrm{T}}\\right]$)。若将样本点投影到直线$\\mathbf{w}$上， 则样本投影的均值、方差分别为$\\mathbf{w}^T\\mu$、$\\mathbf{w}^T\\Sigma\\mathbf{w}$。 同类样例的投影近，意味着同类样例投影的协方差应尽可能小（下式分母）；异类样例的投影远，意味着异类投影中心之间的距离尽可能大（下式分子），即最大化：$$J = \\frac{\\Vert \\mathbf{w}^T\\mu_0 - \\mathbf{w}^T\\mu_1 \\Vert^2_2}{\\mathbf{w}^T\\Sigma_0\\mathbf{w}+\\mathbf{w}^T\\Sigma_1\\mathbf{w}}\\= \\frac{\\mathbf{w}^T(\\mu_0 - \\mu_1)(\\mu_0 - \\mu_1)^T\\mathbf{w}}{\\mathbf{w}^T(\\Sigma_0+\\Sigma_1)\\mathbf{w}}$$ 五. 基于二分类的多分类器有些二分类方法可以推广到多分类（如LDA），但现实中更多地是把多分类任务分解为多个二分类任务，利用二分类模型来解决问题。三种最经典的拆分策略，如下所示： 一对一（One vs. One，OvO）把类别两两配对，假设有$N$个类别，OvO会产生$\\frac{N(N-1)}{2}$个二分类子任务，得到对应的二分类器。测试时，新样本输入到这些模型，产生$\\frac{N(N-1)}{2}$个分类结果，最终预测由投票产生。 一对其余（One vs. Rest，OvR）把一个类的样例当作正例，其他类的样例当作反例，OvR产生$N$个二分类子任务（每个子任务都使用完整数据集）。测试时，新样本输入到这些模型，产生$N$ 个分类结果：若仅有一个模型预测为正，其对应的类别为预测结果；若有多个模型预测为正，则选择置信度最大的类别。OvO和OvR优劣：OvO需要训练较多分类器，存储开销和测试开销大；OvR训练需要要用到所有样例，训练时间开大。 多对多（Many vs. Many，MvM）将多个类作为正例，其他类作为反例，OvO和OvR均为MvM的特例，纠错输出码（Error Correcting Outputs Codes，ECOC）是一种常用的MvM技术，其工作过程分两步： 编码：训练。对$N$个类别做$M$次划分，每次划分将部分类划为正类，剩余类划分为反类，最终得到$M$个模型。每个类别在$M$次划分中，被划为正类记作+1，被划为负类记作-1，于是可以表示为一个$M$维的编码。 解码：预测。将新样本输入$M$个模型，所得的$M$个预测结果也组成一个预测编码。将预测编码和类别编码进行比较，距离最小的类别为最终预测结果。 类别划分由编码矩阵（coding matrix）表示，常见的有二元码（正类为+1，负类为-1）和三元码（停用类为0，表示不使用该类），例子如下所示： ECOC编码对分类器的错误有一定的纠错能力。编码越长，纠错能力越强，但需要训练更多分类器；对等长编码，类别之间的编码距离越远，纠错能力越强（实际中，一般不需要获取最优编码）。 六. 样本平衡若训练集中，不同类别的样例数目稍有差别，通常影响不大，但若差别很大，则会对学习过程造成困扰。例如，若有998个反例，2个正例，那么一个永远将新样本预测为反例的学习器，就能达到99.8%的精度，这样的学习器没有价值。 若直接用类别不平衡的数据集进行训练，所得模型会偏向所占比较大的类别（即，有很高概率被预测为较大占比的类），通常有以下解决方法（默认正类样例较少）： 欠采样欠采样（undersampling）去除部分反例，使得正、反类的样例数目相当。 随机采样由于去掉部分反例，所以训练时间减少，但随机丢弃反例可能会丢失重要信息。 集成学习一种解决方法是利用集成学习，将反例划分为多个集合，用于训练不同的模型，从而使得每个模型都进行欠采样，但全局上并无信息损失：EasyEnsemble（Bagging）、BalanceCascade（Boosting）。 数据清洗清洗重叠的数据，通常配合SMOTE一同使用：SMOTE+Tomek、SMOTE+ENN Tomek Link：若两个样本互为最近邻且分属不同类别，则它们形成了一个Tomek Link，移除Tomek Link就能清洗重叠样本。 Edited Nearest Neighbours(ENN)：对于多数类的一个样本，如果其K个近邻点有超过一半都不属于多数类，则剔除这个样本。 过采样过采样（oversampling）增加正例，使得正、反类的样例数目相当。由于增加了正例，所以训练时间增加。过采样不能简单地通过重复来增加正例，这会引起严重的过拟合。常见的做法是对已有正例进行插值产生新的正例，常见方法如下： SMOTE(synthetic minority oversampling technique)基于K近邻法，求出对于每一个少数类样本$\\mathbf{x}_{i}$，距离最近的k个少数类样本，其中距离为样本间的欧氏距离。然后，从这k个近邻点中随机选取$\\hat{\\mathbf{x}}_i$，根据$\\mathbf{x}_{\\text {new}}=\\mathbf{x}_{i}+\\left(\\hat{\\mathbf{x}}_{i}-\\mathbf{x}_{i}\\right) \\times \\delta$生成样本，$\\delta \\in [0, 1]$为随机数。 Border-line SMOTESMOTE没有考虑周边样本的情况，容易带来两个问题：1）若选取的少数类样本周围也是少数类样本，则新合成的样本不会增广信息；2）如果选取的少数类样本周围都是多数类样本，则新合成的样本可能是噪音。我们希望新合成的少数类样本能处于两个类别的边界附近，提供足够的信息用以分类。Border-line SMOTE对此改进，将少数样本分为三类：noise（所有k近邻样本都属于多数类）、danger（超过一半的k近邻样本属于多数类）、safe（超过一半的k近邻样本属于少数类）。算法从danger样本中随机选择，然后用SMOTE算法产生新样本。 ADASYN（adaptive synthetic sampling）SMOTE中，每个少数类样本合成等量样本，而ADASYN最大的特点是利用分布，决定每个少数类样本需要合成样本的数量。周围多数类样本点较多的少数类样本，合成更多样本。其缺点是，易受离群点的影响。 计算需要合成的样本总量：$G=\\left(S_{m a j}-S_{m i n}\\right) \\times \\beta$ 对于每个少类样本$\\mathbf{x}_i$，找出其K近邻个点，并计算分布$\\Gamma_{i}=\\frac{\\Delta_{i} / K}{Z}$，其中$\\Delta_{i}$为K领域中多数类样本的数量，$Z$为规范化因子（确保$\\Gamma_{i}$构成分布）。 最后，对每个少类别样本$\\mathbf{x}_i$，计算需要合成的样本数量$g_{i}=\\Gamma_{i} \\times G$，再用SMOTE算法合成新样本。 阈值移动阈值移动（threshold-moving）利用的是再缩放思想。 例如，在对数几率回归中，预测值$y&gt;0.5$的样本预测为正例，$y$实际上表达了正例的可能性，阔值0.5恰表明分类器认为正、反例可能性相同（即，假设了样本的真实分布为正、反例各一半）。若正例数目为 $m^+$，反例数目为 $m^-$，可以重定义：$y&gt; \\frac{m^+}{m^- + m^+}$为正例。 但必须注意，再缩放（rescaling）是基于观测几率近似真实几率的假设，现实任务中未必成立。 七. ReferenceBook: 机器学习，周志华Forum: 二元分类为什么不能用MSE做为损失函数？Blog: 机器学习之类别不平衡问题 (3) —— 采样方法Zhihu: 图解SMOTE方法Blog: Why not Mean Squared Error(MSE) as a loss function for Logistic Regression?","link":"/2020/06/06/interview-linear-model/"},{"title":"Model Ensemble","text":"Unity is strength. Ensemble methods combine multiple learning algorithms to obtain better predictive performance than any of the constituent learning algorithms alone. The main hypothesis is that when weak learning algorithms (weak learners) are correctly combined, we can obtain more accurate and/or robust models. Most of the time, these weak learners perform not so well by themselves either because they have a high bias or variance. Then, the idea of ensemble methods is to try reducing bias and/or variance of such weak learners by combining several of them to create a strong learner with better performances. Rule of ThumbHomogeneous weak learners: Bagging and boosting utilize a single base learning algorithm so that we have homogeneous weak learners trained in different ways (different data, different random state but same hyper-parameter). The ensemble model we obtain is then said to be “homogeneous”. Heterogeneous weak learners: Stacking uses different types of base learning algorithms. Some heterogeneous weak learners are then combined into a “heterogeneous” ensembles model. The choice of weak learners should be coherent with the way we aggregate these models. If we choose base models with low bias but high variance, it should be with an ensemble method that tends to reduce variance, and vice versa. BaggingBagging (bootstrap aggregating) is an ensemble method that often considers homogeneous weak learners and learns them independently from each other in parallel. It then combines weak learners following some deterministic averaging process as below: $$\\hat{f}_{b a g}=\\left\\{\\begin{array}{ll}{\\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}^{b}(x)} & {\\text { for Regression Problems }} \\\\ {\\underset{b=1 \\ldots B}{\\arg \\max } \\hat{f}^{b}(x)} & {\\text { for Classification Problems }}\\end{array}\\right.$$ Bagging is designed to improve the stability and reduces variance to avoid overfitting. The base learner it contains is supposed to be the one with low bias but high variance. Why does bagging reduce the variance?Assume that $Y_{1}, Y_{2}, \\ldots, Y_{n}$ represent the predictions of each weak learner and they are random variables with variance $\\sigma^{2}$. If each weak learner is trained on independent training set drawn from the true unknown underlying distribution, then $Y_{1}, Y_{2}, \\ldots, Y_{n}$ are i.d.d.. Then the variance of their average (i.d.d. random variables) is: $$\\operatorname{Var}\\left(\\frac{Y_{1}+Y_{2}+\\ldots+Y_{n}}{n}\\right)=\\operatorname{Var}\\left(\\frac{Y_{1}}{n}\\right)+\\ldots+\\operatorname{Var}\\left(\\frac{Y_{n}}{n}\\right)=\\frac{\\sigma^{2}}{n}$$ , which means that the variance decreases from $\\sigma^{2}$ to $\\frac{\\sigma^{2}}{n}$ (while the average does not change). However, there is one problem here - we do not have access to multiple training data sets. Instead, we can bootstrap by taking repeated samples from the initial training data set. BootstrappingIn statistics, bootstrapping is any test or metric that relies on random sampling with replacement on the initial dataset. There two hypotheses proposed in Bootstrapping. First, the size $N$ of the initial dataset should be large enough to capture most of the complexity of the true unknown underlying distribution so that sampling from the dataset is a good approximation of sampling from the real distribution (representativity). Second, the size $N$ of the dataset should be large enough compared to the size $B$ of the bootstrap samples so that samples are not too much correlated (independence). Under these hypotheses above, these bootstrapped training data sets can be seen as being drawn both directly from the true unknown underlying distribution. So, they can be considered as approximately representative and independent (almost i.i.d.) samples of the true data distribution. Out-of-Bag (OOB) ErrorOOB error is the mean prediction error on each training sample $x_i$, using only the weak learners that did not have $x_i$ in their bootstrap samples. One big advantage of bagging is that we can compute testing error without any cross-validation by computing OOB error. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation. Bagging in tree modelBagging works particularly well for decision trees when they are aggregated as Random forest. BoostingBoosting is an ensemble method that often considers homogeneous weak learners and learns them from previously learned weak learners in sequential. Boosting is designed to improve the stability and reduces bias to avoid underfitting. The base learner it contains is supposed to be the one with low variance but high bias. Why does boosting reduce the bias?In contrast, boosting does not involve bootstrapping. Instead, the weak learner is sequentially fitted on a modified (weighted) version of the original data set. The fitting process is adaptative: each learner is fitted giving more importance to observations in the dataset that were badly handled by the previous learners. Intuitively, each new learner focus its efforts on the most difficult observations to fit up to now, so that we obtain, at the end, a strong learner with lower bias. Boosting in tree modelBoosting works particularly well for decision trees when they are aggregated as Boosted trees. StackingStacking (also called meta ensembling) is a model ensembling technique that often considers heterogeneous weak learners and learns them independently from each other in parallel. Stacked model will outperform each of the individual model due its ability to highlight each base model where it performs best and discredit each base model where it performs poorly. For this reason, stacking is most effective when the base models are significantly different. So intuitively, stacking will ideally reduce both bias and variance, but it’s especially efficient at preventing overfitting and variance. Algorithm First, all of weak learners are trained using the available data. Then, a combiner algorithm (meta-model) is trained to make a final prediction using all the predictions of the weak learners as inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques. In practice, a logistic regression model is often used as the combiner. When we train stacked models: In order to prevent data leakage, we should make sure that the data having been used for the training of the weak learners will not be used again for the training of the meta-model. And to make more efficient use of data, k-folds cross-training approach can be adopted to train the model, which can be shown as the figure below. Train each weak learner on $k-1$ folds and make predictions on the remaining fold, and do it iteratively to obtain predictions for all observations. Then train our meta-model on all these predictions. Multi-levels StackingA possible extension of stacking is multi-level stacking. It consists in doing stacking with multiple layers. We should also mention that adding levels can either be data expensive (if k-folds like technique is not used) or time expensive (if k-folds like technique is used). ReferenceBlog: Ensemble methods: bagging, boosting and stackingKaggle: Stacked Regressions to predict House PricesKaggle: Introduction to Ensembling/Stacking in PythonGuide to Model Stacking (i.e. Meta Ensembling)","link":"/2019/11/29/model-ensemble/"},{"title":"Model Selection & Bias–Variance Tradeoff","text":"In this passage, I summarize some basic concepts in Machine Learning, which mainly include model validation methods in model selection and the interpretation in bias–variance tradeoff. Probabilistic SetupIn the following, we set up a general probabilistic view on the data. Assume that the data is generated as (Data Generation Model) $$y=f(\\mathbf{x})+\\varepsilon$$, where $f$ is some arbitrary and unknown function and $\\mathbf{x}$ is generated according to some ﬁxed but unknown distribution $\\mathcal{D}_{\\mathbf{x}}$ . $\\varepsilon$ is an additive noise with distribution $\\mathcal{D}_{\\varepsilon}$, which is independent from sample to sample and independent from the data. Assume $\\varepsilon$ has zero mean (otherwise this constant can be absorbed into $f$). There is an unknown underlying joint distribution $\\mathcal{D}$ on pairs $(\\mathbf{x}, y)$, with range $\\mathcal{X} \\times \\mathcal{Y}$. The data set $\\mathcal{S}$ we see consists of independent and identically distributed(i.i.d.) samples from $\\mathcal{D}$: $$S=\\left\\{ \\left(\\mathbf{x}_{n}, y_{n}\\right) \\text {i.i.d.} \\sim \\mathcal{D}\\right\\}_{n=1}^{N}$$ Write $f_{\\mathcal{S}} = \\mathcal{A}(\\mathcal{S})$, where $\\mathcal{A}$ denotes the learning algorithm, $f_{\\mathcal{S}}$ is the resulting prediction function or model which depends on the data subset $\\mathcal{S}$ we are given. If we want to indicate that $f_{\\mathcal{S}}$ also depends on hyper-parameters of the model, we can add a subscript to write $f_{\\mathcal{S, \\lambda}}$. Model Selection Model (actually hyper-parameters) is selected by the result of VALIDATION. Given a model $f_{\\mathcal{S}}$, how can we assess if $f_{\\mathcal{S}}$ is any good? Theoretically, the best way is to compute the expected error over all possible samples $(\\mathbf{x}, y)$ chosen from $\\mathcal{D}$: $$L_{\\mathcal{D}}(f_{\\mathcal{S}})=\\mathbb{E}_{\\mathcal{D}}[\\ell(y, f_{\\mathcal{S}}(\\mathbf{x}))]$$ , where $\\ell(\\cdot, \\cdot)$ is our loss function. The quantity $L_{\\mathcal{D}}(f_{\\mathcal{S}})$ is called generalization error. Training, Validation &amp; Test SetTraining Set: A set of data used for learning, to fit the weights of the model. We would use the training set to find the “optimal” weights with the back-propagation algorithm. (Get optimal training error) Validation Set: A set of data used to tune the hyper-parameters of the model. We would use the validation set to find the “optimal” hyper-parameters or determine a stopping point for the back-propagation algorithm. (Get optimal validation error) Test Set: A set of data used only to assess the performance of the fully-trained model. After assessing the final model on the test set, YOU MUST NOT tune the model any further! Why separate test and validation sets?The error estimate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model. Generalization Error V.S. Training ErrorGeneralization error is the quantity we are fundamentally interested in, but we cannot compute it since $\\mathcal{D}$ is unknown. Since we have given a data subset $\\mathcal{S}$, it is natural to compute the equivalent empirical quantity: $$L_{\\mathcal{S}}\\left(f_{\\mathcal{S}}\\right)=\\frac{1}{|\\mathcal{S}|} \\sum_{\\left(\\mathbf{x}_{n}, y_{n}\\right) \\in \\mathcal{S}} \\ell\\left(y_{n}, f_{\\mathcal{S}}\\left(\\mathbf{x}_{n}\\right)\\right)$$ This is often called the training error, but $L_{\\mathcal{S}}(f_{\\mathcal{S}})$ might not be close to $L_{\\mathcal{D}}(f_{\\mathcal{S}})$ for the sake of overﬁtting. Generalization Error V.S. Test ErrorValidating model on the same data we trained it on will result in overfitting, so we could split the data into a training and a test set (a.k.a. validation set), call them $\\mathcal{S}_{\\text {train}}$ and $\\mathcal{S}_{\\text {test}}$, respectively. From the training set, we learn the model $\\mathcal{S}_{\\text {train}}$ and then compute the error on the test set: $$L_{\\mathcal{S}_{\\text {test}}}(f_{\\mathcal{S}_{\\text {train}}})=\\frac{1}{\\left|\\mathcal{S}_{\\text {test }}\\right|} \\sum_{\\left(y_{n}, \\mathbf{X}_{n}\\right) \\in \\mathcal{S}_{\\text {test}}}\\ell\\left(y_{n}, f_{\\mathcal{S}_{\\text {train }}}\\left(\\mathbf{x}_{n}\\right)\\right)$$ , where $L_{\\mathcal{S}_{\\text {test}}}(f_{\\mathcal{S}_{\\text {train}}})$ is called test error (empirical error). Indeed, the expectation of test error equals generalization error: $$L_{\\mathcal{D}}\\left(f_{\\mathcal{S}_{\\text {train }}}\\right)=\\mathbb{E}_{\\mathcal{S}_{\\text {test }} \\sim \\mathcal{D}}\\left[L_{\\mathcal{S}_{\\text {test }}}\\left(f_{\\mathcal{S}_{\\text {train }}}\\right)\\right]$$ But for a particular test set $\\mathcal{S}_{\\text {test}}$, how far are these apart? According to lemma: Chernoff bound. Let $\\Theta_{1}, \\ldots, \\Theta_{N}$ be a sequence of i.i.d. random variables with mean $\\mathbb{E}[\\Theta]$ and range $[a, b]$. Then, for any $\\varepsilon&gt; 0$,$$\\mathbb{P}\\left[\\left|\\frac{1}{N} \\sum_{n=1}^{N} \\Theta_{n}-\\mathbb{E}[\\Theta]\\right|\\geq \\varepsilon \\right] \\leq 2 e^{-2N\\varepsilon^{2}(b-a)^2}$$ we claim that: $$\\mathbb{P}\\left[\\left|L_{\\mathcal{D}}(f)-L_{\\mathcal{S}_{\\text {test}}}(f)\\right| \\geq \\sqrt{\\left.\\frac{(b-a)^{2} \\ln (2 / \\delta)}{2\\left|\\mathcal{S}_{\\text {test }}\\right|}\\right)}\\right] \\leq \\delta$$, from which we could have the **insight** that the difference decreases as $\\mathcal{O}(1 / \\sqrt{\\left|S_{\\text {test }}\\right|})$. So the more data points we have, the more conﬁdent that the **empirical loss we measure is close to the true loss**. Cross Validation Splitting the data once into two parts (one for training and one for testing) is not the most efficient way to use the data. Cross-validation is a better way. Methods: See more K-fold Cross-Validation: Randomly partition the data into K groups. Each time leave out exactly one of the K groups for testing and use the remaining K-1 groups for training. Stratified K-fold: It is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set. Nested Cross-Validation: Always used for time series data, where particular care must be taken in splitting the data in order to prevent data leakage. It means to evaluate the model for time series data on the “future” observations. Nested Cross-Validation is a variation of k-fold which returns first k folds as train set and the (k+1) th fold as test set. Outcomes Cross-validation returns an unbiased estimate of the generalization error and its variance. We can average the K validation errors as the validation result. The purpose of cross-validation is model checking, not model building. Once we have used cross-validation to select the better performing model, we train that model on all the data. Cross-validation will not be used in complex deep learning model with a large dataset, since it’s too computationally expensive. Advantages Use All Data: Splitting the data once into two parts is not the most efficient way to use the data. It only gives predictions on part of the data (test set). If the test set we pick happens to contain a bunch of points that are particularly easy (or hard) to predict, we will not come up with a rational estimate of the model’s ability to learn and predict. Prevent Data Leakage: Work with Dependent/Grouped Data: For example, there are 3 speakers and 500 recordings for each speaker. If we do a random split, our training and test set will share the same speaker saying the same words, which will boost our algorithm performance but fail when tested on a new speaker. The proper way to do it is to split the speakers, i.e., use 2 speakers for training and the third for testing (cross-validation on the speakers level). Cross-validation can be used in models stacking, where we can’t train both our models on the same dataset because then, our second model will learn on ground truth information that our first model already seen. Bias–Variance Tradeoff The bias–variance tradeoff is the property of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples and vice versa. The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). Ideally, one wants to choose a model that both accurately captures the regularities in its training data (high variance), but also generalizes well to unseen data (high bias). There is an inherent bias-variance tradeoff when we perform the model selection. Error DecompositionFollowing Probabilistic Setup and test error defined above, we look at the mean squared error of the prediction function $f_{\\mathcal{S}}$ for a ﬁxed element $\\mathbf{x}_0$. Imagine that we are running the experiment many times: we create $\\mathcal{S}_{\\text {train }}$, learn the model $f_{\\mathcal{S}_{\\text {train }}}$, and then evaluate the performance by computing the mean squared error for $\\mathbf{x}_0$ ($y_0=f(\\mathbf{x}_0)+\\varepsilon$). $$ \\begin{split} & \\mathbb{E}_{\\mathcal{S}_{\\text {train }} \\sim \\mathcal{D}, \\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)+\\varepsilon-f_{\\mathcal{S}_{\\text {train }}}\\left(\\mathbf{x}_{0}\\right)\\right)^{2}\\right] \\\\ = &\\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]+\\mathbb{E}_{\\mathcal{S}_{\\text {train } \\sim \\mathcal{D}}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-f_{\\mathcal{S}_{\\text {train }}\\left(\\mathbf{x}_{0}\\right)}\\right)^{2}\\right] \\\\ &+2\\cdot \\underbrace{\\mathbb{E}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]}_{0}\\cdot\\mathbb{E}_{\\mathcal{S}_{\\text {train } \\sim \\mathcal{D}}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-f_{\\mathcal{S}_{\\text {train }}\\left(\\mathbf{x}_{0}\\right)}\\right)\\right] \\\\ = &\\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]+\\mathbb{E}_{\\mathcal{S}_{\\text {train } \\sim \\mathcal{D}}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-f_{\\mathcal{S}_{\\text {train }}\\left(\\mathbf{x}_{0}\\right)}\\right)^{2}\\right] \\\\ = & \\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}\\left[\\varepsilon\\right]+\\mathbb{E}_{\\mathcal{S} \\sim \\mathcal{D}}\\left[\\left(f\\left(\\mathbf{x}_{0}\\right)-\\mathbb{E}_{\\mathcal{S}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]\\right) + \\left(\\mathbb{E}_{\\mathcal{S}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]-f_{\\mathcal{S}}\\left(\\mathbf{x}_{0}\\right)\\right)^2\\right] \\\\ =& \\underbrace{\\operatorname{Var}_{\\varepsilon \\sim \\mathcal{D}_{\\varepsilon}}[\\varepsilon]}_{\\text {noise variance }} \\\\ & + \\underbrace{\\left(f\\left(\\mathbf{x}_{0}\\right)-\\mathbb{E}_{\\mathcal{S}_{\\text {train }}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}_{\\text {train }}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]\\right)^{2}}_{\\text {bias }}\\\\ & + \\mathbb{E}_{\\mathcal{S}_{\\text {train }} \\sim \\mathcal{D}}[\\underbrace{\\left(\\mathbb{E}_{\\mathcal{S}_{\\text {train }}^{\\prime} \\sim \\mathcal{D}}\\left[f_{\\mathcal{S}_{\\text {train }}^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]-f_{\\mathcal{S}_{\\text {train }}}\\left(\\mathbf{x}_{0}\\right)\\right)^{2}}_{\\text { variance }}] \\end{split} $$ Each of the three terms above is nonnegative, so each of them is a lower bound on the true error for the input $\\mathbf{x}_0$. $\\varepsilon$ is an irreducible error. The bias term is the square of the difference between the actual value $f\\left(\\mathbf{x}_{0}\\right)$ and the expected prediction $\\mathbb{E}_{S^{\\prime} \\sim \\mathcal{D}}\\left[f_{S^{\\prime}}\\left(\\mathbf{x}_{0}\\right)\\right]$. The variance term is the variance of the prediction function. Approaches Simple model, more data, less features, underfitting → Large bias Complex model, less data, more features, overfitting → Large variance Dimensionality reduction and feature selection can decrease variance by simplifying models. Learning algorithms typically have some tunable hyper-parameters that control bias and variance. For example: Linear and generalized linear models can be regularized to decrease variance at the cost of increasing bias. (It’s the essence of regulation to avoid overfitting) In artificial neural networks, the variance increases and the bias decreases as the number of hidden units increase. In k-nearest neighbor models, a high value of k (simple model) leads to high bias and low variance. In decision trees, the depth of the tree determines the variance. Ensemble learning is also one way of resolving the trade-off. For example, boosting combines many “weak” (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines “strong” learners in a way that reduces their variance. ReferenceCourse: Machine Learning@EPFLWikipedia: Bias–variance tradeoffBlog: 5 Reasons why you should use Cross-Validation in Data Science ProjectsStack Exchange: How to choose a predictive model after k-fold cross-validation?Zhihu: 求大神通俗易懂的解释下交叉验证cross-validation？","link":"/2019/10/31/model-selection-&-bias–variance-tradeoff/"},{"title":"Optimization Algorithm in Deep Learning","text":"An optimization algorithm is a procedure which is executed iteratively by comparing various solutions until an optimum or a satisfactory solution is found. Here, we introduce some optimization algorithms commonly used in deep learning. Gradient Descent Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point as following:$$w = w - \\eta \\nabla Q(w) $$ Batch Gradient Descent Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated. Batch gradient descent decreases update frequency but takes much more time per iteration. Batch gradient descent has a more stable cost gradient and the cost goes down on every iteration. However, it may result in premature convergence of the model to a local minimum. It is an estimate of the function gradient. Batch gradient descent requires the entire training dataset in memory and available to the algorithm, which is memory-consuming. Stochastic Gradient Descent Stochastic gradient descent is a variation of the gradient descent algorithm that calculates the error and updates the model for each example in the training dataset. It can be regarded as a stochastic approximation of batch gradient descent optimization, since it replaces the actual gradient by an estimate. Stochastic gradient descent increases model update frequency and can result in faster learning on some problems. Stochastic gradient descent can be extremely noisy, which may cause the model error/cost to jump around (higher variance). It is an unbiased estimate of the full gradient. The noisy update process can allow the model to avoid local minima, but it also makes it hard to settle on a minimum (oscillate around the region of the minimum). Stochastic gradient descent loses the speed up from vectorization (matrix computation). Mini-Batch Gradient Descent Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients. Mini-batch gradient descent seeks to find a balance between the robustness of batch gradient descent and the efficiency of stochastic gradient descent.$$w = w - \\eta \\nabla Q(w) = w - \\eta \\sum_{i=1}^n \\nabla Q_i(w)/n$$ The cost of mini-batch gradient descent should trend downwards with noises (oscillations). Since some easy mini-batches might have a lower cost, while hard mini-batches (with some mislabeled examples) might have a higher cost. The larger step size can be adopted compared to stochastic gradient descent since the gradient is more stable. The batch size is a hyper-parameter that can be tuned as a power of two that fits the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, and so on. Adaptive Learning Rate DescentAdaGrad AdaGrad (adaptive gradient descent) algorithm is a modified stochastic gradient descent with per-parameter learning rate. AdaGrad has a base learning rate $\\eta$, but this is multiplied with the elements of a vector ($G_{j, j}$) which is the diagonal of the matrix:$$G = \\sum_{\\tau=1}^t g_\\tau g_\\tau^T$$ where $g_\\tau = \\nabla Q_i(w)$, the stochastic gradient at iteration $\\tau$. The diagonal is given by $G_{j,j} = \\sum_{\\tau=1}^t g_{\\tau,j}^2$. The formula for an update is now:$$w = w - \\eta, diag(G)^{-\\frac{1}{2}} \\circ g$$, where the $\\circ$ is the element-wise product. It can also be written as per-parameter updates $w_j = w_j - \\frac{\\eta}{\\sqrt{G_{j,j}}} g_j.$ AdaGrad increases the learning rate for sparser parameters and decreases the learning rate for ones that are less sparse. This strategy often improves convergence performance over sparse data. The intuition behind is that: with the number of updates increases, it is getting closer to the optimal solution, the learning rate supposed to be down to avoid oscillations. In AdaGrad, the learning rate will shrink with large updates or frequent updates.RMSProp RMSProp (Root Mean Square Propagation) is also a method in which the learning rate is adapted for each of the parameters. The idea is to divide the learning rate by an exponentially weighted average (detailed described in my previous blog) of the magnitudes of recent gradients. The formula for an update is shown below:$$\\begin{cases}v_w=\\gamma v_w+(1-\\gamma)(\\nabla Q(w))^2 \\\\w=w-\\frac{\\eta}{\\sqrt{v_w+\\epsilon}}\\nabla Q(w)\\end{cases}$$ where, $\\gamma$ is the forgetting factor, $(\\nabla Q(w))^2$ is component-wise squared and $\\epsilon$ is set to ${10}^{-8}$ to avoid blow up. The intuition of RMSProp is quite similar to Adagrad. The advantage of using the moving average instead of the accumulation of $(\\nabla Q(w))^2$ is that the learning rate can not only be gradually adapted smaller but larger if the feature becomes sparse in recent updates. Gradient Descent with Momentum Stochastic gradient descent with momentum remembers the update $\\Delta \\omega$ at each iteration, and determines the next update as a linear combination of the gradient and the previous update. In one sentence, the basic idea is to compute an exponentially weighted average of your gradients, and then use that gradient to update your weights instead. Stochastic gradient descent with momentum remembers the update $w$ at each iteration:$$\\begin{cases}{m_{w}=\\beta m_{w}+(1-\\beta) \\nabla Q(w)} \\\\ \\tilde{m}_ w = \\frac{m_{w}}{1-\\beta^{t}} \\\\{w=w-\\alpha \\tilde{m}_{w}}\\end{cases}$$where $t$ is the iteration, $\\alpha$ is the learning rate, $\\beta$ is the forgetting factor always set to 0.9 (average approximately over last 10 gradients). The name momentum stems from an analogy to momentum in physics. Unlike in classical stochastic gradient descent, it tends to keep traveling in the same direction, preventing oscillations. Momentum averages out oscillations of its gradients. The oscillations in some directions will tend to be averaged out and the gradients will get closer to zero. Adam Adam (Adaptive Moment Estimation ) combines RMSprop together with momentum and is an update to the RMSProp optimizer. Given parameters $w^{(t)}$ and a loss function $Q^{(t)}$, where $t$ indexes the current training iteration (indexed at 0), Adam’s parameter update is given by: $$\\begin{cases}m_w ^ {(t+1)} \\leftarrow \\beta_1 m_w ^ {(t)} + (1 - \\beta_1) \\nabla _w Q ^ {(t)}\\\\v_w ^ {(t+1)} \\leftarrow \\beta_2 v_w ^ {(t)} + (1 - \\beta_2) (\\nabla _w Q ^ {(t)} )^2\\\\\\hat{m}_w = \\frac{m_w ^ {(t+1)}}{1 - (\\beta_1) ^{t+1}}\\\\\\hat{v}_w = \\frac{ v_w ^ {(t+1)}}{1 - (\\beta_2) ^{t+1}}\\\\w ^ {(t+1)} \\leftarrow w ^ {(t)} - \\eta \\frac{\\hat{m}_w}{\\sqrt{\\hat{v}_w} + \\epsilon}\\end{cases}$$ where $\\epsilon$ is a small scalar used to prevent division by 0, and $\\beta_1$ and $\\beta_2$ are the forgetting factors for gradients and second moments of gradients, respectively. Squaring and square-rooting is done elementwise. Learning rate $\\eta$ is important and usually we need to try a range of values and see what works. A common choice for $\\beta_1$ is 0.9 and the hyper parameter $\\beta_2$ is recommend 0.999. $\\eta$ is always set to $10^{-8}$ and bias correction is implemented.Second-Order Methods… to be continued.ReferenceWiki: Stochastic gradient descentCoursera: Deep Learning SpecializationBlog: A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size","link":"/2019/10/16/optimization-algorithm-in-deep-learning/"},{"title":"Normalization in Deep Learning","text":"Normalization refers to a process that makes something more normal or regular. In statistics, normalization means adjusting values measured on different scales to a common scale, or adjustments to bring the entire probability distributions of adjusted values into alignment. Normalization in deep learning can make the model to train efficiently. Why does Normalization work?Intuition 1: Input Normalization $\\frac{X - \\mu}{\\sigma}$ Make training less sensitive to the scale of features. After optimization, the feature with a large scale will always be weighted with a small value ($\\omega_{small}$), and vise versa ($\\omega_{large}$). Because of the large scale of the feature, a tiny change in the small weight ($\\omega_{small}$) will change the prediction by a lot compared to the same change in the large weight ($\\omega_{large}$). It means that setting $\\omega_{small}$ correctly might dominate the optimization process and the feature with a large scale is of more importance which actually makes no sense. Make optimization well-conditioned. The rough intuition is that the cost function will be more round and easier to optimize when features are all on similar scales. Cost function of the unnormalized features can be a very elongated bowl, whereas if we normalize the features, then the cost function will look more symmetric. Having an elongated cost function, we might have to use a very small learning rate and gradient descent might need a lot of steps to oscillate back and forth before it finally gets to the minimum. If we have more spherical contours, then gradient descent can pretty much go straight to the minimum and we can take much larger steps with gradient descent without much oscillation. Intuition 2: Covariate Shift Covariate shift refers to the problem that the distribution of the input values changes, but the concept (model) being learned remains stationary. In deep learning, the basic idea behind normalization is to limit the covariate shift, which allows the model to learn on a more stable distribution, and would thus accelerate the training of the network. Batch NormalizationIn deep learning, we are particularly concerned with the change in the distribution of the inputs to the hidden nodes within a network. A neural network changes the weights of each layer over the training, which means that the activations of each layer change as well. Since the activations of a previous layer are the inputs of the next layer, each layer in the neural network is faced with a situation where the input distribution changes with each step (covariate shift). Batch Normalization at Training Time What batch normalization does is, especially from the perspective of the later layers of the neural network, it limits the earlier layers to not get to shift around much, by restricting them to have the same mean and variance. It weakens the coupling between what the early layers and the later layers. For a layer of the network with d-dimensional input, $x = (x^{(1)},…,x^{(d)})$, each dimension is then normalized separately as following: $$\\hat{x}_i^{(k)} = \\frac{x_i^{(k)}-\\mu_B^{(k)}}{\\sqrt{\\sigma_B^{(k)^2}+\\epsilon}}$$where $\\mu_B^{(k)}$ and $\\sigma_B^{(k)^2}$ are the per-dimension mean and variance, respectively. $\\epsilon$ is added in the denominator for numerical stability and is an arbitrarily small constant. The resulting normalized $\\hat{x}^{(k)}$ have zero mean and unit variance, to restore the representation power of the network or better take advantage of the nonlinearity (if activations are normalized), a transformation step then follows as:$$y_i^{(k)} = \\gamma^{(k)} \\hat{x}_{i}^{(k)} +\\beta^{(k)}$$where the parameters $\\gamma^{(k)}$and $\\beta^{(k)}$ are subsequently learnt in the optimization process and can convert the mean and variance to any value that the network desires. Formally, the transform $BN_{\\gamma,\\beta}: x_{1…m} \\rightarrow y_{1…m}$ is denoted as the Batch Normalizing Transform. There are some debates in deep learning about whether weshould normalize the value before the activation function or after it. In practice, normalizing before the activation function is done much more often. Regularization can be motivated as a technique to improve the generalizability of a learned model. Batch normalization has a slight regularization effect. The mean and variance are a little bit noisy because they are estimated on a mini-batch. Similar to dropout, batch normalization adds small noise to the hidden layers and therefore has a very slight regularization effect. We don’t particularly use to batch normalization as a regularization but use it as a way to speed up learning. Batch Normalization at Test Time At the test time, we use the $\\gamma$ and $\\beta$ learned from training, the $\\mu$ and $\\sigma^2$ are estimated from the training set. Running the whole training set to get $\\mu$ and $\\sigma^2$ could be memory-consuming. So we keep a running average of the $\\mu$ and $\\sigma^2$ for each layer as we train the neural network across different mini-batches to get an estimation of them. In practice, the moving average people usually use here is exponentially weighted average. An exponential moving average (EMA), also known as an exponentially weighted moving average (EWMA), is a first-order filter that applies weighting factors which decrease exponentially. The EMA for a series $Y$ is calculated recursively: $$S_t = \\begin{cases} \\alpha \\cdot Y_1, &amp; t = 1 \\\\ \\alpha \\cdot Y_t + (1 - \\alpha) \\cdot S_{t-1}, &amp; t &gt; 1\\end{cases}$$Where: $\\alpha$ represents the degree of weighting decrease, a constant smoothing factor between 0 and 1. $Y_t$ and $S_t$ are the values at a time period $t$ respectively. This formula can also be expressed in technical analysis terms as follows: How the EMA steps towards the latest datum point:$$\\text{EMA}_\\text{today} = \\text{EMA}_\\text{yesterday} + \\alpha \\left[\\text{price}_\\text{today} - \\text{EMA}_\\text{yesterday}\\right]$$ How the weighting factor on each data point $p_1$, $p_2$, etc., decreases exponentially by expanding out $\\text{EMA}_\\text{yesterday}$ :$$\\begin{split}\\text{EMA}_\\text{today}= &amp; { \\alpha \\left[p_1 + (1 - \\alpha) p_2 + (1 - \\alpha)^2 p_3 + (1 - \\alpha)^3 p_4 + \\cdots \\right] } \\\\= &amp;\\frac{p_1 + (1 - \\alpha) p_2 + (1 - \\alpha)^2 p_3 + (1 - \\alpha)^3 p_4 + \\cdots}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + (1 - \\alpha)^3 + \\cdots}\\end{split}$$since $1/\\alpha = 1 + (1 - \\alpha) + (1 - \\alpha)^2 + \\cdots$ (weighted average). A couple of things about choosing $\\alpha$: A higher $\\alpha$ discounts older observations faster so that the EMA will adapt more quickly to the current changes. When compute EMA, we can think of $S_t$ as approximately averaging over $\\frac{1}{\\alpha}$ number of data. So with a low value of $\\alpha$, the EMA is much smoother and also has more latency. (Always set to 0.1 in deep learning) Bias correction is a technical modification that makes EMA estimate more accurate, especially during its initial phase. The EMA is very low when the iteration just starts off and we will get a much lower value that is not a very good estimate of the first data. In order to fix the problem, we could instead of taking $S_t$, take $\\frac{S_t}{1-(1-a)^t}$, namely bias correction. It will make the EMA larger when $t$ is small and make almost no difference when $t$ is large enough. Weight NormalizationThe Limitations of Batch NormalizationThe key limitation of batch normalization is that it is dependent on the mini-batch. It puts a lower limit on the batch size. Ideally, we want to use the global mean and variance to normalize the inputs to a layer. However, it’s too costly and the mean and variance are simply estimates on mini-batch, which means that they contain a certain amount of noises. Smaller mini-batch sizes increase the variance of these estimates. It can be a problem in settings such as online learning and reinforcement learning which is highly sensitive to noise. It is difficult to apply to recurrent connections in recurrent neural network. In a recurrent neural network, the recurrent activations of each time-step will have different statistics. This means that we have to fit a separate batch normalization layer for each time-step and it forces us to store the statistics for each time-step during training. Theory of Weight Normalization What weight normalization does is it separates the norm of the weight vector from its direction. In an effort to speed up the convergence of optimization procedure, it proposes to reparameterize each weight vector $\\omega$ in terms of a parameter vector $\\boldsymbol{v}$ and a scalar parameter $g$ in the following way: $$\\omega = \\frac{g}{||\\boldsymbol{v}||}\\boldsymbol{v}$$It then optimizes both$\\boldsymbol{v}$ and $g$ using gradient descent. This reparameterization has the effect of fixing the Euclidean norm of the weight vector $\\omega$: we now have $||\\omega||=g$, independent of the parameters $\\boldsymbol{v}$. We, therefore, call this reparameterization weight normalization. Layer NormalizationTheory of Layer Normalization The key feature of layer normalization is that it normalizes the inputs across the features. The equations of layer normalization and batch normalization are similar:$$\\hat{x}_i^{(k)} = \\frac{x_i^{(k)}-\\mu_L}{\\sqrt{\\sigma_L^2+\\epsilon}}$$where $\\mu_L$ and $\\sigma_L^2$ are the mean and variance across the features, respectively. ReferenceQuora: Why do we normalize the dataCoursera: Deep Learning SpecializationBlog: An Intuitive Explanation of Why Batch Normalization Really WorksBlog: Weight Normalization and Layer Normalization ExplainedBlog: An Overview of Normalization Methods in Deep Learning","link":"/2019/10/14/normalization-in-deep-learning/"},{"title":"算法思维：排序算法","text":"排序算法（Sorting algorithm）是能将一串数据依照特定排序方式进行排列的一种算法，最常用到的排序方式是数值顺序以及字典顺序。常见的比较排序算法如下： 一. 基础排序算法1. 冒泡排序123456def bubble_sort(nums): for i in range(len(nums)-1, 0, -1): # 从nums[n]到nums[1], 逐个排序；nums[0]自行补全 for j in range(i): # j最大至i-1 if nums[j+1] &lt; nums[j]: nums[j+1], nums[j] = nums[j], nums[j+1] return nums 主要思路：Repeatedly swapping the adjacent elements if they are in wrong order；Every time it correct the order of last element；t=O(n^2), s=O(1)；确定一个位置的顺序，需多次交换。 2. 选择排序12345678def selection_sort(nums): for i in range(len(nums)-1, 0, -1): tmp = 0 # 对比的起始位置 for j in range(i+1): if nums[j] &gt; nums[tmp]: tmp = j # 记录最大值的位置 nums[i], nums[tmp] = nums[tmp], nums[i] # 将最大值交换到最后 return nums 主要思路：Repeatedly finding the maximum element (considering ascending order) from unsorted part and putting it at the end；t=O(n^2), s=O(1)；确定一个位置的顺序，仅需一次交换。 3. 插入排序12345678def insertion_sort(nums): for i in range(len(nums)): pos, val = i-1, nums[i] while pos &gt;= 0 and val &lt; nums[pos]: nums[pos+1] = nums[pos] pos -= 1 nums[pos+1] = val return nums 主要思路：Insertion sort works the way we sort playing cards；1）Insertion sort iterates, consuming one input element each repetition, and growing a sorted output list. 2） At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there.3）It repeats until no input elements remain；t=O(n^2), s=O(1)。 例子：Leetcode 147. Insertion Sort List 4. 希尔排序123456789101112def shell_sort(nums): n = len(nums) gap = n // 2 while gap &gt; 0: for i in range(0, n, gap): # 即步进为gap的插入排序 pos, val = i - gap, nums[i] while pos &gt;= 0 and nums[pos] &gt; nums[i]: nums[pos + gap] = nums[pos] pos = pos - gap nums[pos + gap] = val gap = gap // 2 return nums 希尔排序，也称递减增量排序算法，是插入排序的一种更高效的改进版本；算法最终需以步长为1进行排序，此时变为普通插入排序，保证了数据一定会被排序（故最终步长为1的任何步长序列都可以工作）；一般步长选择为n/2，并且对步长取半直到1，最坏情况t=O(n^2)。 算法具体如下： 希尔排序将全部元素分为几个区域，以提升插入排序的性能，可以让元素一次性地朝最终位置前进一大步； 逐步取越来越小的步长进行排序，算法的最后一步就是普通的插入排序（此时数据几乎已排好）。 5. 归并排序123456789101112131415161718192021def merge_sort(nums): def divide(nums, left, right): if left &gt;= right: return [nums[left]] mid = left + (right - left) // 2 s_left = divide(nums, left, mid) s_right = divide(nums, mid + 1, right) return conquer(s_left, s_right) def conquer(s_left, s_right): res = [] while s_left and s_right: if s_left[0] &lt; s_right[0]: res.append(s_left.pop(0)) else: res.append(s_right.pop(0)) remain = s_left if s_left else s_right res += remain return res return divide(nums, 0, len(nums)-1) 主要思路：A Divide and Conquer algorithm；It divides input array in two halves, calls itself for the two halves and then merges the two sorted halves；t=O(nlgn), s=O(n)。 例子：Leetcode 88. Merge Sorted ArrayLeetcode 315. Count of Smaller Numbers After Self 6. 快速排序1234567891011121314151617181920def quick_sort(nums): def sort(nums, left, right): if left &gt;= right: # 注意停止条件 return pivot_pos = partition(nums, left, right) # 实际上是conquer的过程 sort(nums, left, pivot_pos-1) sort(nums, pivot_pos+1, right) def partition(nums, left, right): pivot = nums[right] lower_idx = left - 1 # lower_idx左侧为小于pivot的数，右侧为大于pivot的数 for i in range(left, right): # nums[right]作为pivot，未被遍历 if nums[i] &lt; pivot: lower_idx += 1 nums[i], nums[lower_idx] = nums[lower_idx], nums[i] nums[lower_idx+1], nums[right] = nums[right], nums[lower_idx+1] # 将pivot放到确定位置处 return i+1 sort(nums, 0, len(nums)-1) return nums 主要思路：采用分而治之的思想；针对特殊情况（顺序数组或者逆序数组），最好随机化选择基准，否则将导致，子问题划分不平均，时间复杂度提高为O(n^2)；时间复杂度t=O(nlogn)～t=O(n^2)，s=O(lgn)～O(n)（占用的空间主要是栈空间）。 算法具体如下: 从数列中挑出一个元素，称为 “基准”（pivot）；一般选择数组首元素或尾元素为基准； 将比基准值小的元素摆放在基准前，比基准值大的元素摆在基准后（相同的数可在任一边）；这称为分区（partition）操作，分区后，基准的位置被排定； 递归地把小于基准值元素的子数列和大于基准值元素的子数列排序。 7. 堆排序参照队列中的优先队列部分。 二. 其他用例1. 计算右侧小于当前元素的个数：Leetcode 315. Count of Smaller Numbers After Self，[参考1]，[参考2]，[参考3]。12345678910111213141516171819202122232425262728293031323334class Solution(object): def countSmaller(self, nums): if not nums: return [] def sort(nums, left, right): # 在merge sort过程中，进行统计 if left &gt;= right: return [nums[left]] mid = left + (right - left) // 2 l_sort = sort(nums, left, mid) r_sort = sort(nums, mid+1, right) return merge(l_sort, r_sort) def merge(l_sort, r_sort): res, incre = [], 0 while l_sort and r_sort: # 由小到大merge sort if l_sort[0][0] &gt; r_sort[0][0]: incre += 1 res.append(r_sort.pop(0)) else: # l_sort和r_sort均已排序，所以l_sort[0]&lt;r_sort[0]，意味着l_sort[0]比r_sort所有元素都小 count[l_sort[0][1]] += incre res.append(l_sort.pop(0)) while l_sort: count[l_sort[0][1]] += incre res.append(l_sort.pop(0)) remain = l_sort if l_sort else r_sort res.extend(remain) return res count = [0] * len(nums) # count[i]是第i个元素的smaller numbers after self val_idx = [(num, i) for i, num in enumerate(nums)] sort(val_idx, 0, len(nums) - 1) return count 主要思路：右侧小于当前元素的个数，为归并排序的合并（merge）过程中，右半段（r_sort）中小于当前元素的个数。 2. 数组中的第K个最大元素: Leetcode 215. Kth Largest Element in an Array123456789101112131415161718192021222324252627282930# ------------------------------ Solution 1 ------------------------------ #class Solution(object): def findKthLargest(self, nums, k): import heapq # size k, min-heap, t=(n-k)lgk heap = nums[:k] heapq.heapify(heap) for num in nums[k:]: if num &gt; heap[0]: heapq.heapreplace(num) return heap# ------------------------------ Solution 2 ------------------------------ #class Solution(object): # Partition method (of Quicksort) def partition(self, nums, left, right): low, pivot = left, nums[right] for i in range(left, right): if nums[i] &lt; pivot: nums[i], nums[low] = nums[low], nums[i] low += 1 nums[low], nums[right] = nums[right], nums[low] return low def findKthLargest(self, nums, k): pos = self.partition(nums, 0, len(nums) - 1) if pos &gt; len(nums) - k: return self.findKthLargest(nums[:pos], pos - (len(nums) - k)) elif pos &lt; len(nums) - k: return self.findKthLargest(nums[pos+1:], k) else: return nums[pos] 3. 其他例子：Leetcode 75. Sort ColorsLeetcode 179. Largest Number 三. ReferenceWiki: 回溯法Wiki: 希尔排序Leetcode: Python different solutions (bubble, insertion, selection, merge, quick sorts).算法 3：最常用的排序——快速排序Leetcode: 复习基础排序算法（Java）Leetcode: Python 实现的十大经典排序算法GeeksforGeeks: QuickSort","link":"/2020/07/19/sort/"},{"title":"数据结构：栈","text":"栈也称堆栈，具有线性存储结构。栈的特点是先入后出，基本操作有入栈（push）和出栈（pop），如下图所示。 应用场景：在解决问题时，在该次操作完成了之后，需要向前查找到前次操作，即DFS。 一. 栈的设计与实现1. 最小栈: Leetcode 155. Min Stack12345678910111213141516171819class MinStack(object): def __init__(self): self.stack, self.min_stack = [], [] def push(self, x): self.stack.append(x) self.min_stack.append(min(x, self.getMin())) # 保存入栈后，当前的最小值 def pop(self): self.stack.pop() self.min_stack.pop() def top(self): return self.stack[-1] def getMin(self): if self.min_stack: return self.min_stack[-1] return float(\"inf\") 主要思路：栈顶元素出栈后，堆栈最小元素的更新，需要历史数据；为常数时间获得堆栈最小元素，在数据入栈过程中，就应保存对应的最小值。 2. 用队列实现栈: Leetcode 225. Implement Stack using Queues123456789101112131415161718class MyStack(object): def __init__(self): self.queue = [] def push(self, x): self.queue.append(x) def pop(self): for _ in range(len(self.queue) - 1): temp = self.queue.pop(0) self.queue.append(temp) return self.queue.pop(0) def top(self): return self.queue[-1] def empty(self): return len(self.queue) == 0 主要思路：pop过程中，将队列的队头元素弹出后，压入队尾，直至目标元素弹出；使用了单调递减栈。 二. 单调栈单调栈，栈内元素具有单调性，主要用于回答：“比当前元素，更大(小)的下(前)一个元素是什么？（next/last greater/less element）”的问题。 1. Next greater element问题主要模板如下： 1234567891011121314def next_greater_element(nums): # t = O(n) ans, stack = [], [] for i in range(len(nums)-1, -1, -1): # Next great element，需知道后续元素，故反向遍历 while stack and stack[-1] &lt; nums[i]: stack.pop() # 将小于num[i]的都pop出，直至得到大于nums[i]的 # Solution 1: 存储next_great的值 next_greater_i = stack[-1] if stack else -1 res.insert(0, next_greater_i)# ...对num[i]的下一个更大元素next_greater_i的操作 stack.append(nums[i]) # Solution 2: 存储next_great对应的的索引 next_greater_i_index = stack[-1] if stack else -1 # ...nums[next_greater_i_index]的操作 stack.append(i) return ans 具体例子有：Leetcode 496. Next Greater Element I，Leetcode 503. Next Greater Element II。 2. 柱状图问题，例如：柱状图中最大的矩形Leetcode 84. Largest Rectangle in Histogram12345678910111213141516class Solution(object): def largestRectangleArea(self, heights): if not heights: return 0 heights.append(0) # heights末尾家0，用于解决边界问题 length, stack, res_max = len(heights), [], 0 for i in range(length): while stack and heights[stack[-1]] &gt; heights[i]: poped_index = stack.pop() # heights[i]，是stack.pop()的右边界（即stack.pop()右侧，首个小于heights[i]的点） last_index = stack[-1] if stack else -1 # stack[-1]，是stack.pop()的左边界 res_max = max((i-last_index-1) * heights[poped_index], res_max) stack.append(i) return res_max 主要思路：对每一个位置i，找到以heights[i]为高度的，最大矩形；即寻找i左侧、右侧，第一个低于height[i]的位置；可以使用动态规划，也可使用单调递增栈（参考）；相似用例：Leetcode 42. Trapping Rain Water，使用了单调递减栈，使得弹出、现栈顶、现指向，三个元素构成凹槽。 3. 滑窗极值问题，例如：Leetcode 239. Sliding Window Maximum12345678910111213141516class Solution(object): def maxSlidingWindow(self, nums, k): stack = [] # 单调递减栈, 栈底stack[0]为最大元素 n, res = len(nums), [] for i in range(n): while stack and nums[stack[-1]] &lt; nums[i]: stack.pop() stack.append(i) if i - stack[0] &gt;= k: # 若栈底元素超出划窗，则弹出 stack.pop(0) if i &gt;= k-1: # 从i=2开始计算 res.append(nums[stack[0]]) return res 主要思路：详解。相似用例：Leetcode 581. Shortest Unsorted Continuous Subarray。 三. 栈的其他应用1. 表达式顺序，例如：Leetcode 32. Longest Valid Parentheses1234567891011121314151617181920class Solution(object): def longestValidParentheses(self, s): stack = [] for i, p in enumerate(s): if p == \"(\": stack.append((\"(\", i)) if p == \")\": if stack and stack[-1][0] == \"(\": stack.pop() # 出栈配对的括号 else: stack.append((\")\", i)) # 入栈未能配对的括号 stack.append((\"end\", len(s))) res, prev = 0, -1 for i, p in stack: res = max(res, p - prev - 1) prev = p return res 主要思路：弹出成对的括号，留下未成对的括号；最终stack中仅留下未成对的括号，未成对括号之间，即为有效的成对括号；也可以用动态规划解题。相似用例：Leetcode 20. Valid Parentheses、Leetcode 150. Evaluate Reverse Polish Notation、Leetcode 224. Basic Calculator。 2. 字符串或数组问题，例如Leetcode 316. Remove Duplicate Letters123456789101112131415161718192021class Solution(object): def removeDuplicateLetters(self, s): # 遍历，记录出现频率 count_list = [0] * 26 for char in s: count_list[ord(char) - ord(\"a\")] += 1 # 遍历， 维护递增；保持栈内所有元素都小于即将进栈的元素，若不满足，且栈顶元素在后续出现，则需要把栈顶元素出栈 stack = [] exist_list = [False] * 26 # 用于判断当前元素是否存在于栈中，若存在，则该元素已经排序，跳过 for i, char in enumerate(s): count_list[ord(char) - ord(\"a\")] -= 1 if exist_list[ord(char) - ord(\"a\")]: continue else: while stack and ord(s[stack[-1]]) &gt; ord(char) and count_list[ord(s[stack[-1]])-ord(\"a\")] &gt; 0: exist_list[ord(s[stack.pop()]) - ord(\"a\")] = False stack.append(i) exist_list[ord(char) - ord(\"a\")] = True return \"\".join([s[i] for i in stack]) 主要思路：从指定序列中去除部分元素，使得剩下的元素最小（最大）；两个相同位数的数字的大小关系，取决于第一个不同的数的大小；借鉴[1][2][3]，主要通过单调栈解决问题；保持栈内所有元素都小于（大于）即将进栈的元素，若不满足，且栈顶元素在后续出现，则需要把栈顶元素出栈，直到满足要求。相似用例：Leetcode 402. Remove K Digits、Leetcode 321. Create Maximum Number。 四. ReferenceWiki: 堆栈Blog: 栈和队列Blog: 队列实现栈|栈实现队列Blog: Leetcode 单调栈问题总结（超详细！！！）Blog: LeetCode 总结 - 搞定 Stack 面试题Blog: LeetCode Monotone Stack Summary 单调栈小结Blog: 一招吃遍力扣四道题，妈妈再也不用担心我被套路啦～","link":"/2020/07/09/stack/"},{"title":"Tree-Based Learning Algorithms - Part 2","text":"Decision tree can be very non-robust. A small change in the training data can result in a large change in the tree and, consequently, the final predictions. To resolve the problem above, some techniques, often called ensemble methods, construct multiple decision trees that are combined to yield a single and better consensus prediction. Random ForestThe random forest approach is a bagging method where deep trees (why deep), fitted on bootstrap samples, are combined to produce an output with lower variance. However, it thus increasing the computational complexity by $B$ times. Feature Sampling:In addition to bagging, in the case of random forests, as each tree is constructed, only a random sample of predictors (features) is taken before each node is split, which decorrelates the trees. This random small tweak has benefits below: There are often a few predictors that dominate the decision tree fitting process. Consequently, many other predictors, which could be useful for very local features of the data, are rarely selected as splitting variables. With this tweak, each predictor will have at least several opportunities to be the predictor defining a split. It makes the decision making process more robust to missing features: Observations with missing features can still be regressed or classified based on the features that are not missing. Feature Importance Measures:We can obtain an overall summary of the importance of each predictor (feature) estimating the metrics we mentioned in last post. We can record the total amount of metrics due to splits over a given predictor, averaged over all $B$ trees. A large value indicates an important predictor. Essential parameters: n_Trees: In practice, few hundreds trees is often a good choice. n_Features: Typically, if there are a total of $D$ predictors, $D/3$ predictors in the case of regression and $\\sqrt{D}$ predictors in the case of classification make a good choice. Boosted TreesIn this post, we will focus specifically on the two most common boosting algorithms: AdaBoost and Gradient Boosting. Generic Setting and AlgorithmIn supervised learning problems, one has an output variable $y$ and a vector of input variables $\\mathbf{x}$ described via a joint probability distribution $P(\\mathbf{x}, y)$. Using a training set $\\{(\\mathbf{x}_{1},y_{1}),\\dots ,(\\mathbf{x}_{n},y_{n})\\}$, the goal is to find an approximation ${\\hat {F}}(\\mathbf{x})$ that minimizes the expected value of some specified loss functions. $$\\hat{F}=\\underset{F}{\\arg \\min } \\mathbb{E}_{\\mathbf{x}, y}[L(y, F(\\mathbf{x}))]$$ In boosting method, it seeks ${\\hat {F}}(\\mathbf{x})$ in the form of a weighted sum of functions $h_{i}(\\mathbf{x})$ (weak learners) from some class $\\mathcal {H}$: $$\\hat{F}(\\mathbf{x})=\\sum_{i=1}^{M} \\gamma_{i} h_{i}(\\mathbf{x})+\\text { const. }$$ It does so by starting with a model, consisting of a constant function $F_{0}(\\mathbf{x})$: $${F_{0}(\\mathbf{x})=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, \\gamma\\right)}$$ , and incrementally expands it in a greedy fashion: $$F_{m}(\\mathbf{x})=F_{m-1}(\\mathbf{x})+\\gamma_{m}h_m\\\\{F_{m}(\\mathbf{x})=F_{m-1}(\\mathbf{x})+\\underset{h_{m} \\in \\mathcal{H}}{\\gamma_{m}\\arg \\min }\\left[\\sum_{i=1}^{n} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)+h_{m}\\left(\\mathbf{x}_{i}\\right)\\right)\\right]}$$, where $h_{m}\\in {\\mathcal {H}}$ is a weak learner function, $F_{m}(\\mathbf{x})$ is the aggregated function after $m$ steps. The above algorithm of boosting is generic. One can come with several strategies for the choice of the weak learner, residual (error) to fit, and the weighting parameter $\\lambda$. Therefore, there are various boosting methods. Adaptive BoostingThe AdaBoost (Adaptive Boosting) approach is a boosting method where shallow trees (why shallow), fitted on weighted samples, are combined to produce an output with lower bias. Be Compared to Random Forest:Compared to Random Forest, AdaBoost is generally less computational (shallow trees). However, these computations to fit different weak learners can’t be done in parallel. Overall, Random Forest is usually less accurate than Boosting on a wide range of tasks and usually slower in the runtime. Algorithm Setting:The total error $E$ defined in AdaBoost is the sum of its exponential loss ($\\sum_{i=1}^{N} e^{-y_{i} f\\left(\\mathbf{x}_{i}\\right)}$, different loss function result in different variations) on each data point, given as follows: $$E=\\sum_{i=1}^{N} e^{-y_{i} F_{m}\\left(\\mathbf{x}_{i}\\right)}=\\sum_{i=1}^{N} e^{-y_{i} F_{m-1}\\left(\\mathbf{x}_{i}\\right)} e^{-y_{i} \\gamma_{m} h_{m}\\left(\\mathbf{x}_{i}\\right)}\\\\E=\\sum_{i=1}^{N} w_{i}^{(m)} e^{-y_{i} \\gamma_{m} h_{m}\\left(\\mathbf{x}_{i}\\right)}$$, where $w_{i}^{(m)}=e^{-y_{i} F_{m-1}\\left(\\mathbf{x}_{i}\\right)}$ is a constant error of last iteration. So we can determine the optimal weak learner $h_m$ at step $m$ by $\\underset{h_m}{\\arg \\min }E$ and then the weighting parameter $\\gamma_m$​ by $\\frac{d E}{d \\gamma_{m}}=0$. A derivation of them in binary classification can be seen at wikipedia. Intuition of the Algorithm:AdaBoost weighs more on misclassified samples and good learners than the correctly classified samples and bad learners, respectively. First, it weighs observations in the dataset and train a new weak learner with a special focus given to the misclassified observations:$$E=\\sum_{i=1}^{N} \\underbrace{e^{-y_{i} F_{m-1}\\left(\\mathbf{x}_{i}\\right)}}_{Weights} \\underbrace{e^{-y_{i} \\gamma_{m} h_{m}\\left(\\mathbf{x}_{i}\\right)}}_{\\text{Loss of each observation}}$$So $\\underset{h_{m}}{\\arg \\min } E$ can not only be seen as an optimization of the boosted model after $m$ steps on the initial dataset but also a weak learner on the weighted dataset. Second, it adds the weak learner with a coefficient $\\gamma_m$ that expresse the performances of this weak learner: the better a weak learner performs, the more it contributes to the strong learner. For example, in binary classification case: $\\gamma_m=\\frac{1}{2} \\ln \\left(\\frac{\\sum_{y_{i}=h_{m}\\left(\\mathbf{x}_{i}\\right)} w_{i}^{(m)}}{\\sum_{y_{i} \\neq h_{m}\\left(\\mathbf{x}_{i}\\right)} w_{i}^{(m)}}\\right)$, in which the larger amount of misclassified data is the smaller $\\lambda_m$ is. Gradient BoostingDiffer from Adaboost, in Gradient Boosting, the decision tree model is trained on the pseudo-residuals, rather than residuals. Algorithm Setting:The idea is to apply a steepest descent step to this minimization problem (functional gradient descent, which regards the entire function as a variable). We would update the model in accordance with the following equations: $$ r_{im}=\\nabla_{F_{m-1}} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)\\right)\\\\ {F_{m}(x)=F_{m-1}(\\mathbf{x})-\\gamma_{m} \\sum_{i=1}^{n}r_{im}} $$ , where $r_{im}$ is the pseudo-residuals and the model is initialized with a constant value $F_{0}(\\mathbf{x})=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, \\gamma\\right)$. Generic Gradient Boosting Method: Initialize model with a constant value $F_{0}(\\mathbf{x})=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, \\gamma\\right)$. For $m=1$ to $M$, a weak learner $h_{m}(\\mathbf{x})$ is fitted to pseudo-residuals $\\left\\{\\left(\\mathbf{x}_i, r_{i m}\\right)\\right\\}_{i=1}^{n}$. Compute multiplier $\\gamma _{m}$ using line search:$$\\gamma_{m}=\\underset{\\gamma}{\\arg \\min } \\sum_{i=1}^{n} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)+\\gamma h_{m}\\left(\\mathbf{x}_{i}\\right)\\right)$$ Update the model: $F_{m}(\\mathbf{x})=F_{m-1}(\\mathbf{x})+\\gamma_{m} h_{m}(\\mathbf{x})$ Compared to Adaboost:Gradient boosting uses instead a gradient descent approach and can more easily be adapted to large number of loss functions. Thus, gradient boosting can be considered as a generalization of Adaboost to arbitrary differentiable loss functions. Regularization/Parameters of Boosted TreesSeveral so-called regularization techniques reduce this overfitting effect by constraining the fitting procedure. Iterations: Namely, the number of trees in the model. Increasing it reduces the error on training set, but may lead to overfitting. Shrinkage / Learning Rate: Shrinkage consists in modifying the update rule as follows:$$F_{m}(\\mathbf{x})=F_{m-1}(\\mathbf{x})+\\nu \\cdot \\gamma _{m}h_{m}(\\mathbf{x}),\\quad 0 Empirically using small learning rates (such as $\\nu &lt;0.1$) improves models’ generalization ability (increase bias but decrease variance). However, it comes at the price of increasing computational time both during training and predicting, since lower learning rate requires more iterations. Sub-Sampling: Motivated by the bagging method, a decision tree can be fit on a subsample of the training set drawn at random without replacement. Sampling ratio ($f$) introduces randomness into the algorithm and help prevent overfitting acting as a kind of regularization. $0.5\\leq f\\leq 0.8$ (typically set to $0.5$) leads to good results. Also, like in bagging, subsampling allows one to define an out-of-bag error, which helps avoid the need for an independent validation dataset, but often underestimate actual performance improvement and the optimal number of iterations. Minimum Sample Size for Splitting Trees: Ignore any splits that lead to nodes containing fewer than this number of training set instances, which reduces variance. Sampling Features: Randomly choosing small subsets of features for different trees/layers/nodes, as in the case of Random Forest. Penalize Complexity of Tree:Penalize model complexity of the learned model. The model complexity can be defined as: The proportional number of leaves in the learned trees. $\\ell _{2}$ penalty on the leaf values (score). The joint optimization of loss and model complexity corresponds (equals) to a post-pruning algorithm to remove branches that fail to reduce the loss by a threshold. XgboostThe biggest drawback of the gradient boosting trees is that the algorithm is quite sequential and hence is very slow. XGBoost provides an optimized implementation of gradient boosted trees. It uses various tricks like regularization, column block for parallel learning, and cache-aware access to accelerate the speed. Algorithm Setting:The objective loss of Xgboost and its Second-order Taylor approximation is as following: $$\\begin{split}\\tilde{\\mathcal{L}}&=\\underbrace{\\sum_{i=1}^{n} L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)+h_{m}\\left(\\mathbf{x}_{i}\\right)\\right)}_{Original loss}+\\underbrace{\\eta T+\\sum_{j=1}^{T}\\frac{1}{2} \\lambda\\left\\|\\omega_{j}\\right\\|^{2}}_{Regularization}\\\\&\\approx\\sum_{i=1}^{n}\\left[L\\left(y_{i}, F_{m-1}\\right)+\\frac{\\partial{L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)\\right)}}{\\partial{F_{m-1}}} h_{m}\\left(\\mathbf{x}_{i}\\right) + \\frac{1}{2} \\frac{\\partial^2{L\\left(y_{i}, F_{m-1}\\left(\\mathbf{x}_{i}\\right)\\right)}}{\\partial F_{m-1}^2}h_{m}^{2}\\left(\\mathbf{x}_{i}\\right)\\right]\\\\&+ \\eta T+\\sum_{j=1}^{T}\\frac{1}{2} \\lambda\\left\\|\\omega_{j}\\right\\|^{2}\\\\&= \\sum_{i=1}^{n}\\left[L\\left(y_{i}, F_{m-1}\\right)+g^1_i h_{m}\\left(\\mathbf{x}_{i}\\right) + \\frac{1}{2} g^2_i h_{m}^{2}\\left(\\mathbf{x}_{i}\\right)\\right]+ \\eta T+\\sum_{j=1}^{T}\\frac{1}{2} \\lambda\\left\\|\\omega_{j}\\right\\|^{2}\\end{split}$$, where $\\omega_j$ is the score in leaf nodes of weak learner $h_m$, and assume $I_{j}=\\left\\{i | q\\left(\\mathbf{x}_{i}\\right)=j\\right\\}$ as the instance set of leaf $j$: $$\\sum_{i=1}^{n}h_{m}\\left(\\mathbf{x}_{i}\\right)=\\sum_{j=1}^{T}\\left(\\sum_{i \\in I_{j}} 1\\right) w_{j}$$ By setting derivative of loss to 0, we can get the optimal leaf scores for $h_m$ : $$ w_{j}^{*}=-\\frac{\\sum_{i \\in I_{j}} g^1_{i}}{\\sum_{i \\in I_{j}} g^2_{i}+\\lambda} $$ But there could be an infinite number of possible trees with these optimal leaf scores, detailed method is needed to specify the tree. Compared to GBDT: From the perspective of optimization, GBDT adopts fastest descent method to get the optimal solution of loss function, while Xgboost uses the analytic method to expand the loss function to its second-order approximation and find the analytic solution. The parallelism of Xgboost is different from random forest. Xgboost can only carry out the next iteration after the previous iteration. The parallelism of xgboost is on the feature level. ReferenceBlog: Understanding Boosted Trees ModelsBlog: Ensemble methods: bagging, boosting and stackingStackExchange: How can we explain the fact that “Bagging reduces the variance while retaining the bias” mathematically?Blog: XGBoost原理和底层实现剖析PPT: Introduction to Boosted TreePaper: XGBoost: A Scalable Tree Boosting System","link":"/2019/11/24/tree-based-learning-algorithms-part-2/"},{"title":"Tree-Based Learning Algorithms - Part 1","text":"Tree-based models consist of decision tree and other approaches involving producing multiple trees that are combined to yield a single and better consensus prediction. These models partition the feature space into a set of polyhedrons, and then fit a simple model (like a constant) in each one. Unlike linear models, they map non-linear relationships quite well. They are adaptable at solving both classification or regression. In this post, we will look at the mathematical details and parameter explanations of several tree-based models, including Decision tree, Random forest, Boosted trees. Decision TreeA decision tree is a flowchart-like structure in which: Each internal node represents a “test” on an attribute. Each branch represents the outcome of the test. Each leaf node represents a class label (decision taken after computing all attributes). The paths from the root to the leaf represent classification rules. Decision Tree Learning Decision tree learning is the construction of a decision tree from class-labeled training tuples. There are many specific decision-tree algorithms, and notable ones include ID3, C4.5, CART. Algorithms for constructing decision trees is greedy which: Usually, work top-down by choosing a variable at each step that best splits the set of items. This process is repeated on each derived subset in a recursive manner (Depth First). The recursion is completed when the subset at a node reaches a predefined stopping criterion. According to the above, there will naturally be a question: What are the best splits? Different algorithms use different metrics for measuring “best”. These generally measure the homogeneity of the target variable within the subsets. Variance reductionIntroduced in CART, variance reduction is often employed in cases where the target variable is continuous (regression tree). The variance reduction of a node $N$ is defined as the total reduction of the variance of the target variable $x$ due to the split at this node: $$\\begin{split} I_{V}(N)=&\\frac{1}{|S|^{2}} \\sum_{i \\in S} \\sum_{j \\in S} \\frac{1}{2}\\left(x_{i}-x_{j}\\right)^{2}\\\\&-\\underbrace{\\left(\\frac{1}{\\left|S_{t}\\right|^{2}} \\sum_{i \\in S_{t}} \\sum_{j \\in S_{t}} \\frac{1}{2}\\left(x_{i}-x_{j}\\right)^{2}+\\frac{1}{\\left|S_{f}\\right|^{2}} \\sum_{i \\in S_{f}} \\sum_{j \\in S_{f}} \\frac{1}{2}\\left(x_{i}-x_{j}\\right)^{2}\\right)}_{\\text {Minimum: Largest Homogeneity}}\\end{split}$$ , where $S$, $S_{t}$, and $S_{f}$ are the set of pre-split sample indices, set of sample indices for which the split test is true, and set of sample indices for which the split test is false, respectively. Gini impurityIntroduced in CART for classification trees, Gini impurity is a measure of how often a randomly chosen element ($p_{i}$) from the set would be incorrectly labeled if it was randomly labeled according to the DISTRIBUTION of labels in the subset ($1-p_{i}$). The Gini impurity can be computed by summing the probability $p_{i}$ of an item with label $i$ being chosen times the probability $1-p_{i}$ of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category: $$\\operatorname{Gini}(T)=\\sum_{i=1}^{J} p_{i} \\sum_{k \\neq i} p_{k}=\\sum_{i=1}^{J} p_{i}\\left(1-p_{i}\\right) $$, where $J$ is the amount of classes and $i\\in \\{1,2,...,J\\}$. And the Gini impurity reduction can be defined as below: $$\\begin{split}{IG(T, a)}&= \\underbrace{\\mathrm{Gini}(T)}_{\\text {Gini (parent) }}-\\quad \\underbrace{\\mathrm{Gini}(T | a)}_{\\text {Weighted Sum of Gini (Children) }} \\\\&=\\sum_{i=1}^{J} p_{i}\\left(1-p_{i}\\right)-\\sum_{a} p(a) \\sum_{i=1}^{J}-\\operatorname{P}(i | a) (1- \\operatorname{P}(i | a))\\end{split}$$ Information gainInformation gain is based on the concept of entropy in information theory, which we described in previous post. Entropy is defined as below: $$\\mathrm{H}(T)=\\mathrm{I}_{E}\\left(p_{1}, p_{2}, \\ldots, p_{J}\\right)=-\\sum_{i=1}^{J} p_{i} \\log _{2} p_{i}$$, where $p_{1},p_{2},...$ are fractions that add up to 1 and represent the percentage of each class present in the child node. $$\\begin{split}{IG(T, a)}&= \\underbrace{\\mathrm{H}(T)}_{\\text {Entropy (parent) }}-\\quad \\underbrace{\\mathrm{H}(T | a)}_{\\text {Weighted Sum of Entropy (Children) }} \\\\&=-\\sum_{i=1}^{J} p_{i} \\log _{2} p_{i}-\\sum_{a} p(a) \\sum_{i=1}^{J}-\\operatorname{P}(i | a) \\log _{2} \\operatorname{P}(i | a) \\end{split}$$ CART was implemented in Scikit-learn - sklearn.tree.DecisionTreeClassifier and sklearn.tree.DecisionTreeRegressor. The parameters below are essential: max_features: The number of randomly chosen features to consider when looking for the best split. A low value can be set to control over-fitting. Since the algorithm is greedy, setting max_features &lt; n_features will try more splitting possibilities. max_depth, min_samples_split, … : These parameters resulting in a less complex tree (less nodes) can control over-fitting. Tree Pruning Decision-tree learners can create over-complex trees that do not generalize well from the training data. (Overfitting) Tree pruning is a technique that reduces the size of the decision tree by removing sections of the tree (substitute sub-trees with their leaf nodes) that provide little power to classify instances. Pruning reduces the complexity of the final decision tree, and hence improves predictive accuracy by the reduction of overfitting. There are two types of Pruning: Pre-pruning (also known as early stopping)Pre-pruning will stop the tree-building process early while constructing a decision tree. To avoid overfitting, if the error does not decrease significantly enough or the tree reaches its maximum depth (doesn’t meet the threshold), then we stop. However, early stopping may underfit by stopping too early. The current split may be of little benefit, but having made it, subsequent splits more significantly reduce the error. Post-pruning After the growth of the decision tree is completed, the tree can be post-pruned to be simplified. The process of post-pruning is to check each sub-tree that, if we substitute the sub-tree with their leaf nodes, whether the increase in impurity measure is less than the specified threshold. Post-pruning in CARTThe post-pruning in CART has two steps: Continuously prune the decision tree $T_0$ built by CART to generate a sub-tree sequence ${T_0, T_1, …, T_n}$. For each non-leaf node, you can compute its regularizied cost as below: $$C_{a}\\left(T_{t}\\right)=C\\left(T_{t}\\right)+a|T_t|$$, where $T_t$ is an arbitrary sub-tree whose root is $t$, $C(T_t)$ is the total impurity measure of the sub-tree, $\\alpha \\geq 0$ is the coefficient of the regularization term, $| T_t |$ is the number of leaf nodes of the sub-tree. If the decision tree is pruned at non-leaf node $t$, then $T_t$ turns into a single node tree whose regularizied cost becomes as below: $$C_{\\alpha}(t)=C(t)+\\alpha * 1$$ Only if the regularizied cost decreases ($C_{\\alpha}(t) \\leq C_{\\alpha}\\left(T_{t}\\right)$), then this sub-tree $T_t$ will be pruned, namely: $$C(t)+\\alpha \\leq C\\left(T_{t}\\right)+\\alpha\\left|T_{t}\\right|\\Rightarrow\\alpha \\geq \\frac{C(t)-C\\left(T_{t}\\right)}{\\left|T_{t}\\right|-1}$$ It means that for a non-leaf node $t$, the minimum regularization coefficient $\\alpha$ needed to prune $T_t$ is $\\alpha = \\frac{C(t)-C\\left(T_{t}\\right)}{\\left|T_{t}\\right|-1}$. We can iteratively compute the $\\alpha$ for each node in the last pruned tree and prune the node with the minimum $\\alpha$. We do it until only the root node $T_0$ is remained and finally will get the sub-tree sequence ${T_0, T_1, …, T_n}$. Test each subtree in the sequence of cross-validation subtrees, and select the optimal subtree as the final pruning result. ReferenceBlog: A Practical Guide to Tree-Based Learning AlgorithmsBlog: Machine Learning: Pruning Decision TreesZhihu: CART树怎么进行剪枝Blog: 决策树及其剪枝原理","link":"/2019/11/19/tree-based-learning-algorithms-part-1/"},{"title":"Build Transformer from Scratch - Part 1","text":"The Transformer from “Attention is All You Need” is a new architecture that utilizes the Attention concept to help improve the performance of NLP tasks. This passage follows the pytorch implementation in “The Annotated Transformer” of Harvard’s NLP group to compactly illustrate the structure of the Transformer. BackgroundIn traditional (RNN or CNN) models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, which makes it more difficult to learn dependencies between distant positions. To solve the issue above, the Transformer comes into being, which reduces the operations to a constant number. It eschews recurrence or convolution and relies entirely on an attention mechanism to draw global dependencies between input and output. Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure as below: Here, the encoder maps an input sequence of symbol representations $\\left(\\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n}\\right)$ (always represented by a $(n \\times d_{model})$ matrix) to a sequence of continuous representations $\\mathbf{z}=\\left(z_{1}, \\dots, z_{n}\\right)$. Given $\\mathbf{z}$, the decoder then generates an output sequence $\\left(\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{m}\\right)$($(m \\times d_{model})$ matrix) of symbols one element at a time. 1234567891011121314151617181920212223242526272829303132class EncoderDecoder(nn.Module): \"\"\" A standard Encoder-Decoder architecture. Base for this and many other models. \"\"\" def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): \"\"\" Initialize inner architectures of EncoderDecoder. \"\"\" super(EncoderDecoder, self).__init__(self, encoder, decoder, src_embed, tgt_embed, generator) self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator # Generate predicted distribution from hidden layer def forward(self, src, tgt, src_mask, tgt_mask): \"\"\" The flow of data through the EncoderDecoder. \"\"\" return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): \"\"\" 'src' and 'src_mask' are only required input for producing intermediate state (memory). \"\"\" return self.encoder(self.embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): \"\"\" 'src_mask' is needed in decoding process for masking the padding tokens in memory sequence. \"\"\" return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) 12345678910111213class Generator(nn.Module): \"\"\" Generate predicted distribution from hidden layer: linear + softmax. \"\"\" def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) # A kind of transposed embedding matrix def forward(self, x): \"\"\" log(softmax(x)): Doing these two operations separately is slower and numerically unstable. \"\"\" return F.log_softmax(self.proj(x), dim=-1) The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of the figure below, respectively. Encoder and Decoder StacksEncoderThe encoder is composed of a stack of $N=6$ (larger to increase model complexity) identical layers. 12345def clones(module, N): \"\"\" Produce N identical layers. \"\"\" return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) Note：nn.ModuleList and nn.Sequential in PyTorch nn.ModuleList holds submodules in a list and can be indexed like a regular Python list, but modules it contains are properly registered (can be displayed by print(module)). If a Python list is used, these modules will not be registered, so their parameters cannot be updated (trained). nn.Modulelist only stores different submodules together. There is no order between these modules. The execution order of them is determined by the forward function. nn.Sequential implements the internal forward function, and the modules in it must be arranged in order. So it must be ensured that the output size of the previous module is consistent with the input size of the next module. nn.ModuleList is more commonly used in the case that you want to customize the forward function. For example, the output of the current layer needs to be fused with the previous layer and then be forwarded to next layer. Each layer has two sub-layers. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the actual output of each sub-layer is $LayerNorm(\\mathbf{x}+Sublayer(\\mathbf{x}))$. 123456789101112131415class LayerNorm(nn.Module): \"\"\" A layernorm module. \"\"\" def __init__(self, size, eps=1e-6): super(LayerNorm, self).__init__(self) self.a = nn.Parameter(torch.ones(size)) # Parameter that controls the variance self.b = nn.Parameter(torch.zeros(size)) # Parameter that controls the bias self.eps = eps def forward(self, x): # If 'keepdim' is True, the output tensor is of the same size as input. Otherwise, dim is squeezed. mean = x.mean(dim=-1, keepdim=True) std = x.std(-1, keepdim=True) return self.a * (x - mean) / (std + self.eps) + self.b Note：nn.Parameter and nn.Module.register_buffer in PyTorch nn.Parameter are torch.Tensor subclasses and it will be automatically added to the list of its parameters when used with Modules. (Appear e.g. in parameters() iterator.) nn.Module.register_buffer can add named tensors (buffer) to the module, but these tensors are not meant to learn via gradient descent. They can be iterated by Module.buffers(). Buffers will go to GPU with the module in Module.to(), and they will be saved together with the module in Module.state_dict(). Buffer is used to saving intermediate status, for example, BatchNorm’s running_mean is not a parameter, but is part of the persistent state. 12345678910111213141516class SublayerConnection(nn.Module): \"\"\" A residual connection followed by a layer norm. In practical implementation, the norm is first for better experimental results. \"\"\" def __init__(self, size, dropout): \"\"\" Module attributes will automatically be added to its parameters list (if it's registerable). \"\"\" super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): return x + nn.dropout(sublayer(self.norm(x))) To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model} = 512$. The first sub-layer is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. 123456789101112131415class EncoderLayer(nn.Module): \"\"\" Encoder is made up of self-attn and feed forward (defined below). \"\"\" def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward # Need 2 different sublayer connection modules (e.g. different parametes in LayerNorm) self.sublayers = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayers[1](x, self.feed_forward) 12345678910111213141516class Encoder(nn.Module): \"\"\" A stack of N basic encoder layers. \"\"\" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): \"\"\" Pass the input and mask through each layer in turn. \"\"\" for layer in self.layers: x = layer(x, mask) return self.norm(x) # Why? DecoderThe decoder is also composed of a stack of $N=6$ identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, it employs residual connections around each of the sub-layers, followed by layer normalization. 123456789101112131415161718class DecoderLayer(nn.Module): \"\"\" Decoder is made of self-attn, src-attn, and feed forward. \"\"\" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayers = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memeory, src_mask, tgt_mask): \"\"\" 'memory' is the output of stacked Encoder. \"\"\" x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayers[1](x, lambda x: self.src_attn(x, memeory, memeory, src_mask)) return self.sublayers[2](x, self.feed_forward) 12345678910111213class Decoder(nn.Module): \"\"\" Generic N layer decoder with masking. \"\"\" def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x) Thie paper also modifies the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions (data leakage). 12345678def subsequent_mask(length): \"\"\" Mask out subsequent positions. 'length' here is the length of input sequence. \"\"\" attn_shape = (1, length, length) # Upper triangle of an array subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') return torch.from_numpy(subsequent_mask) == 0 This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$. 12self.trg = trg[:, :-1] # [\"&lt;SS&gt;\", \"Hola\", \", \", \"como\", \"estás\", \"?\"]self.trg_y = trg[:, 1:] # [\"Hola\", \", \", \"como\", \"estás\", \"?\", \"&lt;EOS&gt;\"] AttentionAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Why Attention is Needed?Language heavily relies on context. For example, the pronouns in sentences refer to nouns. Attention is such a mechanism that encodes these kinds of reliances. It bakes model’s understanding of context of a certain word (its relevant words) and explains that word using this context (contextual embedding). Basically, the attention mechanism is a feature fusion process. The Attention MechanismThe two most commonly used attention functions are additive attention, and dot-product attention. The two are similar in complexity, but dot-product attention can be implemented using highly optimized matrix multiplication code (faster). The particular modified attention used in paper is called “Scaled Dot-Product Attention“. The input consists of queries and keys of dimension $d_k$ and values of dimension $d_v$. We compute the dot products of the query with all keys. Since for large values of $d_k$, the dot products grow large in magnitude ($q \\cdot k=\\sum_{i=1}^{d_{k}} q_{i} k_{i}$), pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, dot products was scaled by $\\sqrt{d_{k}}$. Finally, a softmax function is applied to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$ ($n\\times d_{k}$). The keys and values are also packed together into matrices $K$ ($n\\times d_{k}$) and $V$ ($n\\times d_{v}$). We compute the matrix of outputs as: $$ \\text {Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$$ 12345678910111213141516def attention(query, key, value, mask=None, dropout=None): \"\"\" Compute 'Scaled Dot Product Attention'. Since it's just a dot product and has no model parameters inside. \"\"\" d_k = query.size(-1) # Last dimension of query (d_model) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # Mask is set to the dot-product score: the corresponding element is set to -inf if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # Apply softmax to obtain proportion p_attn = F.softmax(scores, dim=-1) if dropout is not None: # Randomly zeroes some of the feature elements of the input tensor with probability `p`. The elements, namely network nodes. p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn Note: torch.Tensor.masked_fill(): Fills elements of self tensor with value where mask is True. torch.matmul(A, B): If the dimension of both A and B is greater than 2, batch_mat_mul will be called, which will do an element-wise matrix multiplication for the batch of A and B. A and B can be different except for the last two dimensions. Other dimensions (batch size) should be the same. Last two dimensions of A and B should meet the requirements of matrix multiplication. torch.Tensor.view/flatten(): The Tensor will be folded from ‘inner’ dimension to ‘outer’ dimension. Multi-head AttentionMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. It works as following:$$\\text { MultiHead }\\left.(Q, K, V)=\\text { Concat(head }_{1}, \\ldots, \\text { head }_{\\mathrm{h}}\\right) W^{O}$$, where $\\text {head}_i=\\text {Attention}(QW_i^Q,KW_i^K,VW^V_i)$ and the projections are parameter matrices $W_{i}^{Q} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{k}}, W_{i}^{K} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{k}}, W_{i}^{V} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{v}} \\text { and } W^{O} \\in \\mathbb{R}^{h d_{v} \\times d_{\\text {model }}}$. In this paper, it employs $h=8$ parallel attention layers, or heads. For each of these, it uses $d_k=d_v=d_{model}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 12345678910111213141516171819202122232425262728293031class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # Actually, d_model=query.size(-1), but to get d_k, it has to be input. self.h = h self.d_k = d_model // h self.linears = clones(nn.linears(d_model, d_model), 4) self.dropout = nn.Dropout(dropout) self.attn = None def forward(self, query, key, value, mask=None): nbatches = query.size(0) if mask is not None: mask = mask.unsqueeze(1) # (nbatches, length, length) -&gt; (nbatches, 1, length, length) # 1) Do all the linear projections for h headers in one matrix multiplication query, key, value = [ # (nbatches, length, d_model) -&gt; (nbatches, length, d_model) # -&gt;(nbatches, length, h*d_k) -&gt; (nbatches, h, length, d_k) l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value)) ] # 2) Apply attention on all the projected vectors in batch. # Put dropout outside 'attention' to manage all modules in one class. x, self.attn = attention(query, key, value, mask, self.dropout) # 3) \"Concat\" using a view and apply a final linear. # (nbatches, h, length, d_k) -&gt; (nbatches, length, d_model) x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) return self.linears[-1](x) Note: torch.Tensor.contiguous(): It is generally used after torch.Tensor.transpose() or torch.Tensor.permute() and before torch.Tensor.view(). After dimension transformation, the sensor is no longer stored continuously in memory. However, torch.Tensor.view() operation requires continuous memory storage of the tensor, so it needs contiguous() to return a continuous copy. Intuitive Explanation of AttentionSelf-attention is processed along the path of each token in the segment. The significant components are three vectors: Query: The query vector is a representation of the current word used to score against all the other words’ keys. Key: The key vectors are like labels for all the words and are matched against for the relevant word. Value: The value vectors are actual word representations. Once we’ve scored how relevant each word is, these values will be added up to represent the current word. An analogy is to think of Attention like searching through a filing cabinet as the figure below. The query is like a sticky note with the topic you’re researching. The keys are like the labels of the folders inside the cabinet. When you match the tag with a sticky note, we take out the contents of that folder, and these contents are the value vectors. Except you’re not only looking for one value but a blend of values from a blend of folders. The Masked Self-Attention A normal self-attention block allows a position to peak at tokens to its right. Masked self-attention prevents that from happening. The masking step always happens after the dot product of the query and the key and before applying softmax, namely when you get the score (matrix). It is just going to set minus infinity to all the masked entries in the score matrix. For example: Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways: Self-attention layers in encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Self-attention layers in the decoder. They allow each position in the decoder to attend to all positions up to and including that position. It prevents leftward information flow in the decoder to preserve the auto-regressive property. In “encoder-decoder attention“ layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence, which mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models. Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network. This consists of two linear transformations with a ReLU activation in between. $$\\mathrm{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}$$ The dimensionality of input and output is $d_{model}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. 123456789class PositionwiseFeedForward(nn.Module): def __init__(self, d_model, d_ff, dropout=0.1): self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): # Dropout is always used in weighted sum operation return self.w_2(self.dropout(F.relu(self.w_1(x)))) ReferenceGithub: OpenNMT-pyMedium: Dissecting BERT Part 1: Understanding the TransformerBlog: The Illustrated TransformerBlog: The Illustrated GPT-2Paper: Neural Machine Translation by Jointly Learning to Align and TranslatePaper: Attention is All You NeedStackOverflow: ModuleList and Sequential in PyTorchStackOverflow: Understanding torch.nn.ParameterBlog: Tensorflow中矩阵乘法matmul和multiply详解Zhihu: PyTorch中矩阵乘法总结Zhihu: Pytorch之contiguous函数","link":"/2019/12/09/build-transformer-from-scratch-part-1/"}],"tags":[],"categories":[]}